var searchContent =  [
  {
    "title": "MapReduce",
    "description": "From the basics to the actually useful",
    "content": "\n\n\nRecently, Tim Anglade culminated his NOSQL World Tour with the release of [the NOSQL Tapes](http://nosqltapes.com), a collection of live interviews collected globally. Not only do I find that site aesthetically pleasing (courtesy of our own [Steadicat](http://twitter.com/steadicat)), but there is a tremendous amount of solid content, from technical discussions to opinions and conjectures from many of the young leaders in the field. I was fortunate to get a bit of Tim’s time when he stopped in Seattle and we recorded a [longish video](http://nosqltap.es/8) on MapReduce in my lab at UW. While I love the old school feel of a chalk-talk, I must admit I’m pretty embarrassed at how rambling and fragmented my explanations are, not to mention my shoddy use of available board space. Therefore the Physics Professor in me feels obligated to try again, albeit offline this time.\n\n### Brief NOSQL Taxonomy\n\nTo set the MapReduce stage, I’m going to offer a very naive taxonomy of NoSQL data stores (apologies to the experts). I have yet to meet a database that isn’t a key/value store, so let’s focus on some higher level distinction. In particular, let’s focus on the difference between _big tables_ (BigTable, HBase, Cassandra, etc.) and _document store_s (CouchDB, MongoDB). All of these solutions are finding solid acceptance in the wild, so what – if any – difference is there? My one-word answer is _flexibility_. Huh, isn’t all NoSQL flexible? Well… yes and no. In my eyes, BigTable and its derivatives excel at one thing in particular: structuring your data for the efficient I/O access that is necessary to get blazing read speeds for large data sets. Somewhat orthogonal, document stores, especially CouchDB, prioritize flexibility. In particular, there is no requirement to define columns, column families, or super columns up front. You simply encapsulate your data as documents and push them into the store. How then, you may ask, can you get efficient queries? MongoDB treats the problem with blazingly fast in-memory table scans and document introspection for index builds, whereas CouchDB takes a different approach with something called incremental MapReduce. I’m going to briefly explain the latter and then show you how it works in action.\n\n### MapReduce Primer\n\nMapReduce is an old design pattern that was recently made famous by Google. There are many different implementations, and I won’t even try to address them all. It suffices to say that MapReduce is all about giving programmers an efficient way to consume data without needing to know how or where it is actually stored. Further, MapReduce excels at traversing datasets that live on more than one machine. It is simple, a bit restrictive, but extremely powerful. I’m going to show you how it works in CouchDB (well, actually in BigCouch hosted on Cloudant.com).\n\nBroadly speaking, CouchDB (and Cloudant’s BigCouch) use MapReduce as a tool to let you introspect your data and build persistent ‘views’ (indices) for fast query responses. And, oh yeah, it does it incrementally. Let’s examine the pieces in detail:\n\n* **View builds.** The process of scanning over your data to create indices that make queries fast. This step also often performs data normalization.\n* **MapReduce.** The framework by which user code gets executed for index builds.\n* **View queries**. How you get the isnformation out of your indices\n* **Incremental.** When new documents are added or existing documents are modified/removed, you don’t need to rescan your entire dataset. Instead the view engine only rescans the new/modified documents, and deletes the contributions from the deleted/modified documents. It’s a complex implementation, but one that really separates the Couch MapReduce model from, e.g., Hadoop’s or Riak’s implementations.\n\nFor full details on Cloudant’s implementation of MapReduce, take a look at [Adam’s O’Reilly Webcast](http://oreillynet.com/pub/e/1760). So what do you actually have to do? Well, you simply need to write one or two functions. The ‘map’ function is required, and the ‘reduce’ is optional. For simple sorting problems (e.g. building a SEO reverse index) map alone suffices. For data aggregation, reduce comes into the picture. CouchDB allows you to write these functions in any language you choose, although JavaScript is the most common. Let’s put some meat on the bones with examples. Below I’m going to show you how to do three of the most common things people do with CouchDB’s MapReduce:\n\n1. sort documents based on arbitrary information\n2. calculate aggregate statistics\n3. perform a time-series analysis\n\n### Our Example Problem\n\nWe are going to load and analyze FAA plane crash data from [data.gov](http://data.gov/) using Cloudant. If you want to follow along at home, you have two choices.\n\n1. Use CouchDB replication to copy my db into yours via: {% highlight tcsh %}$ curl 'http://<usr>:<pwd>@<usr>.cloudant.com/_replicate' -Hcontent-type:application/json -d '{\"source\":\"http://mlmiller.cloudant.com/planes\",\"target\":\"http://<usr>:<pwd>@<usr>.cloudant.com/planes\",\"create_target\":true}'{% endhighlight %}\n2. Repeat from scratch by [getting the code](http://github.com/mlmiller/examples/tree/master/aviation) from Github.\n\n(1) is simpler and _doesn’t require any special software installed on your machine_. All you need is to have a free account on [Cloudant](http://cloudant.com). (2) is a bit more effort but should work with any CouchDB server. If you go with (1), the best way to check the progress is to [login to your account](http://cloudant.com), click on your user dashboard (top right) and then click on the newly created ‘planes’ database. If you examine the Stats tab and refresh periodically, you will see the db size increasing, as well as some MapReduce ‘views’ running as the data imports:\n\n![](/img/planes-indexing.png)\n\nThat bar that registers 31.7% is telling you that MapReduce is being used to index your documents in real time without you having to write any code at all. The ‘2839 of 8952’ on the right represents the fraction of freshly added/updated documents that is processed in this current pass, and if you click on ‘show details’ you can see the results of the MapReduce processes on each of the cluster nodes that contain any chunk of your data. That's just how easy it is to get your code running, in parallel, in realtime!\n\n### Data Import\n\nI’ve chosen to import the FAA crash data using my favorite tools: Python and Benoit’s wonderful [Couchdbkit](http://couchdbkit.org/) library. If you use the Python [Boto](https://code.google.com/p/boto/) module for AWS, you will feel right at home. In [upload.py](https://github.com/mlmiller/examples/blob/master/aviation/upload.py), I use the very handy `DictReader` class from Python’s `csv` module to parse the `AviationData.txt.gz`. `DictReader` slurps a line of a CSV (or similarly structured text file) with column headings and gives you back a dictionary. It’s a one-liner to serialize that Python dictionary and push it into Cloudant. If you want to try this yourself the `README` file has instructions on installing the Couchdbkit module and executing the `upload.py` script, but it just boils down to:\n\n```sh\npython upload.py AviationData.txt.gz 'http://<username>:<password>@<username>.cloudant.com' planes\n```\n\nMy inported data lives at [http://mlmiller.cloudant.com/planes](http://mlmiller.cloudant.com/planes). One thing to note: you can see that I don’t take any steps to discover the type of the data on import. Instead, we just throw it all into Cloudant and we can sort it out later using MapReduce (_flexible_). Once the data is fully imported, I can login into [cloudant.com](https://cloudant.com/) and view the ‘planes’ database in the dashboard:\n\n![](/img/planes-stats.png)\n\nI’ve also gone into the Permissions tab and granted read access to the data:\n\n![](/img/planes-permissions.png)\n\nEach document contains quite a bit of data, too much to fit onto a single screen shot, in fact:\n\n![](/img/planes-documents.png)\n\nBut the important thing to note is that we have data for nearly 70,000 domestic accidents, spanning the range 1988–2010. If you want to look at a single document from your command line, you can via:\n\n```sh\n$ curl -X GET 'http://<usr>:<pwd>@<usr>.cloudant.com/planes/_all_docs?limit=1&include_docs=true'\n```\n\nand if you want it pretty printed, my favorite is:\n\n```sh\n$ curl -X GET 'http://<usr>:<pwd>@<usr>.cloudant.com/planes/_all_docs?limit=1&include_docs=true' | python -m json.tool\n```\n\nSince I've granted read permissions on mlmiller/planes, you can execute this yourself via:\n\n```\n$ curl -X GET 'http://mlmiller.cloudant.com/planes/_all_docs?limit=1&include_docs=true' | python -m json.tool\n{\n    \"offset\": 0,\n    \"rows\": [\n        {\n            \"doc\": {\n                \"\": \"\",\n                \"Accident Number\": \"LAX08CA100\",\n                \"Air Carrier\": \"\",\n                \"Aircraft Category\": \"\",\n                \"Aircraft Damage\": \"Substantial\",\n                \"Airport Code\": \"U42\",\n                \"Airport Name\": \"Salt Lake City Muni 2\",\n                \"Amateur Built\": \"Yes\",\n                \"Broad Phase of Flight\": \"\",\n                \"Country\": \"United States\",\n                \"Engine Type\": \"Reciprocating\",\n                \"Event Date\": \"04/12/2008\",\n                \"Event Id\": \"20080513X00662\",\n                \"FAR Description\": \"\",\n                \"Injury Severity\": \"Non-Fatal\",\n                \"Investigation Type\": \"Accident\",\n                \"Latitude\": \"40.619445\",\n                \"Location\": \"Salt Lake City, UT\",\n                \"Longitude\": \"-111.992777\",\n                \"Make\": \"Questair, Inc.\",\n                \"Model\": \"Venture\",\n                \"Number of Engines\": \"1\",\n                \"Publication Date\": \"05/28/2008\",\n                \"Purpose of Flight\": \"Personal\",\n                \"Registration Number\": \"N36V\",\n                \"Report Status\": \"Probable Cause\",\n                \"Schedule\": \"\",\n                \"Total Fatal Injuries\": \"\",\n                \"Total Minor Injuries\": \"2\",\n                \"Total Serious Injuries\": \"\",\n                \"Total Uninjured\": \"\",\n                \"Weather Condition\": \"VMC\",\n                \"_id\": \"0fedbabcd21f15785750457f83000947\",\n                \"_rev\": \"1-3e9637bb5214c82fc7b29bb4a75e765b\"\n            },\n            \"id\": \"0fedbabcd21f15785750457f83000947\",\n            \"key\": \"0fedbabcd21f15785750457f83000947\",\n            \"value\": {\n                \"rev\": \"1-3e9637bb5214c82fc7b29bb4a75e765b\"\n            }\n        }\n    ],\n    \"total_rows\": 68388\n}\n```\n\n### Example 1: Sort\n\nNow that we have the data, let’s get down to business. Key/Value is great, but I want to be able get find all the documents for a given brand of plane, say Cessna. This is really an inverted index, and is in the absolute sweet spot for MapReduce. We can accomplish this with a ‘map-only’ view that looks like:\n\n```js\nfunction(doc) {\n  emit(doc.Make, null);\n}\n```\n\nThe MapReduce code is actually stored in the database as part of a view in a design document (see the [CouchDB Book](http://guide.couchdb.org/editions/1/en/views.html) for more details). My favorite method is to use a client-side tool such as `couchapp` or `couchdbkit` that allows me to organize `map.js` and `reduce.js` files according to a filesystem hierarchy and then automatically upload them to the database as a design document. I actually snuck that two-liner into [`upload.py`](https://github.com/mlmiller/examples/blob/master/aviation/upload.py#L20), where I have:\n\n```py\nloader = FileSystemDocsLoader('_design/')\nloader.sync(db, verbose=True)\n```\n\nThose two lines unpack the contents of the `_design` directory and build a design document. That directory looks like:\n\n![](/img/folders.png)\n\nThis gives me a design document with the name `example`; that contains two views, one called `date` and one called `make`. You can see the entire design document from the command line via:\n\n```sh\n$ curl -X GET http://mlmiller.cloudant.com/planes/_design/example\n```\n\nor you can just browse the ‘_design’ code in the file [`_design/example/views/make/map.js`](https://github.com/mlmiller/examples/blob/master/aviation/_design/example/views/make/map.js). The code looks like this:\n\n```js\nfunction(doc) {\n  emit(doc.Make, 1);\n}\n```\n\nThe `emit()` function generates a key/value pair that gets sorted according to CouchDB's deterministic [collation order](http://wiki.apache.org/couchdb/View_collation#Collation_Specification). These key/value pairs are either stored directly in the view index, or passed through the reducer first (see below). Don’t worry about the second argument to the emit method, that’s for the the reduce phase and we don’t need that for a basic sort. The moment that we upload that design document to Cloudant, the index is kept up to date automatically. That’s quite different than the default CouchDB behavior, but it’s a nice optimization to make sure that the query response is snappy.\n\nQuery (finally!). Let’s find all of the documents (‘rows’) that have `doc.Make==\"Cessna\"`. Since we emitted `doc.Make` in our map code, `doc.Make` is therefore the ‘key’ for this index, and that means we get snappy responses to queries like:\n\n```sh\n$ curl -X GET 'http://mlmiller.cloudant.com/planes/_design/example/_view/make?reduce=false&key=\"Cessna\"&limit=10'\n{\"total_rows\":68387,\"offset\":13905,\"rows\":[\n{\"id\":\"0fedbabcd21f15785750457f8300324c\",\"key\":\"Cessna\",\"value\":1},\n{\"id\":\"0fedbabcd21f15785750457f83003a2f\",\"key\":\"Cessna\",\"value\":1},\n{\"id\":\"0fedbabcd21f15785750457f830060ec\",\"key\":\"Cessna\",\"value\":1},\n{\"id\":\"0fedbabcd21f15785750457f83006bfb\",\"key\":\"Cessna\",\"value\":1},\n{\"id\":\"0fedbabcd21f15785750457f8300a5ae\",\"key\":\"Cessna\",\"value\":1},\n{\"id\":\"0fedbabcd21f15785750457f8300e15d\",\"key\":\"Cessna\",\"value\":1},\n{\"id\":\"0fedbabcd21f15785750457f830162b7\",\"key\":\"Cessna\",\"value\":1},\n{\"id\":\"0fedbabcd21f15785750457f8301a3a4\",\"key\":\"Cessna\",\"value\":1},\n{\"id\":\"0fedbabcd21f15785750457f8301a54e\",\"key\":\"Cessna\",\"value\":1},\n{\"id\":\"0fedbabcd21f15785750457f83058740\",\"key\":\"Cessna\",\"value\":1}\n]}\n```\n\nThe full list of query options is [given here](http://wiki.apache.org/couchdb/HTTP_view_API#Querying_Options), but really there are only a handful of options I typically use. For this query, if you remove the `limit` argument you will get the entire list of crashes with a Cessna involved. If you want to do some fun statistics, you can try:\n\n```sh\n$ curl -X GET 'http://mlmiller.cloudant.com/planes/_design/example/_view/make?reduce=false&key=\"Airbus\"' | wc -l 15\n$ curl -X GET 'http://mlmiller.cloudant.com/planes/_design/example/_view/make?reduce=false&key=\"Boeing\"' | wc -l 773\n```\n\nWell that’s interesting! But wait, isn’t there a more efficient way to calculate statistics than slurping the entire set of Boeing lines to the client and doing a client-side count? You bet, and that’s where reduce comes in.\n\n### Example 2: Data Aggregation\n\nNow we want to go beyond simple sorts and aggregate to find the total number of crashes for different types of planes. Reduce is perfect for this, and that’s why we did `emit(doc.Make,1)` in `map.js`. If you look at the corresponding [`reduce.js`](https://github.com/mlmiller/examples/blob/master/aviation/_design/example/views/make/reduce.js) file for that view you’ll see nothing more than: `_sum`.\n\nWhat the heck is that? Well there are certain aggregation tasks that are simply so common (count, sum, average, min, max, etc.) that we have coded them up in erlang and pushed them into the database. Cloudant has an ever growing number of built-in reduces (_builtins_), and using a builtin is always faster and less error prone than writing it yourself. As an avid user, I can admit that I haven’t used a non-builtin reduce in at least 9 months. So the `_sum` that we use here is simply going to add up the numbers that we emitted as the 2nd argument to `emit()` in the `map.js` function. Now we can answer the Boeing vs. Airbus question without any client side work via:\n\n```sh\n$ curl -X GET 'http://mlmiller.cloudant.com/planes/_design/example/_view/make?key=\"Boeing\"'\n{\"rows\":[\n{\"key\":null,\"value\":771}\n]}\n$ curl -X GET 'http://mlmiller.cloudant.com/planes/_design/example/_view/make?key=\"Airbus\"'\n{\"rows\":[\n{\"key\":null,\"value\":13}\n]}\n```\n\nCheck that out! Hmm, now suppose I want to find the total number of accidents, I can then just do:\n\n```sh\n$ curl -X GET 'http://mlmiller.cloudant.com/planes/_design/example/_view/make'\n{\"rows\":[\n{\"key\":null,\"value\":68387}\n]}\n```\n\nVery cool, and if I want to make a graph of how they plot out vs manufacture I can add the `group=true` option:\n\n```sh\n$ curl -X GET 'http://mlmiller.cloudant.com/planes/_design/example/_view/make?group=true'\n{\"rows\":[\n{\"key\":\"\",\"value\":53},\n{\"key\":\"107.5 Flying Corporation\",\"value\":1},\n{\"key\":\"1200\",\"value\":1},\n{\"key\":\"1977 COLFER-CHAN\",\"value\":1},\n{\"key\":\"1ST FTR GP\",\"value\":1},\n{\"key\":\"2000 McCoy\",\"value\":1},\n{\"key\":\"2001 MCGIRL\",\"value\":1},\n{\"key\":\"2003 Nash\",\"value\":1},\n{\"key\":\"2007 Savage Air LLC\",\"value\":1},\n{\"key\":\"67 FLYING DUTCHMAN\",\"value\":1},\n{\"key\":\"85 MANISTA\",\"value\":1},\n{\"key\":\"A Pair of Jacks\",\"value\":1},\n{\"key\":\"A. H. GETTINGS\",\"value\":1},\n{\"key\":\"A. LE FRANCOIS\",\"value\":1},\n…\n```\n\n(truncated because it’s a looong list!)\n\nOk, so aggregation seems pretty straightforward. Now let’s go the last step to time-series.\n\n### Example 3: Time Series Analysis\n\nLet’s get a bit more numerical and ask, what’s the distribution of crashes and fatalities vs time. That’s what the `date` view is for. If you inspect any of our crash documents you will see that they have a two other important fields: `Event Date` and `Total Fatal Injuries`. Let’s create an index that allows us to answer time-based queries about both the number of accidents and the number of fatalities. While a bit morbid, it’s good to know what the odds are! This example is a bit more complex because we are going to use an array for both arguments of `emit()`. Take a look at the code in the [`map.js`](https://github.com/mlmiller/examples/blob/master/aviation/_design/example/views/date/map.js):\n\n```js\nfunction(doc) {\n    var then = new Date(Date.parse(doc['Event Date']));\n    var fatalities = 0;\n    if (doc['Total Fatal Injuries']!=\"\") {\n        fatalities = parseInt(doc['Total Fatal Injuries']);\n    }\n    emit([then.getFullYear(), then.getMonth()], [1, fatalities]);\n}\n```\n\nThis is a bit more complex because we do some processing of the data. In line #2 we unpack the `Event Date` variable into a JavaScript Date object, very handy. I find myself using `new Date(Date.parse(some_weird_timestamp))` all the time. Line #6 simply casts our fatalities info into an integer. The magic is in `emit`, where we use JavaScript arrays `[]` for both the key and the value. Then, for reduce we use good ol’ `_sum`. One of Cloudant’s additional builtins is a `_sum` that works on arrays, very handy. Now to query this index we get rows that are sorted by year, then month. The index looks like:\n\n```sh\n$ curl -X GET 'http://mlmiller.cloudant.com/planes/_design/example/_view/date?group_level=2' | more\n{\"rows\":[\n{\"key\":[1982,0],\"value\":[207,185]},\n{\"key\":[1982,1],\"value\":[232,96]},\n{\"key\":[1982,2],\"value\":[280,95]},\n{\"key\":[1982,3],\"value\":[303,113]},\n{\"key\":[1982,4],\"value\":[387,133]},\n{\"key\":[1982,5],\"value\":[349,106]},\n{\"key\":[1982,6],\"value\":[433,283]},\n{\"key\":[1982,7],\"value\":[399,96]},\n{\"key\":[1982,8],\"value\":[332,119]},\n{\"key\":[1982,9],\"value\":[239,129]},\n{\"key\":[1982,10],\"value\":[224,134]},\n{\"key\":[1982,11],\"value\":[208,96]},\n{\"key\":[1983,0],\"value\":[199,102]},\n{\"key\":[1983,1],\"value\":[210,72]},\n{\"key\":[1983,2],\"value\":[264,91]},\n…\n```\n\nSo, e.g., 1982 month 0 (January) had 207 crashes and 185 fatalities. What was that `group_level` command? Let’s try changing it:\n\n```sh\n$ curl -X GET 'http://mlmiller.cloudant.com/planes/_design/example/_view/date?group_level=1' | more\n{\"rows\":[\n{\"key\":[1982],\"value\":[3593,1585]},\n{\"key\":[1983],\"value\":[3556,1273]},\n{\"key\":[1984],\"value\":[3457,1229]},\n{\"key\":[1985],\"value\":[3096,1648]},\n{\"key\":[1986],\"value\":[2880,1180]},\n{\"key\":[1987],\"value\":[2828,1237]},\n…\n```\n\nWell that’s interesting! `group_level=1` vs `2` has a big effect: with `group_level=2` we get the full granularity/dimensionality of the array we used for the key (first argument to `emit`). But we can, at query time, reduce even further by integrating over month! Very cool, and the gory details are few: `group_level` runs between `0` and `length(key)`, so in this case valid values are 0, 1, 2. If we specify `group_level=0`, we’ll get sum statistics over the entire date range:\n\n```sh\n$ curl -X GET 'http://mlmiller.cloudant.com/planes/_design/example/_view/date?group_level=0'\n{\"rows\":[\n{\"key\":null,\"value\":[68387,38652]}\n]}\n```\n\nAwesome. And therein lies the power of using MapReduce for an efficient index build. With Cloudant you don’t need to install any software, you simply use the cloud hosted cluster to run your code. If you think about it, all we really had to do was write 9 lines of javascript for the map method, and no original code for the reduce method. We can use this same exact index to query for a given date range, say 1994:\n\n```sh\n$ curl -X GET 'http://mlmiller.cloudant.com/planes/_design/example/_view/date?group_level=2&startkey=\\[1994\\]&endkey=\\[1995\\]'\n{\"rows\":[\n{\"key\":[1994,0],\"value\":[116,60]},\n{\"key\":[1994,1],\"value\":[142,42]},\n{\"key\":[1994,2],\"value\":[191,67]},\n{\"key\":[1994,3],\"value\":[179,72]},\n{\"key\":[1994,4],\"value\":[224,74]},\n{\"key\":[1994,5],\"value\":[223,98]},\n{\"key\":[1994,6],\"value\":[267,131]},\n{\"key\":[1994,7],\"value\":[260,101]},\n{\"key\":[1994,8],\"value\":[212,200]},\n{\"key\":[1994,9],\"value\":[169,129]},\n{\"key\":[1994,10],\"value\":[131,79]},\n{\"key\":[1994,11],\"value\":[143,130]}\n]}\n```\n\nor if we want to do can turn off the reduce in the query and get all of the crashes from a given year/month combo:\n\n```sh\n$ curl -X GET 'http://mlmiller.cloudant.com/planes/_design/example/_view/date?key=\\[1994,4\\]&reduce=false' | more\n{\"total_rows\":68387,\"offset\":34959,\"rows\":[\n{\"id\":\"88601d815897032516f87f0c510c7c60\",\"key\":[1994,4],\"value\":[1,1]},\n{\"id\":\"88601d815897032516f87f0c510c87b7\",\"key\":[1994,4],\"value\":[1,0]},\n{\"id\":\"88601d815897032516f87f0c510c951a\",\"key\":[1994,4],\"value\":[1,2]},\n{\"id\":\"88601d815897032516f87f0c510c9b0a\",\"key\":[1994,4],\"value\":[1,0]},\n{\"id\":\"88601d815897032516f87f0c510c9bbb\",\"key\":[1994,4],\"value\":[1,0]},\n{\"id\":\"88601d815897032516f87f0c510c9c15\",\"key\":[1994,4],\"value\":[1,1]},\n{\"id\":\"88601d815897032516f87f0c510c9d6d\",\"key\":[1994,4],\"value\":[1,0]},\n{\"id\":\"88601d815897032516f87f0c510ca925\",\"key\":[1994,4],\"value\":[1,0]},\n…\n```\n\nWow, handy! And if you want to really blow your mind, add the query-string parameter `?include_docs=true` to get the full documents themselves.\n\n### Summary\n\nThere’s a lot of power in MapReduce, and I hope I’ve given you enough to get your hands dirty running it an an extremely simple way using a hosted service. If you have more questions, there are lots of examples on the [CouchDB wiki](http://wiki.apache.org/couchdb/) or on the ([Cloudant support center](https://cloudant.tenderapp.com)). But the rule of thumb is:\n\n* **sort:** map only\n* **aggregate:** use a `_sum` reduce + `group_level`\n* **time-series:** use an array of `[year, month, day, …]` + `group_level`\n\nIn the next post, we’ll show you how to link Cloudant MapReduce views with builtin full-text/numerical search for killer data visualizations.\n",
    "url": "/2011/01/13/mapreduce-from-the-basics-to-the-actually-useful.html",
    "tags": "MapReduce",
    "id": "1"
  },
  {
    "title": "Introduction to Conflicts - 1/3",
    "description": "Cloudant document conflicts - what are they?",
    "content": "\n\n\nThis is the first of a three-part blog series on how to deal with conflicts in the IBM Cloudant JSON document store. This blog assumes you have a working knowledge of Cloudant's database, and its API.\n\nIn part one, we introduce the concept of a document conflict, describe what it looks like, and explain what happens if conflicts are left unresolved. Later in this series, we show how to tidy up conflicts, and discuss how they can be avoided.\n\n![conflict]({{< param \"image\" >}})\n> Photo by [Frida Bredesen on Unsplash](https://unsplash.com/photos/76dgUcMupv4)\n\n## Introduction\n\nCloudant allows data to be stored as JSON documents in a highly-resilient cluster of connected nodes. Cloudant's HTTP interface can be accessed natively through a web browser, through an interactive dashboard, or through programming languages for most platforms. Cloudant is a distributed system, with Replication at it's heart. It enables mobile developers to replicate their data to portable devices, and allows data to be replicated across the globe. The effect is similar to a content delivery network (CDN) for your data.\n\nThe [Consistency, Availability and Partition tolerance (CAP)](https://en.wikipedia.org/wiki/CAP_theorem) theorem states that a distributed system can have only two of the three characteristics. In Cloudant, Consistency is sacrificed in favour of availability and partition tolerance. Specifically, Cloudant is an eventually consistent database, which means that no locking occurs when data is written. This characteristic enables the system to offer best-of-class uptime and scalability.\n\nIn order to keep the service available at all times, Cloudant must allow the same document id to be altered on different nodes in the database. To reconcile the data, Cloudant maintains a revision history for every document in the database. This is a timeline of changes to the document; not the document body itself, only a history of the revision tokens:\n\n![one](/img/Conflicts-Part-One-1.png)\n\nIn this example, document \"abc\" has had three revisions. The revision token is made up of a sequential number and a hash of the document content. For convenience, a shortened version of the hash appears in the diagram.\n\nOne of the consequences of eventual consistency is that documents might enter a conflicted state if the **same version of a document is modified in different ways on two disconnected nodes**. An exception is where the change made on the two nodes is the same change, to the same revision of the document. In this case, no conflict is generated, because the hash remains the same.\n\n![two](/img/Conflicts-Part-One-Introduction-to-Cloudant-and-documnet-conflicts-21.png)\n\nIn the above example there are two Cloudant databases (A and B) that are not connected. They both have the same document, with identical revision histories for revisions 1 and 2. At revision 3, the databases diverged. If we replicate database A to B, or alternatively replicate database B to A (in Cloudant, bi-directional replication is simply two separate replication processes in opposite directions), then the document enters a conflicted state:\n\n![three](/img/Conflicts-Part-One-3.png)\n\n## How Can Conflicts Arise in Your Application?\n\nThe three scenarios below are essentially descriptions of the same thing, but with different application architectures:\n\n### Mobile apps\n\nMany Cloudant customers create one database for every one of their users. This architecture is especially suited to mobile app developers as it allows the app to continue to function offline and can sync its data to the cloud whenever it is connected. A document might become conflicted if it is modified on the app (via the phone) and on Cloudant itself (via a web dashboard, for example) and then the two copies are subsequently synced.\n\n### Replication\n\nA Cloudant customer might have two clusters hosted in separate geographic locations. The clusters are connected by continuous replication. If the same document is modified in each cluster while the inter-site connection is down, a conflict is recorded when the clusters are reconnected.\n\n### Race condition\n\nEven in a small Cloudant cluster, a conflict can arise if changes to the same document are sent to two nodes at the same time.\n\n## What Does A Conflict Look Like?\n\nNormally, when retrieving a single document, we would receive only the latest revision:\n\n```\nGET /mydb/0f900fc85f2c5249759d9dd939b9c080 \n{\n  \"_id\": \"0f900fc85f2c5249759d9dd939b9c080\",\n  \"_rev\": \"3-cb1624f72667f6f0378d628e0e065f24\",\n  \"name\": \"Glynn Bird\",\n  \"age\": 24\n }\n ```\n\nIf we additionally pass in the parameter ?conflicts=true, Cloudant will return the document and a list of conflicting revision tokens:\n\n```\nGET /mydb/0f900fc85f2c5249759d9dd939b9c080?conflicts=true \n{\n  \"_id\": \"0f900fc85f2c5249759d9dd939b9c080\",\n  \"_rev\": \"3-cb1624f72667f6f0378d628e0e065f24\",\n  \"name\": \"Glynn Bird\",\n  \"age\": 24,\n  \"_conflicts\": [\n    \"3-ba7697cffda8cdfdfc63267473ffaf7d\"\n  ]\n}\n```\n\nIf no _conflicts parameter is returned, then the document is conflict-free.\n\n## What Happens If I Ignore Conflicts In My Database?\n\nCloudant continues to serve out the documents as before, but if a conflict occurs:\n\n- Cloudant returns what it considers to be the \"winning\" revision while retaining the \"non-winning\" revisions internally. The algorithm that determines the winner is deterministic; different nodes with the same conflict scenario will return the same winner but the revision that Cloudant considers to be the winner may not be your idea of the winner. Your application should either resolve the conflicts as they arise to publish the document that your application needs, or should adopt a design pattern that avoids the generation of conflicts in the first place.\n- the database's size is inflated because Cloudant keeps the bodies of unresolved conflicts in full.\n- performance suffers if there are many conflicts in the same document.\n\nIt is good practice, as an application developer, to deal with any conflicts that arise in your documents. The benefit is a reduction in data size, and an optimised performance.\n\nIn [Part Two]({{< ref \"/2015-01-20-Introduction-to-Conflicts-Part-Two.md\" >}}) of this series, we'll return to this subject, and show how conflicts can be dealt with in your application.",
    "url": "/2015/01/12/Introduction-to-Conflicts-Part-One.html",
    "tags": "Conflicts",
    "id": "2"
  },
  {
    "title": "Introduction to Conflicts - 2/3",
    "description": "Detecting and resolving Cloudant document conflicts.",
    "content": "\n\n\nIn [Part One]({{< ref \"/2015-01-12-Introduction-to-Conflicts-Part-One.md\" >}}) of this series we looked at:\n\n- what are document conflicts in Cloudant?\n- how do they arise?\n- what does a conflict look like?\n- the consequences of conflicts\n\nIn this blog we will discuss how we can detect conflicts and how we should go about resolving them.\n\nConflicts are most often a natural side-effect of having a distributed database architecture. Conflicts retain the history of a document, keeping versions of a document that has been modified in different ways on two independent systems (e.g on a mobile app & on the server). It is the application's responsibility to detect the conflicts and resolve them.\n\n![fox]({{< param \"image\" >}})\n> Photo by [CloudVisual on Unsplash](https://unsplash.com/photos/DCtwjzQ9uVE)\n\n## Detecting Conflicts - Piecemeal\n\nWhen fetching single documents, simply adding `?conflicts=true` will ask Cloudant to additionally return a `_conflicts` key, listing conflicting revisions:\n\n```\nGET /mydb/0f900fc85f2c5249759d9dd939b9c080?conflicts=true\n\n{\n    \"_id\": \"0f900fc85f2c5249759d9dd939b9c080\",\n    \"_rev\": \"3-cb1624f72667f6f0378d628e0e065f24\",\n    \"name\": \"Glynn Bird\",\n    \"age\": 24,\n    \"_conflicts\": [\n        \"3-ba7697cffda8cdfdfc63267473ffaf7d\"\n    ]\n}\n```\n\nIf we fetch the document again, but this time with the parameter `?open_revs=all`, Cloudant will return an array of the conflicting documents, including their bodies:\n\n```\nGET /mydb/0f900fc85f2c5249759d9dd939b9c080?open_revs=all\n[\n  {\n    \"ok\": {\n      \"_id\": \"0f900fc85f2c5249759d9dd939b9c080\",\n      \"_rev\": \"3-ba7697cffda8cdfdfc63267473ffaf7d\",\n      \"name\": \"Glenn Bard\",\n      \"age\": 24\n    }\n  },\n  {\n    \"ok\": {\n      \"_id\": \"0f900fc85f2c5249759d9dd939b9c080\",\n      \"_rev\": \"3-cb1624f72667f6f0378d628e0e065f24\",\n      \"name\": \"Glynn Bird\",\n      \"age\": 24\n    }\n  }\n]\n```\n\n## Detecting Conflicts – Bulk\n\nTo detect which of the documents in your database are conflicted requires a map/reduce view to be created:\n\n```\nmap:\n  function(doc) {\n    if (doc._conflicts) {\n      emit(null, null);\n    }\n  }\n\nreduce:\n  _count\n```\n\nWe can then use this view to:\n\n- determine the number of documents in conflict\n- fetch a list of document ids whose conflicts need resolving\n\nFor example, a script could page through the documents in the view, resolving the conflicts as it goes.\n\n## How Do I Resolve A Conflict?\n\nResolution of a conflict is to:\n\n- delete the conflicting revisions\n- optionally, post a new version of the document (e.g. a document that you consider to be the winner)\n\n![one](/img/Conflicts-Part-Two-Resolving-doc-conflicts-1.png)\n\nIn the above example, if we decided that \"3-uvwx\" should be the winning revision, all we would have to do is delete revision \"3-qrst\" which would leave the document without conflicts, restoring revision \"3-uvwx\" to the head of the revision list (as it is the only remaining \"revision 3\"):\n\n```\nDELETE \"/test/abc?rev=3-qrst\"\n```\n\n![two](/img/Conflicts-Part-Two-Resolving-doc-conflicts-2.png)\n\n## Resolving Conflicts In Bulk\n\nIn more complicated examples, where there are lots of conflicts to fix, it is more efficient to delete the unwanted revisions in bulk:\n\n```\nPOST /test/_bulk_docs\n{ \"docs\": [ \n    { \"_id\": \"abc\", \"_rev\": \"3-uvwx\", \"_deleted\": true}, \n    { \"_id\": \"abc\", \"_rev\": \"2-qrst\", \"_deleted\": true} \n  ]\n}\n```\n\nN.B. this technique is only useful if you are deleting conflicting revisions that are not the \"winning\" revision and should be limited to a maximum of 500 deletions per request.\n\n## Conflict Resolution Strategies\n\nMost often, resolution of a conflict isn't as simple as removing old conflicting revisions; that is simply destroying data. What if the old revisions contain something you want to keep? What if you need to merge two documents together?\n\nThat is where Cloudant respectfully says \"that is an application-specific\" problem. In other words, Cloudant will never try to merge two JSON documents together to form a hybrid.\n\nImagine an email application. There is a mobile application with a synced copy of an inbox and a copy of the same inbox on the server. What should happen if an item is marked \"read\" on the server and \"unread\" on the phone? The answer is that only your application can make that decision.\n\nYour application should have a conflict resolution algorithm (perhaps with a suite of automated tests); a sandbox where all of the possible conflict scenarios can be simulated and solved in a way that makes sense to your application. Options include:\n\ntimestamp each document. When a conflict arises, always favour the most recent change\nif your application has different levels of user access, discard 'standard' user edits over 'admin' user changes\nif your document schema is suitable, data can be merged from conflicting revisions\nor a combination of the above; it's up to you\nConflicts are not an error condition, they are the result of your infrastructure allowing the same data set to be modified across disconnected systems. The introduction of such conflicts in such a topology is the expected behaviour and their programmatic resolution is a core piece of application logic.\n\n\n## But I'm Getting Conflicts And I'm Not Even Using Replication!\n\nEven on a Cloudant cluster, with no replication to remote databases, conflicts can arise if the same document is updated in different ways on two nodes, before the two nodes have had chance to communicate with each other.\n\nThe solution to that problem is subject of the [Part Three]({{< ref \"/2015-01-26-Introduction-to-Conflicts-Part-Three.md\" >}}) of this series, we'll look at ways you can avoid conflicts arising in the first place by employing conflict-proof design patterns.",
    "url": "/2015/01/20/Introduction-to-Conflicts-Part-Two.html",
    "tags": "Conflicts",
    "id": "3"
  },
  {
    "title": "Introduction to Conflicts - 3/3",
    "description": "Avoiding document conflicts through design.",
    "content": "\n\n\nIn [Parts One]('2015/01/12/Introduction-to-Conflicts-Part-One.html') and [Two]('2015/01/20/Introduction-to-Conflicts-Part-Two.html') of this series, we looked at:\n\n- what are document conflicts in Cloudant?\n- how do they arise?\n- what does a conflict look like?\n- the consequences of conflicts\n- how to detect conflicts singly and in bulk\n- how to resolve conflicts\n\nIf your application replicates data between Cloudant and mobile device and the data is allowed to be modified (like an email inbox), then conflicts will arise in your application when the two replicas are combined, and the resolution of those conflicts is your application's responsibility.\n\nSome Cloudant users are surprised to find conflicts arising in their application even when they are not replicating to and from a remote database. It is this manifestation of conflicts that we will be addressing in this post.\n\nBut first, we should answer the question \"what is a 409\"?\n\n![fox]({{< param \"image\" >}})\n> Photo by [jean wimmerlin on Unsplash](https://unsplash.com/photos/e1daGOrmkIk)\n\n\n## I'm Getting A HTTP 409 Error When I Try To Write A Document\n\nImagine your application pulls a document from a Cloudant database (say revision 1), makes a change to the document and posts the new document back to Cloudant, but Cloudant replies back with a HTTP 409 status code and the message\n\n```\n{\"error\":\"conflict\",\"reason\":\"Document update conflict.\"}\n```\n\nThis means that the document (id + revision) you are trying to edit has already been updated and your change has been rejected. This isn't a conflict as such (the conflict is not stored in the database), it is Cloudant preventing a conflicting revision from happening in the first place by indicating to you that you are trying to modify an old version of the document.\n\nThis set of circumstances is visualised in the diagram below:\n\n![one](/img/Conflicts-Part-Three-Preventing-conflicts.png)\n\nIn the time it has taken Rita to fetch the document and post a new revision, Sue has beaten her to it and posted her own change first. In order for Rita to commit her change, she needs to:\n\n- pull the document again, to get the latest revision (revision 2)\n- apply the change to the document (assuming it doesn't clash with Sue's change)\n- write a third revision of the document\n\nIf this is happening frequently, or indeed if you find you're modifying the same document over and over, then you may want to consider a different approach.\n\n## Preventing Conflicts By Design\n\nIf your application is the recording of website events in a Cloudant database, it is tempting to design a schema like this:\n\n```js\n{\n  \"website\": \"http://mydomain.com\",\n  \"impressions\": 70252,\n  \"visitors\": 1556,\n  \"avg_page_delivery_time\": 0.52\n}\n```\n\nAs each page view occurs, you increment the \"impressions\" element of the document. This is a bad way to store data in Cloudant because:\n\n- every write requires the client to read the document before it can be updated\n- with even a moderate amount of traffic, 409s will start to occur as reads and writes overlap each other\n- conflicts will be created and stored because different Cloudant nodes will accept a revision before data is synced between nodes\n\nA better way to store such data is to store one document per event:\n\n```js\n{\n   \"date\": \"2014-11-01 23:59:02 UTC\",\n   \"website\": \"http://mydomain.com\",\n   \"event_type\": \"impression\",\n   \"delivery_time\": 0.3\n}\n```\n\nWe can then create a design document to create materialized views of data. e.g.\n\n- page impression counts grouped hierarchically by year/month/day/hour etc\n- average/min/max page delivery times by time\n- total traffic breakdown by site and time\n\nStoring the events is simplified because data is only ever written to the database meaning that document conflicts can never occur. This write-only design pattern is used widely with Cloudant e.g.\n\n- a blogging platform, instead of having one document per post (a document which is updated every time the post is modified), could have a document for each version of the post. Not only does this avoid conflicts, it gives a persistent revision history for each post. (Although Cloudant maintains revision tokens for each document change, document revisions cannot be relied on to provide rollback or a full revision history)\n- a cache framework, instead of having one document per cache key (which is updated as the key's value changes), could have one document for each version of the key. A timestamp in the document would allow the latest key/value pair to be extracted.\n\n## Conclusion\n\nCloudant is a distributed database built to store data at a massive scale, whether through volume of data or concurrent user count. To be able to scale out horizontally, Cloudant imposes no locking of documents between nodes. Each shard of the database acts independently of each other, allowing your service to be always-available and able to survive a network partition.\n\nA by-product of this is that when disconnected nodes finally communicate, a document may have been modified in a different way on separate nodes. A conflict is stored in the document, with a copy of each competing revision and the application is able resolve the conflict with no data loss.\n\nMobile applications need this flexibility because if a mobile phone had to connect to the internet for every operation then it would not be able to function offline, in a tunnel or with flaky reception. Cloudant's conflict storage mechanism allows you to build intelligent mobile applications, collecting data offline and syncing with the cloud when an internet connection is available.\n\nIn some cases, document conflicts can be avoided altogether by adopting a write-only design pattern and using Cloudant Query or Map/Reduce views to aggregate the data on a massive scale.",
    "url": "/2015/01/26/Introduction-to-Conflicts-Part-Three.html",
    "tags": "Conflicts",
    "id": "4"
  },
  {
    "title": "Slack to Cloudant",
    "description": "With Slack slash commands",
    "content": "\n\n\nSlack’s Integration API and Cloudant’s HTTP API make it simple to store data directly into a Cloudant database without breaking a sweat. This tutorial shows how to create a custom slash command in Slack and how to post it directly to Cloudant.\n\nSlack is a messaging and team-working application that is used widely to allow disparate teams of people to chat, share files, and interact on desktop, tablet, and mobile platforms. We use Slack in IBM Cloud Data Services to coordinate our activities, to work in an open collaborative environment, and to cut down on email and meetings.\n\nOne of the strengths of Slack is that it integrates with other web services, so events happening in Github or Stack Overflow can be surfaced in the appropriate Slack channels. Slack also has an API that lets you create custom integrations. The simplest of these is slash commands: when a user starts a Slack message with a forward slash followed by a command string, Slack can be configured to POST that data to an external API. Say you create the slash command /lunch. A user could type:\n\n```\n/lunch pepperoni pizza and fanta\n```\n\nWhen they do so, Slack makes an HTTP POST to any URL you specify with some data that identifies who issued the command, in which channel:\n\n```\ntoken=kt1207qsOYtAVDrkt1207qsOYtAVDr\nteam_id=T0001\nteam_domain=example\nchannel_id=C2147483705\nchannel_name=test\nuser_id=U2147483697\nuser_name=Steve\ncommand=/lunch\ntext=pepperoni pizza and fanta\n```\n\nAll you need to do is create an API service, build an application, handle the post, and then store the data in your database.\n\n![](/img/sl_1.png)\n\nBut there is a simpler way. Because IBM Cloudant is a cloud-based, NoSQL database that has an HTTP API, you can cut out the middle layer and post data directly into Cloudant:\n\n![](/img/sl_2.png)\n\nThis blog post shows you how to create a new Slash Command from scratch and how to get the data stored in Cloudant.\n\nCreate the command in Slack\n\n- Log into the Slack website.\n- On the top left of the screen, click the menu and choose Configure Integrations.\n- Scroll to the bottom of the page and click Slash Commands.\nTip: You can also get here via this URL: https://YOURACCOUNTNAME.slack.com//services/new/slash-commands\n\nThe following form appears:\n\n![](/img/sl_3.png)\n\n- Enter the command you want to create, like /lunch\n- Click Add Slash Command Integration.\n\nSlack sets up this service, and shows you the token that will be transmitted in every post originating from this Slack command. Make a note of this token, as we’ll need it later when we set up Cloudant.\n\n![](/img/sl_4.png)\n\nWe will come back to this page shortly, after we’ve set up Cloudant.\n\n## Set up Cloudant\n\nSign up for a Cloudant account and log into your Cloudant Dashboard. In the Dashboard, create a new database called 'slack':\n\n![](/img/sl_5.png)\n\nAs Slack doesn’t integrate directly with the CouchDB/Cloudant API, we need to direct the posted data to an Update Handler Function. This is a mechanism in CouchDB/Cloudant that allows custom HTTP requests to be handled by a JavaScript before being written to the database. In our case we want to only accept requests that contain the correct Slack security token, so our code looks like this:\n\n```js\nfunction(doc, req) {\n  // only accept valid requests containing the token\n  if (req.form.token == 'YOUR_REV_TOKEN') {\n \n    // create a new object with the data to save\n    var d = req.form;\n \n    // remove the token, so it's not saved\n    delete d.token;\n \n    // add a timestamp\n    d.timestamp = new Date().getTime();\n \n    // and a unique id\n    d._id = req.uuid;\n \n    // instruct Cloudant to save it\n    return [d, JSON.stringify({ok: true})];\n \n  } else {\n \n    // don't save anything\n    return [null, JSON.stringify({ok: false})];\n  }\n};\n```\n\nConverting that function into a JSON string, we get the following code. In Cloudant, create a Design Document in your slack database, and copy this code into it, replacing YOUR_REV_TOKEN with the token you got from Slack:\n\n```js\n{\n  \"_id\": \"_design/slack\",\n  \"updates\": {\n    \"slashcommand\": \"function (doc, req) { n  if (req.form.token == 'YOUR_REV_TOKEN') {  n    var d = req.form;  n    delete d.token; n    d.timestamp = new Date().getTime(); n    d._id = req.uuid;n    return [d, JSON.stringify({ok: true})]; n  } else {n    return [null, JSON.stringify({ok: false})];n  }n}\"\n  }\n}\n```\n\n![](/img/sl_6.png)\n\nThe last thing to do on the Cloudant side is create a Cloudant API key that has write access to the slack database. In your Cloudant Slash database, click Permissions, then click Generate API Key. Turn on the Writer checkbox for the newly generated key. Make a note of the API Key and password for the final step.\n\n![](/img/sl_7.png)\n\n## Finish off the Slack integration\n\nBack in the Slack Integration Settings page, enter the URL of the Cloudant update handler, substituting the placeholder text in capitals with the details from Cloudant:\n\n```\nhttps://APIKEY:PASSWORD@SUBDOMAIN.cloudant.com/slack/_design/slack/_update/slashcommand\n```\n\n![](/img/sl_8.png)\n\nThen click Save Integration.\n\n## Test!\n\nNow in any Slack channel, you should be able to issue a /lunch command:\n\n![](/img/sl_9.png)\n\nAnd see the reply that was returned by the Cloudant Update Handler function:\n\n![](/img/sl_91.png)\n\n## Conclusion\n\nWith a simple Update Handler function, you can configure slack to store data in Cloudant whenever a user issues a custom slash command. What will you do with the data? Create a lunch-ordering system, time-keeper, bug-tracker, or anything else you can dream up. It’s up to you!\n",
    "url": "/2015/08/13/Slack-to-Cloudant.html",
    "tags": "Slack",
    "id": "5"
  },
  {
    "title": "Command-line tools",
    "description": "Backup, shell, migration and more",
    "content": "\n\n\nSome developers spend their days dragging and dropping in a graphical user interface, others are more comfortable typing green letters into a black background on a command-line terminal. If you are the latter type of developer, then this blog post is for you. We introduce a range of command-line tools that you can use to interface with [IBM Cloudant](https://cloudant.com/) (or plain [Apache CouchDB](http://couchdb.apache.org/)).\n\nCloudant and CouchDB share an RESTful HTTP API allowing access from any programming language or from the command-line using the [curl](http://curl.haxx.se/) utility. The packages featured in this blog post are all free to download and open-source, allowing you to fork and modify them for your own purposes.\n\n## ccurl\n\nMost Cloudant & CouchDB developers use [curl](http://curl.haxx.se/) to access the RESTful HTTP API. The trouble with curl is that the commands can get overly long, with lots of repetition between commands, for example:\n\n```sh\ncurl -X POST -H 'Content-type: application/json' -g \"https://myusername:mypassword@myhost.cloudant.com/mydb\"\n  -d '{\"val\": \"json\"}'\n```\n\nThe utility [ccurl](https://www.npmjs.com/package/ccurl) is a wrapper around [curl](http://curl.haxx.se/) that removes some of the repetition. It adds the following features:\n\n* The protocol, username, password and hostname are not required; instead they are taken from an environment variable.\n* The content-type header \n* The [\"-g\" fix](http://glynnbird.tumblr.com/post/61760654532/making-curl-play-nice-with-couchdb) \n\n\nConfiguring *ccurl* is a one-off task: simply set your Cloudant or CouchDB URL as the `COUCH_URL` environment variable:\n\n```sh\nexport COUCH_URL=\"https://myusername:mypassword@myhost.cloudant.com\"\n```\n\nor\n\n```sh\nexport COUCH_URL=\"http://localhost:5984\"\n```\n\n*ccurl* makes command-line API requests much less verbose, as these examples show: \n\n```sh \n# fetch stats about database ‘mydb’\nccurl /mydb\n\n# fetch single document\nccurl /mydb/12345\n\n# add a document\nccurl -X POST -d'{\"a\":1,\"b\":2}' /mydb\n```\n\n| Name        | Description               | URL                                    | Installation                   |\n| ",
    "url": "/2015/10/19/Command-line-tools-for-Cloudant.html",
    "tags": "CLI",
    "id": "6"
  },
  {
    "title": "Understanding Read/Write",
    "description": "Understanding how reads/writes work in a cluster",
    "content": "\n\n\nCouchDB 2.0 has clustering code contributed by Cloudant, which was inspired by Amazon's [Dynamo paper](http://www.allthingsdistributed.com/2007/10/amazons_dynamo.html).\n\nWhen using CouchDB in a cluster, databases are sharded and replicated. This means that a single database is split into, say, 24 shards and each shard is stored on more than one node (replica). A shard contains a specific portion of the documents in the database. A [consistent hashing](https://en.wikipedia.org/wiki/Consistent_hashing) technique is used to allocate documents to shards. There are almost always three replicas; this provides a good balance of reliability vs. storage overhead.\n\nRead and write behaviour differs in the clustered database because data is stored on more than one database node (server). I'm going to try to explain what the behaviour is and why it is how it is.\n\nTo simplify the explanation, I'm going to assume a database with a single shard, though still with three replicas. This just makes the diagrams simpler, it doesn't change behaviour. In this case we have a three node cluster, so the database has a replica on every database node in the cluster.\n\n![rw 1](/img/rw1.jpg)\n\n## Behaviour in a healthy cluster\n\nTo start with, in a healthy cluster, a read or write proceeds like this:\n\n1. Client request arrives at the cluster's load balancer.\n2. The load balancer selects a node in the database cluster to forward the request to. This can be any node. The selected node is called the coordinator for this request.\n3. Each node maintains a shard map which tells the node where to find the shard replicas for a given database, and what documents are found in each shard. The coordinator uses its shard map to work out which nodes hold the documents for the request.\n4. The coordinator makes a request to each node responsible for holding the data required by the request. This might be a read request, write request, view request etc. Note: if the coordinater holds a required shard replica, one of these requests is to itself.\n5. The coordinator waits for a given number of responses to arrive from the nodes it contacted. The number of responses it waits for differ based on the type of request. Reads and writes wait for two responses. A view read only waits for one. Let's call the number needed R (required responses). A few requests allow this to be customised using parameters on the request, but the defaults are almost always most appropriate.\n6. The coordinator processes the responses from the nodes it contacted. This can be a varying amount of work. For reads and writes, it just has to compare the responses to see if they are the same before returning a result to the client. For a view request, it might have to perform a final reduce step on the results returned from the nodes.\n7. Finally, the coordinator passes the response back to the load balancer which passes the response back to the client.\n\n![rw 2](/img/rw2.jpg)\n\nIn this article, we're interested in what happens when R responses are not received in step (5). CouchDB indicates this via a response's HTTP status code for some requests. For other requests, it's not currently possible to tell whether `R` responses were received.\n\n## Behaviour in a partitioned cluster\n\nThe behaviour of the cluster is similar whether it's partitioned or a node has been taken offline. A partitioned cluster just makes life easier for me because I can illustrate more scenarios with a single diagram.\n\nOur scenario is:\n\n- A cluster with three nodes which is split into two partitions, A and B.\n- Partition A contains two nodes, and therefore two replicas of the data.\n- Partition B contains one node, and therefore one replica of the data.\n- For whatever reason, the load balancer from the cluster can talk to both partitions, but the nodes in each partition are isolated from each other.\n- We assume the nodes know that the partition has occurred. Details on the converse state are below.\n- We further assume that the nodes on each side of the partition operate correctly. In particular, that they're able to promptly reply to the coordinators requests. The reason for this will become clear later.\n\n![rw 3](/img/rw3.jpg)\n\nFrom the above description of the read and write path, it should be clear that in this scenario some reads and writes will be allocated to coordinator nodes in Partition A and some to the node in Partition B.\n\n## Reads and writes to Partition A\n\nAs noted above, the default value for `R` is 2 for reads and writes to the database. For searches, view reads and Cloudant query requests `R` is implicitly 1. It should be clear that all reads and writes to Partition A will be able to receive at least `R` responses in all cases. In this case, the responses to the client will be as follows:\n\n- A read or query will return `200` HTTP code along with the data.\n- A write will return a `201 Created` HTTP code to indicate it wrote the data to `R` (or more) replicas.\n\n## Reads and writes to Partition B\n\nPartition B is more interesting. It's clear we can still meet the implicit `R` for searches, view reads and Cloudant Query requests. However, the nodes in Partition B cannot meet the two responses required for `R` for document read and write operations. So what do we do? In essence, the available node will do its best to read and write the data. Specifically, it will read from and write to its own replica of the data. Its returned HTTP status codes and body data are:\n\n- A read will still return `200` along with the latest version of the document or view/search/query data the node has (read from the single replica). Currently there is no way to tell that fewer than `R` replies were received from nodes holding replicas of shards.\n- A write will return a `202 Accepted` to indicate that the coordinator received a reply from at least one replica (i.e., the one in Partition B, itself) but fewer than `R` replies.\n\n## A note on responses\n\nIn the above, it's easy to overlook the fact that `R` is all about replies to the coordinator's requests. For writes in particular, this has some ramifications that it's important to take note of.\n\n1. A `202` could be received in a non-partitioned cluster. For some reason a replica might receive the write request from the coordinator node, write the data but for some reason not respond in a timely manner. Responses to a client therefore guarantee only a lower-bound to the number of writes which occurred. While there may have been `R` (or more) successful writes, the coordinator can only know about those for which it received a response. And so it must must return `202`.\n2. Interestingly, this also means writes may happen where the coordinator receives no responses. The client receives a `500` response with a `reason: timeout` JSON body, but the cluster has still written the data. Or perhaps it didn't -- neither the coordinator nor therefore the client can know.\n3. For reads, the behaviour differs slightly when the coordinator knows that it cannot possibly receive `R` responses. In that case, it returns when it has received as many as it is able to -- in Partition A this is two, in Partition B this is one. If, instead, nodes are just slow to respond, if the coordinator doesn't receive R responses before its timeout, it will return a `500`. See this [mailing list thread](https://mail-archives.apache.org/mod_mbox/couchdb-dev/201510.mbox/%3CCAJ1bcfFO1H+e-j=fmo0DAAjKmHnwnB6pKfg7UhH5wK0UxhhgPw@mail.gmail.com%3E).\n\nThis all illustrates that it's essential to remember that R is all about responses received by the coordinator, and not about, for example, whether data was written or not to a given number replica. This is why it was called out in the description of the partitioned cluster that nodes reply promptly.\n\n## Concurrent modifications\n\nAs time goes by, it's clear from the above that CouchDB 2.0 will accept writes to the same document on both sides of the partition. This will mean the data on each side of the partition will diverge.\n\nFor writes to different documents, all will be well. When the partition heals, nodes will work out amongst themselves which updates they are missing and update themselves accordingly. Soon all nodes in the cluster will be consistent with each other, and fully up-to-date.\n\nFor changes made to the same document on both sides of the partition, the nodes will reconcile divergent copies of a document to create a *conflicted document*, in exactly the same manner as happens when remote CouchDB instances replicate with each other. Again, all nodes will become consistent; it's just some documents are conflicted in that consistent state. It is the user's responsibility to detect and fix these conflicts.\n\nThere is a brief discussion of resolving conflicted documents in the [CouchDB docs](http://docs.couchdb.org/en/1.6.1/replication/conflicts.html#working-with-conflicting-documents). In essence, the cluster will continue to operate, returning one of the versions of the document that was written during the partition. The user must provide the logic for merging the changes made separately during the partition.\n\n## Conclusion\n\nThis short discussion shows that CouchDB 2.0's clustering behaviour favours availability of data for both reading and writing. It also describes the coordinating node's behaviour in the face of partitions and how it indicates this to the client application, in particular how and why it is related *purely to the number of responses received* during operations.\n\nIt's important to understand this, and what happens as data diverges on the different sides of a network partition -- and how to fix the results of this divergence -- in order to create well-behaved applications. However, this behaviour does allow the building of extremely robust programs with the careful application of knowledge and conflict handling code.",
    "url": "/2015/10/19/Read-Write-Behaviour-in-a-cluster.html",
    "tags": "Cluster",
    "id": "7"
  },
  {
    "title": "Understanding Indexing",
    "description": "Understanding how indexes work in a cluster",
    "content": "\n\n\nWhile less convoluted than the [read and write behaviour in a cluster](https://dx13.co.uk/articles/2015/10/19/couchdb-20s-read-and-write-behaviour-in-a-cluster.html), the behaviour of indexing in a clustered CouchDB or Cloudant environment merits a discussion. Understanding how indexing and querying works in a cluster will help avoid confusing situations where queries to a view index on Cloudant will produce different results because the indexes contain different data.\n\n**tl;dr:** Writing indexing functions (`map`/`reduce` in views, `index` for search indexes and so on) which can return different values for the same input document will cause the indexes on shard replicas to differ, meaning they could return different values for the same query even when data doesn't change. It's the developer's responsibility to write their indexing functions such that they always emit the same values for indexing for the same JSON input.\n\n## Single node querying\n\nFirst, a quick recap on how indexing and querying works on a non-clustered CouchDB instance. A single CouchDB node looks a bit like this internally:\n\n![cluster 1](/img/cluster1.jpg)\n\nWriting works as follows:\n\n1. Request handler receives a write request. It forwards the request to the Storage engine.\n2. The Storage engine writes the JSON document into the on-disk file. Each write to the on-disk file increments a sequence value for the on-disk JSON data file.\n3. The Storage engine returns success or failure to the Request handler, which returns to the client.\n\nA query is processed like this:\n\n1. Request handler receives a query. It forwards the request to the Querying engine.\n2. The Querying engine requests that the Indexer updates the on-disk index with the latest changes from the JSON data stored on-disk.\n3. The Indexer looks up the last sequence value it has for index being queried. The Storage engine uses this to give the Indexer a list of documents that have changed (new, updated or deleted) since that sequence value. The 4. Indexer takes this list of documents and updates the index with the changes. Of course, there may be no changes to process.\n4. When the index is up to date, the Querying engine looks up the result of the query in the index and returns it to the client via the Request handler.\n\nFor the purposes of this discussion, it's fine to assume all the various index types CouchDB 2.0 and Cloudant support work the same way. In addition, we'll put to one side the background index updating Cloudant does, and the various querying options which can change whether the index is updated in response to a query. These are irrelevant to the main points to be aware of when using CouchDB in a cluster or Cloudant.\n\n## Clustered querying\n\nRecall that in a cluster databases are divided into shards. Each shard is then stored three times across the cluster.\n\n![cluster 2](/img/cluster2.jpg)\n\nWhen considering indexing, it's worth considering the cluster at two levels of abstraction. The first is the cluster layer: the code which makes the machines in the cluster appear as a single CouchDB instance. The second is thinking about indexing from the point of view of each node, more specifically:\n\n- Each node N1, N2 and N3 is a standalone CouchDB instance.\n- From the point of view of the lower-level code which implements indexing on N1, N2 and N3, each database shard replica is actually an independent CouchDB database sitting on that node's disk. (It's the clustering layer that handles knitting these databases into the larger, single, externally visible database).\n\n![cluster 3](/img/cluster3.jpg)\n\nIn a cluster, writing happens as follows (with the caveats from the this post):\n\n1. Write request arrives at the Clustered request handler.\n2. It works out which shard contains the document being written. It looks up which nodes host the replicas of the shard for the document.\n3. The Clustered request handler sends a request to each replica-hosting node asking it to write the document.\n4. Each node writes the document separately and responds to the Clustered request handler success or failure.\n5. Once the Clustered request handler receives n/2+1 responses (by default), it returns its response to the client.\n\nThere are a bunch of subtleties and failure modes in the above, but for now let's pretend that writes always succeed.\n\nThe key point is that the nodes hosting the replicas write to their own component of the database *independently*.\n\nQuerying is similar:\n\n1. Query request for a given database arrives at the Clustered request handler.\n2. The Query request handler must ask every node that hosts a replica of a shard in the database for their answer to the query. Depending on how the shard replicas are distributed across the cluster, this might be all nodes in the cluster, or only a subset.\n3. Recall each node treats each shard as a database of itself. At this point the node proceeds as for the single database case, where its shard is the database: the node takes its shard replica, updates the index as above using the sequence value for its shard -- each shard has its own sequence value because it's a database -- and returns the results for its shard. Each node could host replicas of multiple shards, so the node could be carrying out this process for several shards in parallel.\n4. The Clustered request handler takes the first response received from a node hosting each shard and knits the responses together into the aggregate response for the database as a whole.\n\nPoint (3) is the key one here. Each shard has its own index and each node updates the index for its replica of the shard independently of the other nodes. This means that if the values a node calculates to store in the index are different to other nodes, the indexes stored on each node will be different. In that event, *each replica may return different results for a given query even if the data does not change*. Scary.\n\n## Avoiding inconsistent queries\n\nThe good news is that its simple to avoid this situation. It just requires that the developer writing indexing functions follow a single rule: never use information aside from the values passed to the indexing function -- usually the JSON document, but it could also be the inputs to a reduce function -- to calculate values for the index.\n\nTo understand the rule, first consider the question: how can a replica end up with a different value in its index to those of the other replicas of a shard? It's down to the way CouchDB handles calculating index values: the developer provides (typically) a JavaScript function which processes a document and outputs the values to be indexed for that document. If the developer doesn't take care to ensure that the function *always* returns the same index values for a given document, different replicas may end up with different values in their indexes.\n\nThe most common example is using the current date during indexing. For example the following map function is almost certain to cause problems:\n\n```js\nfunction map(doc) {\n    emit(Date.now(), doc);\n}\n```\n\nEach replica will run this function at a different time, and so the emitted key will be different on every node. Often the emitted values will only be a millisecond or two different, but it could be much more. Imagine a node in the cluster needs to be replaced. One option to do that is to comission a new node and replicate the data to that node. The new node will index the data as it receives it -- which could be days, months or even years after the other nodes indexed each document!\n\nSo remember the key rule when writing your indexing function: never use any information that's not contained in the input passed to the function.\n\nAs a side note, [Cloudant Query/Mango](https://developer.ibm.com/dwblog/2015/mango-json-vs-text-indexes/) uses a declarative syntax to declare indexes and so doesn't leave open the possibility of introducing inconsistent indexes in this manner.\n\n## Why indexing works the way it does\n\nOn the surface, another way to avoid inconsistency in replica indexes would be to only calculate the values to index on one node, and then send that value to the other replicas.\n\nThis approach wouldn't work in practice, however, if we want to maintain availability. To do things this way, there would need to be some concept of a primary node which is in charge of generating indexed values. In the face of a network partition, nodes on one half of the partition wouldn't have access to the primary indexing node, so would be unable to update their indexes, effectively making querying unavailable on that side of the partition.\n\nAs availability is a primary goal of CouchDB's clustering, indexing necessarily must happen independently on each node. To do this successfully does require some care as a developer using CouchDB. However, the flexibility of custom indexing functions combined with CouchDB's availability often makes this care worth taking.\n",
    "url": "/2016/01/31/Understanding-Cloudant-Indexing.html",
    "tags": "Cluster",
    "id": "8"
  },
  {
    "title": "couchbackup",
    "description": "For command-line backup",
    "content": "\n\n\nHow do you back up an Apache CouchDB&trade; or Cloudant database? One solution is to use CouchDB's built-in replication API. Let's say we have a Cloudant database called  **mydata** that we need to back up.\n\nIn CouchDB 1.x, backing up an entire database was as simple as locating the database's `.couch` file and copying it somewhere else. With its 2.x release, CouchDB and the Cloudant database shard the data, splitting a single database into pieces and distributing the data across multiple servers. So backing up a database is no longer as simple as copying a single file.\n\nThen how do you back up? This blog post presents 3 options:\n\n* back up to a text file\n* replicate via the command-line\n* replicate via the Cloudant dashboard\n\n## Back up to a text file\n\nCloudant has a RESTful HTTP API, so it is easy to create your own tools to interact with the service. I created a command-line tool called [couchbackup](https://www.npmjs.com/package/@cloudant/couchbackup), which you can use to spool an entire database (either CouchDB or Cloudant) to a text file.\n\n![couchbackup](/img/couchbackup.gif)\n\nTo install the tool:\n \nYou must have [Node.js](https://nodejs.org/en/) installed, together with its \"npm\" package manager. Then follow these steps:\n\n1. Run:\n\n```sh\n> npm install -g @cloudant/couchbackup\n```\n\n2. Define an environment variable which holds the path of either:\n\n   - your remote Cloudant database:\n\n```sh\n> export COUCH_URL=\"https://myusername:mypassword@myhost.cloudant.com\"\n```\n\n   - or local CouchDB instance:\n\n```sh\n> export COUCH_URL=\"http://localhost:5984\"\n```\n\n3. Back up individual databases to their own text files:\n\n```sh\n> couchbackup --db mydb > mydb.txt\n```\n\n4. If you want to restore data from a backup into an *empty* database, then use the tool `couchrestore` which was also installed with `couchbackup`:\n\n```sh\n> cat mydb.txt | couchrestore --db mydb\n```\n\n5. To increase the speed of the restore operation you can perform multiple write operations in parallel:\n\n```sh\n> cat mydb.txt | couchrestore --db mydb --parallelism 5\n```\n\n## Replication via the command-line\n\nAnother option is to **replicate** the database to another Cloudant account or to another CouchDB service by issuing an API call to set off a replication task that copies data from the source database to the target database.\n\nStart replication by adding a document into the `_replicator` database; a  document that lists the source and target database, including authentication credentials. You can achieve all of this from the command-line using a single `curl` command:\n\n```sh\n> export SOURCE=\"https://myusername:mypassword@myhost.cloudant.com\"\n> export TARGET=\"https://myotherusername:myotherpassword@myotherhost.cloudant.com\"\n> export JSON=\"{\\\"source\\\":\\\"$SOURCE/mydata\\\",\\\"target\\\":\\\"$TARGET/mydata\\\"}\"\n> curl -X PUT -H \"Content-Type: application/json\" -d \"$JSON\" \"$SOURCE/_replicator\"\n{\"id\":\"0b05156eefc1feca97e48cd6bd000380\",\"_rev\":\"1-a301b0fbfa8840f3ca936876729e37cc\"} \n```\n\nThe API returns with a JSON object containing the id of a document, which you can fetch to monitor the status of the replication job:\n\n```sh\n> curl \"$SOURCE/_replicator/0b05156eefc1feca97e48cd6bd000380\"\n``` \n\nIf you have Apache CouchDB installed locally and you intend to back up data from a Cloudant cluster, then instruct your local CouchDB installation to perform the replication. Why your local machine?  Because it has visibility to the Cloudant service, but not vice-versa.\n\n```sh\n> export SOURCE=\"https://myusername:mypassword@myhost.cloudant.com\"\n> export TARGET=\"https://localhost:5984\"\n> export JSON=\"{\\\"source\\\":\\\"$SOURCE/mydata\\\",\\\"target\\\":\\\"$TARGET/mydata\\\"}\"\n> curl -X PUT -H \"Content-Type: application/json\" -d \"$JSON\" \"$TARGET/_replicator\"\n{\"id\":\"0b05156eefc1feca97e48cd6bd001976\",\"_rev\":\"1-ac15e7843682715ccb712fac41169cf5\"} \n```\n\n## Replication via the Cloudant dashboard\n\nYou can also start and monitor a replication using the web-based user interface of the Cloudant dashboard. \n\n1. On the left, choose the **Replication** tab,\n2. Click **New Replication** \n3. Complete the form and click **Replicate**. \n\nIn the above example, we are replicating a database that lives in the current user's Cloudant account (the \"My Databases\" tab in the Source Database section) to another Cloudant account (the \"Remote Database\" tab in the Target Database section). But we could also be in the target account. Use the same form to perform replications between all combinations of local and remote sources and targets.\n\n## The difference between replication and couchbackup\n\nCouchDB/Cloudant replication is a sophisticated sync protocol that ensures all data from the source database is transferred to the target. If the target database already contains some documents, then clashing revisions are stored as [document conflicts](https://cloudant.com/blog/introduction-to-document-conflicts-part-one/). In addition, deleted documents from the source database are also transferred to the target database.\n\n`couchbackup` simply iterates through the `/db/_all_docs` endpoint fetching the \"winning revisions\" of documents from the source database. Unlike replication, it ignores deleted documents and conflicts from the source database. During the restoration of backed up data, it also assumes that the target database is empty; no conflicting revisions are created. The result of a `couchrestore` operation is a collection of \"first revisions\" that matches the winning revisions of the source database.\n\n> \"Apache\", \"CouchDB\", \"Apache CouchDB\", and the CouchDB logo are trademarks or registered trademarks of The Apache Software Foundation. All other brands and trademarks are the property of their respective owners.\n",
    "url": "/2016/03/22/Simple-CouchDB-Cloudant-backup.html",
    "tags": "Backup",
    "id": "9"
  },
  {
    "title": "Chrome Extensions",
    "description": "With PouchDB & Cloudant",
    "content": "\n\n\nGoogle Chrome extensions are small web applications that are downloaded and installed in the user's Chrome browser. They have a minimal user interface, usually a small icon to the right of the URL bar and a pop-down panel, but have additional rights over and above normal websites:\n\n* they can be bundled and submitted to the Chrome Web store as a distribution mechanism\n* they have limited access to the host computer to save files, access networking tools and communicate with connected devices\n* they have access to the browser's inner workings and can alter the rendering of web pages\n\nChrome extensions are really just web applications built from HTML, CSS and JavaScript, and we can use the framework of our choice to build them.\n\nWhen it comes to storing data, the [chrome.storage](https://developer.chrome.com/extensions/storage) API allows your extension to store data, which can be synced between other instances of the extension with the same Google account, such as between your desktop and laptop browsers. If you don't want to use the Google APIs, then you can use your own storage tools such as [PouchDB](https://pouchdb.com/), an in-browser JSON document store that can sync with remote PouchDB, [CouchDB](http://couchdb.apache.org/) or [Cloudant](https://cloudant.com/) databases. \n\nThis post shows two sample Chrome extensions\n\n**linkshare - a very simple bookmarking tool**\n\n![linkshare screenshot](/img/chrome1.png)\n\n**volt - a password database**\n\n![volt screenshot](/img/chrome2.png)\n\nBoth apps store their data *locally first* using PouchDB and allow the user to optionally sync the data with a remote server.\n\n## Installing\n\nThe samples presented here have not been submitted to the Chrome Web Store, but you can install them from source as long as you enable developer mode. If you subsequently modify the source code on your machine, the changes will be reflected in Chrome the next time you open the extension.\n\nOn the command-line, clone the *linkshare* repository:\n\n```sh\n> git clone https://github.com/glynnbird/linkshare.git\n```\n\nand/or the *volt* repository:\n\n```sh\n> git clone https://github.com/glynnbird/volt.git\n```\n\n* Visit *chrome://extensions* in your Chrome browser\n* Switch on Developer mode\n* Click \"Load unpacked extension...\"\n* Navigate to the directory where you clone the git repository\n* Repeat for the other extension\n\nSource code:\n\n* https://github.com/glynnbird/linkshare\n* https://github.com/glynnbird/volt\n\n[Full installation instructions for non-packaged extensions here](https://developer.chrome.com/extensions/getstarted).\n\n## Creating an Chrome extension from scratch\n\nThe life of a Chrome extension begins with a manifest file called \"manifest.json\":\n\n```js\n{\n  \"manifest_version\": 2,\n  \"name\": \"Linkshare\",\n  \"description\": \"A simple link sharing Chrome extension\",\n  \"version\": \"1.0\",\n  \"browser_action\": {\n    \"default_icon\": \"linkshare.png\",\n    \"default_popup\": \"linkshare.html\",\n    \"default_title\": \"Linkshare\"\n  },\n  \"permissions\": [\n    \"activeTab\",\n    \"https://ajax.googleapis.com/\"\n  ],\n  \"content_security_policy\": \"script-src 'self' 'unsafe-eval'; object-src 'self'\"\n}\n```\n\nIt declares the metadata about the app and which APIs and options it will use. It references a PNG file containing an icon and the HTML page that contains the popup panel. Here's what you need to add:\n\n* an icon file\n* a page of HTML\n* a JavaScript file (for Chrome extensions, your JavaScript must reside in a separate file, not in the HTML page)\n\nOnce installed, you can modify your local files and your installed extension will reflect the changes you make.\n\n## Storing data in a Chrome extension\n\nUsing PouchDB couldn't be easier. First of all it needs to be downloaded and referenced in our single-page web app:\n\n```html\n<script src=\"js/pouchdb-5.3.2.min.js\"></script>\n```\n\nOur page can then create a database and start saving data:\n\n```js\nvar db = new PouchDB(\"linkshare\");\ndb.post({a:1,b:2}).then(function(data) {\n  console.log(\"done\");\n});\n```\n\nFor debugging, the JavaScript console can be accessed by right-clicking the extension icon and choosing \"Inspect popup\".\n\n## A simple bookmarking app\n\nChrome extensions have access to the current tab that is being viewed, so it's simple to create a bookmarking tool that keeps a list of URLs that you want to save permanently, read later or share. By storing such data in PouchDB, we can then optionally sync the data with an Apache CouchDB or IBM Cloudant databases to ensure that there is more than one copy of the data. Furthermore, other users sharing that database can also access the data, allowing the sharing of links with your partner, work colleagues or other ad-hoc groups of people who also have the extension installed.\n\n![schematic](/img/chrome.png)\n\nThe JSON schema we will be using is this:\n\n```js\n{\n  \"_id\": \"123\",\n  \"_rev\": \"456\",\n  \"url\": \"http://myfavourite.website.com/is/this/one.html\",\n  \"date\": \"2016-06-01 12:44:22 Z\",\n  \"title\": \"My favourite website\"\n}\n```\n\nWhen a user presses the \"Save\" button in our extension window, we fetch the page URL and title from the Google Chrome API, add a timestamp and turn the data into a JSON object before storing it in the PouchDB database. PouchDB automatically generates the `_id` and `_rev` fields. These fields are reserved by the database to enforce uniqueness of document IDs and to keep track of any document revisions.\n\n## Loading data on startup\n\nWhen our Chrome extension is activated, our code fetches a list of documents from the PouchDB database in reverse date order (newest first). We implement this behaviour using the `query` function in _linkshare.js_:\n\n```js\n  db.query(map, {include_docs:true, descending:true}).then(function(result) {\n    // render the result\n  });\n```\n\nThe `map` function is a MapReduce function that defines the index we wish to create.\n\n```js\nvar map = function(doc) {\n  emit(doc.date, null);\n};\n```\n\nThe map function is called with every document in the collection, and the key/value pairs it emits form the basis of an index we can subsequently interrogate (query). In this case we simply want the data in reverse order (`descending:true`), and we want the whole document bodies back (`include_docs:true`). There's no reduce function in our code because we don't need one. If we needed to aggregate query results, we could use one of PouchDB's built-in reduce functions. See the [PouchDB guide on queries](https://pouchdb.com/guides/queries.html) for more on the built-in reduces for `_count`, `_sum` and `_stats`.\n\n## Syncing with an Apache CouchDB or Cloudant server\n\nPouchDB makes it very simple to sync with a remote server that speaks the CouchDB replication protocol:\n\n```js\n  db.sync(url, { live:true, retry:true });\n```\n\nThis single line of code:\n\n* initiates replication from the client to the server\n* initiates replication from the server to the client\n* handles reconnection if the connections are interrupted\n* streams live changes happening on the client or server to the other party\n\nThe URL that our app syncs with is provided by the user in the settings panel of our Chrome extension. e.g\n\n```\n   https://myusername:mypassword@myhost.cloudant.com/mydatabase\n```\n\nThe URL itself is also stored in PouchDB, so it is remembered for the next session. But this value is stored in a [_local document](https://pouchdb.com/guides/local-documents.html), which is a special class of document that resides in the same database as the other bookmark documents, but only in the local copy of the database. \"Local\" documents are not replicated or indexed.\n\n## Offline-first means always on\n\nWriting data to an in-browser database means that your web application will always work, whether you have a network connection or not. The data is synced to a cloud service too, but the primary mechanism for reading and writing data is the local PouchDB database. So even if remote syncing is impossible, the Chrome extension is always available to store and retrieve data from its local store.\n\nSyncing data also makes sharing it easy. Syncing to a remote copy allows a collection of data to be shared with other users who have access to the same database. All subscribed users will see all the changes when they sync. Coordinating the transfer of data between PouchDB and a cloud-based copy and vice versa is a complex programming task if you were building it yourself, but thanks to PouchDB it's a one-liner!",
    "url": "/2016/05/11/Chrome-Extensions-with-PouchDB.html",
    "tags": "PouchDB",
    "id": "10"
  },
  {
    "title": "Caching HTTP requests",
    "description": "With cachemachine and redis",
    "content": "\n\n\nA cache is a copy of data stored in place where it can be retrieved quickly with the original and most up-to-date copy being a separate, slower repository. Examples of caching include:\n\n- microprocessors contain on-board data caches to save having to fetch the data from memory.\n- spinning disks contain a cache of a data in memory to save having to fetch data on disk.\n- web browsers cache a page's assets on the client machine to save having to fetch data from the network.\n\n<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">Latency numbers every programmer should know <a href=\"https://t.co/H0Bp2nYivt\">pic.twitter.com/H0Bp2nYivt</a></p>&mdash; Mario Fusco (@mariofusco) <a href=\"https://twitter.com/mariofusco/status/785948217217282048\">October 11, 2016</a></blockquote>\n<script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\nYour own systems can achieve huge performance benefits if you build in a caching layer, choosing to cache data that is accessed frequently--data that still has value when stale--and deciding how long to cache each item. A cache provides speed benefits for your application, and takes load away from your primary data source, freeing up resources for other work. \n\n<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">Table of system latencies makes for great reading <a href=\"https://t.co/346giJZQnh\">pic.twitter.com/346giJZQnh</a></p>&mdash; Azeem Azhar (@azeem) <a href=\"https://twitter.com/azeem/status/669678968359034880\">November 26, 2015</a></blockquote>\n<script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\n## Caching fast changing data\n\nImagine I am storing web metrics data in an [IBM Cloudant](https://cloudant.com/) database where each JSON document represents an interaction with my web page:\n\n```js\n{\n  \"date\": \"2016-09-01 10:24:13\",\n  \"type\": \"click\",\n  \"path\": \"/Born-Run-Bruce-Springsteen/dp/1471157792/\",\n  \"userAgent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36\",\n  \"host\": \"www.bookseller.com\",\n  \"userid\": \"yJBsMCs4qEyF89jvGD1cI1V81YdNw6\"\n}\n```\n\nI can use a MapReduce function in Cloudant with the `_count` reducer to create an aggregated view of my data, counting the number of 'click' documents by day:\n\n```js\nfunction(doc) {\n  if (doc.type === 'click') {\n    var d = doc.date.split(' ')[0];\n    emit(d, null);\n  }\n}\n```\n\nI can then create a Node.js dashboard to query the view and present the results:\n\n```js\nvar myurl = 'https://myusername:mypassword@myhost.cloudant.com';\nvar cloudant = require('cloudant')({ url: myurl });\nvar db = cloudant.db.use('metrics');\ndb.view('clicks', 'byday', {group: true}, function(err, data) {\n  console.log(data);\n  // present the data on the dashboard.\n});\n````\n\nThe call to the Cloudant view will retrieve data of this form:\n\n```js\n{ \"rows\":[\n    {\"key\":\"2016-09-01\", \"value\":10985882},\n    {\"key\":\"2016-09-02\", \"value\":11884271},\n    {\"key\":\"2016-09-03\", \"value\":12004155},\n    {\"key\":\"2016-09-04\", \"value\":11094426}\n  ]\n}\n```\n\nCloudant is more than happy to answer queries like this, as its distributed design shares the computational load around its cluster. But every time your dashboard web page loads, you ask Cloudant to recalculate the totals and to rebuild the view to incorporate the newly arrived documents. As the rate of change of data is not insignificant (circa 11m documents a day, or 127 per second), then Cloudant has its work cut out for it!\n\nYou can employ caching in this use-case to take some load from Cloudant by storing the daily totals in cache when they are first requested. The time-to-live (TTL) of the cache is a choice: is it ok for the users to see data that is 1 hour old? 10 minutes old? The longer the TTL, the less load Cloudant sees, but the more out-of-date the dashboard is. \n\nUsing an in-memory cache in front of Cloudant would give the dashboard a faster load time and let Cloudant get on with the core task of storing and indexing the incoming data.\n\n## Redis\n\n[Redis](http://redis.io/) makes an ideal cache because it stores its data in RAM for fast storage and retrieval. Redis at its simplest, stores key/value pairs and can additionally expire the keys at a pre-defined TTL. To store a key (mykey) with an associated value (xyz) that expires in an hour, you can use Redis's `SETEX` command:\n \n\n```\n  SETEX mykey 3600 \"xyz\"\n``` \n\nRetrieve the value at any time within the next hour with\n\n```\n  GET mykey\n```\n\n## Caching Cloudant data with cachemachine\n\nTo cache HTTP requests to Cloudant, we can use the [cachemachine](https://www.npmjs.com/package/cachemachine) utility as a drop-in replacement for the [request](https://www.npmjs.com/package/request) library with a twist--it caches the paths you specify in a Redis cache. The first time you ask for the data, Cloudant responds and the result is cached. All other requests for the same URL (within the TTL) are answered by calls to Redis!\n\nIn this case, we want to cache requests that access views for only one hour (paths beginning with the database name followed by `/_design/`):\n\n```js\nvar paths = [ { path: '^/mydb/_design/.*', ttl: 60*60 }];\nvar cachemachine = require('cachemachine')({redis: true, paths: paths});\n```\n\nWe can then tell the [Cloudant Node.js library](https://www.npmjs.com/package/cloudant) to use this as a plugin:\n\n```js\nvar cloudant = require('cloudant')({ url: myurl, plugin: cachemachine });\n```\n\nNow requests we make to our view are cached automatically:\n\n```js\ndb.view('clicks', 'byday', {group: true}, function(err, data) {\n  // data is returned and cached transparently\n  console.log(data);\n});\n```\n\nCloudant handles the first request for the data, but subsequent requests for the same data come from Redis, until the cache key expires.\n\n## Building a caching proxy server with Express and cachemachine\n\nYou can also create your own proxy server in front of Cloudant, caching the requests you want to intercept and passing all other requests untouched. \n\n```js\nvar express = require('express'),\n  app = express(),\n  cachemachine = require('cachemachine')(),\n  couchURL = process.env.COUCH_URL || 'http://localhost:5984';\n\n// intercept all requests for GET requests for views\napp.get('/:db/_design/:design/_view/:view', function (req, res, next) {\n  var r = {\n    url: couchURL + req.path,\n    qs: req.query\n  };\n  // handle them with cachemachine\n  cachemachine(r, function(err, response, body) {\n    res.send(body);\n  });\n});\n\n// Catch all other paths and proxy them to CouchDB\napp.use('/', require('express-http-proxy')(couchURL));\n\napp.listen(3000, function () {\n  console.log('Example app listening on port 3000!');\n});\n```\n\nThis code doesn't use cachemachine's path matching, it simply catches the paths it needs to intercept and routes them through cachemachine, leaving all other traffic to be transparently proxied.\n\n## Using cachemachine with Redis on Compose\n\nDeploying a highly-available Redis cluster is easy with [Redis on Compose](https://www.compose.com/redis). It deploys in minutes in a choice of data centers. Using `cachemachine` with Compose's Redis is just as simple:\n\n```js\nvar opts = {\n  redis: true,\n  hostname: 'myhostname.dblayer.com',\n  port: 10000,\n  password: 'mypassword'\n};\nvar request = require('cachemachine')(opts);\n```\n\nsubstituting the values of `hostname`, `port` and `password` for those found in your Compose dashboard.\n\n## Conclusion\n\nCaching provides quicker access to frequently-requested data, trading-off speed of delivery for freshness of the data. You tell your app how stale you can afford to let the data get by configuring the TTL of cache key. Caching lets your application respond quickly, while not overburdening underlying data storage services. Use the [cachemachine](https://www.npmjs.com/package/cachemachine) to transparently cache any HTTP service with Redis as the cache store, and plug it directly into the [Cloudant Node.js library](https://www.npmjs.com/package/cloudant).",
    "url": "/2016/11/22/Caching-Requests-with-Cachemachine.html",
    "tags": "Cache Redis",
    "id": "11"
  },
  {
    "title": "The Revision Tree",
    "description": "How Cloudant stores revisions in a tree",
    "content": "\n\n\nOn the surface, Cloudant's document API appears to be a reasonably simple way to upload JSON data to a key value store and update the JSON data for a particular key. However, the reality is more complicated -- isn't it always? -- and understanding what's happening is important in using Cloudant effectively and becoming an advanced user of the database.\n\nThe takeaway point is this: **a document in Cloudant is a tree and Cloudant's document HTTP API is essentially a tree manipulation API**.\n\nBelow, we'll create and manipulate these trees to get a feel for what this means. The default behaviours of many calls allow you to ignore the tree in order that Cloudant appears more like a database, and we'll explore why and how this works as we go.\n\nAside: why a tree? A document's tree representation is key to Cloudant's replication, which is essentially transferring tree nodes from one location to another in order to synchronise data robustly.\n\nAll this information is valid for CouchDB, but I've chosen to refer to Cloudant throughout for simplicity. This mini-tutorial assumes a basic knowledge of the Cloudant document API. Let's get started.\n\n## Building a simple document tree\n\nUploading a new document to Cloudant gives you something like this:\n\n```sh\n> curl -XPUT \\\n    'http://localhost:5984/tree-demo/mydoc' \\\n    -d '{\"foo\":\"bar\"}'\n{\"ok\":true,\"id\":\"mydoc\",\"rev\":\"1-4c6114c65e295552ab1019e2b046b10e\"}\n```\n\nUpdating that document gives you this:\n\n```sh\n> curl -XPUT \\\n    'http://localhost:5984/tree-demo/mydoc?rev=1-4c6114c65e295552ab1019e2b046b10e' \\\n    -d '{\"foo\":\"baz\"}'\n{\"ok\":true,\"id\":\"mydoc\",\"rev\":\"2-cfcd6781f13994bde69a1c3320bfdadb\"}\n```\n\nEach call has returned a new rev ID in the response. If we didn't already know the two rev IDs for this document, we could find them out by making a request for the document using the revs=true querystring parameter:\n\n```sh\n> curl -XGET \\\n    'http://localhost:5984/tree-demo/mydoc?revs=true' | jq .\n{\n  \"_id\": \"mydoc\",\n  \"_rev\": \"2-cfcd6781f13994bde69a1c3320bfdadb\",\n  \"foo\": \"baz\",\n  \"_revisions\": {\n    \"start\": 2,\n    \"ids\": [\n      \"cfcd6781f13994bde69a1c3320bfdadb\",\n      \"4c6114c65e295552ab1019e2b046b10e\"\n    ]\n  }\n}\n```\n\nThe special thing about this request is that the response lists the rev IDs in order -- which defines the history of the document. From this call, we can construct a tree for the document with a single branch and two nodes. We use the _revisions field, and construct the tree backwards using the start and ids array:\n\n![tree 1](/img/tree1.png)\n\nThe tree has two nodes. Cloudant labels each node with its rev ID. Cloudant also labels one node winner; we'll come to that in a moment.\n\n*Note: Cloudant calls these tree nodes revisions but I'm going to stick with node to emphasise the tree semantics.*\n\nNow we've seen the tree, let's see how we can think about the three calls if we consider them in terms of trees rather updates to a JSON document.\n\n\n1. The first call creates a new document tree and its root node, 1-4c6114c65e295552ab1019e2b046b10e.\n2. The second call adds a node to the tree, parented by the node we pass in via the rev querystring parameter. The call returns the rev ID of the new node, 2-cfcd6781f13994bde69a1c3320bfdadb.\n3. The final call uses the revs=true parameter to return the history of the 2- node along with its content.\n\nOne other thing is worth mentioning at this point: a rev ID can be used to retrieve any node of the tree at any time. This is done by using the rev querystring parameter when making a GET request for the document. While that node will be returned, the JSON content of the document at that point may not: Cloudant cleans up the content of old tree nodes to conserve space.\n\n## The winner node\n\nThe winner abstraction is key to Cloudant's database semantics. It's used to give the appearance of a single value for a document rather than a tree. It does this by freeing us from having to specify the rev ID of the node we want with every call -- though some calls, like updates, always need one. Let's look at how this works.\n\nCloudant applies the winner label to the node at the tip of the tree as shown in the diagram above. This node is the one that Cloudant will refer to when responding to document retrieval requests that do not have any particular rev ID specified. The rev ID for this node is included in the response.\n\nWhile the node that Cloudant chooses to label the winner for any given document is an implementation detail, it's essentially decided by choosing the longest path in the document tree that's not terminated by a deletion node. This will be more obvious later when we introduce a branch to the tree.\n\nAn essential property is that Cloudant chooses which node to label winner in a deterministic way. This means that when the database is replicated, Cloudant will label the same node as winner at all replicas, given the same document tree.\n\nConsider the last call above:\n\n```sh\n> curl -XGET \\\n    'http://localhost:5984/tree-demo/mydoc?revs=true'\n```\n\nBy not specifying a rev ID to return, we're instructing Cloudant to return information about the tree node pointed to by the winner label.\n\n## Updates are tree manipulation\n\nWe've now seen that an update to a document can be viewed as adding a new node to the document's tree. And we have seen that the update request's rev parameter is specifying which existing node in the document tree should be used as the parent for this new node.\n\nIf we update twice more, we can see the tree grow. In the background, Cloudant will be updating the winner pointer to a new rev ID each time. It sends that back in the rev field of the response:\n\n```sh\n> curl -XPUT \\\n    'http://localhost:5984/tree-demo/mydoc?rev=2-cfcd6781f13994bde69a1c3320bfdadb' \\\n    -d '{\"foo\":\"bop\"}'\n{\"ok\":true,\"id\":\"mydoc\",\"rev\":\"3-2766344359f70192d3a68bf205c37743\"}\n\n> curl -XPUT \\\n    'http://localhost:5984/tree-demo/mydoc?rev=3-2766344359f70192d3a68bf205c37743' \\\n    -d '{\"foo\":\"bloop\"}'\n{\"ok\":true,\"id\":\"mydoc\",\"rev\":\"4-a5be949eeb7296747cc271766e9a498b\"}\\\n```\n\nRetrieving the tree description again using revs=true, we can see this has changed the JSON description of the document tree history to include the new rev IDs (node labels):\n\n```sh\n> curl -XGET 'http://localhost:5984/tree-demo/mydoc?revs=true' | jq .\n{\n  \"_id\": \"mydoc\",\n  \"_rev\": \"4-a5be949eeb7296747cc271766e9a498b\",\n  \"foo\": \"bloop\",\n  \"_revisions\": {\n    \"start\": 4,\n    \"ids\": [\n      \"a5be949eeb7296747cc271766e9a498b\",\n      \"2766344359f70192d3a68bf205c37743\",\n      \"cfcd6781f13994bde69a1c3320bfdadb\",\n      \"4c6114c65e295552ab1019e2b046b10e\"\n    ]\n  }\n}\n```\n\nThe tree now has four nodes, as shown by the _revisions array. As we didn't specify a rev in the request, Cloudant has again returned the node content and history for the winner node, as we'd expect. As with any direct request to retrieve a document, we could have supplied a rev parameter to receive information about a different node.\n\n![tree 2](/img/tree2.png)\n\n## Branching in the document tree\n\nTo show how the tree can branch, we need to make this document a conflicted document. A conflicted document is a document where there are two or more active branches. An active branch is one that ends in a node that is not deleted. This often happens during replication, where a document with a common history may have been updated in different ways in the databases being replicated.\n\nIf you've been using Cloudant for a while, you'll be familiar with Cloudant's behaviour when you send the \"wrong\" rev ID with an update: the 409 Conflict response. With our new tree knowledge, we can see that what Cloudant is doing is either (a) rejecting a request that tries to add a node to a parent which already has a child, or (b) trying to use a parent node that doesn't exist. It does this so the tree remains a single stem, which makes sense for a database. This can be seen if we try to add a second child to our 1- revision:\n\n```sh\n$ curl -XPUT \\\n    'http://localhost:5984/tree-demo/mydoc?rev=1-4c6114c65e295552ab1019e2b046b10e' \\\n    -d '{\"foo\":\"baz\"}' -v\n[...]\n> PUT /tree-demo/mydoc?rev=1-4c6114c65e295552ab1019e2b046b10e HTTP/1.1\n[...]\n< HTTP/1.1 409 Conflict\n[...]\n{\"error\":\"conflict\",\"reason\":\"Document update conflict.\"}\n```\n\nHowever, we can use the _bulk_docs call to bypass these protections (which is what Cloudant's replicator does). Clearly, this is something you'd never use day-to-day, but it's instructive to see the effects. In the following call, we show this bypassing in action by creating a new branch rooted at the first update we made above.\n\nThe _bulk_docs request takes a JSON body describing the updates to make. First, we create the body of the request in bulk.json:\n\n```sh\n{\n    \"docs\": [\n        {\n            \"_id\": \"mydoc\",\n            \"_rev\": \"3-917fa2381192822767f010b95b45325b\",\n            \"_revisions\": {\n                \"ids\": [\n                    \"917fa2381192822767f010b95b45325b\",\n                    \"cfcd6781f13994bde69a1c3320bfdadb\",\n                    \"4c6114c65e295552ab1019e2b046b10e\"\n                ],\n                \"start\": 3\n            },\n            \"bar\": \"baz\"\n        }\n    ],\n    \"new_edits\": false\n}\n```\n\nThis document describes a new node for the document mydoc that we created above, `3-917fa2381192822767f010b95b45325b`. This node has a history that *diverges from the existing document*. The new node's history is described using the `_revisions` field, using the same format as we received in `revs=true` calls above. In this history, the first two rev IDs are the same, but the third is different. This will cause a branch to happen at rev `2-`.\n\nBy including `new_edits=false` in the JSON, we tell Cloudant to graft this node into the document tree, creating any parent nodes required. Without `new_edits=false`, Cloudant would reject this update as the history is incompatible with the `winner` node.\n\nNext, send the request using this body:\n\n```sh\n> curl -XPOST \\\n    'http://localhost:5984/tree-demo/_bulk_docs' \\\n    -d @bulk.json \\\n    -H \"Content-Type:application/json\"\n```\n\nWe now have a tree which branches. To check this, we can make a request to Cloudant that will return the content of the nodes at the tip of each branch, along with the history of each of these nodes.\n\nThe `open_revs` parameter is specifically used for retrieving the content of multiple tip nodes at once. To return all the branches' tip nodes, we pass `open_revs=all` into the `GET` for the document. This is a second way to get non-`winner` nodes, along with specifying `rev`.\n\nAs above, we also include `revs=true` so that the history of each tip node is also returned. We also specify `Accept: application/json` to get a simpler return format: all the tip nodes in a JSON array.\n\n```sh\n> curl -XGET \\\n    'http://localhost:5984/tree-demo/mydoc?open_revs=all&revs=true' \\\n    -H \"Accept:application/json\" | jq .\n[\n  {\n    \"ok\": {\n      \"_id\": \"mydoc\",\n      \"_rev\": \"4-a5be949eeb7296747cc271766e9a498b\",\n      \"foo\": \"bloop\",\n      \"_revisions\": {\n        \"start\": 4,\n        \"ids\": [\n          \"a5be949eeb7296747cc271766e9a498b\",\n          \"2766344359f70192d3a68bf205c37743\",\n          \"cfcd6781f13994bde69a1c3320bfdadb\",\n          \"4c6114c65e295552ab1019e2b046b10e\"\n        ]\n      }\n    }\n  },\n  {\n    \"ok\": {\n      \"_id\": \"mydoc\",\n      \"_rev\": \"3-917fa2381192822767f010b95b45325b\",\n      \"bar\": \"baz\",\n      \"_revisions\": {\n        \"start\": 3,\n        \"ids\": [\n          \"917fa2381192822767f010b95b45325b\",\n          \"cfcd6781f13994bde69a1c3320bfdadb\",\n          \"4c6114c65e295552ab1019e2b046b10e\"\n        ]\n      }\n    }\n  }\n]\n```\n\nWhich represents the following tree:\n\n![tree 3](/img/tree3.png)\n\n## Resolving the conflicted state\n\nA document is conflicted so long as it has two or more branches which don't end in a deletion node. A conflicted document is clearly in a bad state for a database: some data is effectively hidden. So before we finish, let's *resolve* the conflict. In tree terms, resolving means updating the document's tree so that there is only one active branch.\n\nAs we have two active branches, we need to update one branch with a deletion node, called a tombstone, to make that branch inactive. We do this by making a `DELETE` request to the document's resource. This request specifies the parent rev ID (`rev`) as the node at the tip of the branch we wish to mark as a dead end.\n\nBefore this, we need to choose which branch to keep. To do this, we need to retrieve both. Instead of using `open_revs=all`, we can retrieve each individually.\n\nFirst, let's look at what Cloudant considers the *winner* document node. This corresponds to the data it returns when a simple document lookup is carried out:\n\n```sh\n> curl -XGET \\\n    'http://localhost:5984/tree-demo/mydoc' | jq .\n{\n  \"_id\": \"mydoc\",\n  \"_rev\": \"4-a5be949eeb7296747cc271766e9a498b\",\n  \"foo\": \"bloop\"\n}\n```\n\nAnd, as stated earlier, we can get the other branch's data by specifying the rev ID of the node at the tip of the other branch:\n\n```sh\n> curl -XGET \\\n    'http://localhost:5984/tree-demo/mydoc?rev=3-917fa2381192822767f010b95b45325b' | jq .\n{\n  \"_id\": \"mydoc\",\n  \"_rev\": \"3-917fa2381192822767f010b95b45325b\",\n  \"bar\": \"baz\"\n}\n```\n\nLet's say that the non-winning node, `3-917fa2381192822767f010b95b45325b` contains the JSON data we actually want for this document. We issue a delete request specifying the unwanted branch's tip node's rev ID as its parent:\n\n```sh\n> curl -XDELETE \\\n    'http://localhost:5984/tree-demo/mydoc?rev=4-a5be949eeb7296747cc271766e9a498b'\n{\"ok\":true,\"id\":\"mydoc\",\"rev\":\"5-ab21cb5ac4c8da916c47c45330d8a655\"}\n```\n\nRecapping what we're doing here, this request instructs Cloudant to add a *tombstone* node to the parent node specified by the rev parameter, thereby rendering that branch inactive.\n\nRecall the algorithm to select the node the `winner` label points to is to choose the longest active path in a document tree. Therefore, after this operation, Cloudant will have moved the `winner` label to point at the tip of the remaining active branch. A lookup of the doc ID without specifying a rev ID therefore now returns the other node:\n\n```sh\n> curl -XGET \\\n    'http://localhost:5984/tree-demo/mydoc' | jq .\n{\n  \"_id\": \"mydoc\",\n  \"_rev\": \"3-917fa2381192822767f010b95b45325b\",\n  \"bar\": \"baz\"\n}\n```\n\nA corollary of this is that a document is not considered deleted -- that is, a query for that document will not return a `404 Not Found` -- until all branches are inactive. This can be the source of great confusion if one doesn't know beforehand that a document is conflicted. Deleting the document appears to bring back to life another version of the document!\n\nIf a document has many active branches, it will take several requests to add a tombstone to every branch tip to make the document actually appear deleted. Using the open_revs=all argument will show how many active branch tips there are.\n\nReturning to our example, we can study the resulting tree using the request that we've made a few times now:\n\n```sh\n> curl -XGET \\\n    'http://localhost:5984/tree-demo/mydoc?open_revs=all&revs=true' \\\n    -H \"Accept:application/json\" | jq .\n[\n  {\n    \"ok\": {\n      \"_id\": \"mydoc\",\n      \"_rev\": \"5-ab21cb5ac4c8da916c47c45330d8a655\",\n      \"_deleted\": true,\n      \"_revisions\": {\n        \"start\": 5,\n        \"ids\": [\n          \"ab21cb5ac4c8da916c47c45330d8a655\",\n          \"a5be949eeb7296747cc271766e9a498b\",\n          \"2766344359f70192d3a68bf205c37743\",\n          \"cfcd6781f13994bde69a1c3320bfdadb\",\n          \"4c6114c65e295552ab1019e2b046b10e\"\n        ]\n      }\n    }\n  },\n  {\n    \"ok\": {\n      \"_id\": \"mydoc\",\n      \"_rev\": \"3-917fa2381192822767f010b95b45325b\",\n      \"bar\": \"baz\",\n      \"_revisions\": {\n        \"start\": 3,\n        \"ids\": [\n          \"917fa2381192822767f010b95b45325b\",\n          \"cfcd6781f13994bde69a1c3320bfdadb\",\n          \"4c6114c65e295552ab1019e2b046b10e\"\n        ]\n      }\n    }\n  }\n]\n```\n\nThe response still shows both branches. This is expected, as the branches still exist! However, we can see that the branch ending in `5-` is deleted and so the `3-` branch is the only active branch. Which therefore gives us this final tree:\n\n![tree 4](/img/tree4.png)\n\n## Conclusion\n\nHopefully, this has shown how the Cloudant API for documents is really a tree manipulation API. There convenience wrappers and behaviours for common operations which make the API more database-like, most of which make use of the tree node labelled winner.\n\nIt's useful to think this way, as it explains what effect requests are having on stored data which helps with understanding how to make Cloudant work effectively. It also helps to understand replication as the replication process is just using calls like the ones above to retrieve and duplicate document tree nodes from one database to another.\n\nBefore finishing, it's worth repeating that the content of tree nodes that are not at the tip of a branch is discarded periodically by Cloudant -- so don't rely on it sticking around!",
    "url": "/2017/01/01/The-Cloudant-Revision-Tree.html",
    "tags": "Revisions",
    "id": "12"
  },
  {
    "title": "Build a serverless web app",
    "description": "Using Cloud Functions & Cloudant",
    "content": "\n\n\nIs it possible to create a web application that collects data from a web form without using any servers? Of course not, but you would be forgiven for thinking that when reading the word \"serverless\". \"Serverless\" means that instead of having fixed numbers of dedicated machines sitting in data centres waiting for traffic to come in, we can supply the code that handles a single piece of work (e.g. the submission of a web form) to a an application framework like [IBM Cloud Functions](https://www.ibm.com/cloud-computing/bluemix/openwhisk) and let *it* scale out the computing power required. If no traffic arrives, you don't pay anything - no fixed costs!\n\nLet's take an example. In the UK, there is a government petitions website where members of the public can gather support for issues they would like debated in the House of Commons. Recently, over 1.8m signatures were collected on a petition urging the government to [rescind Donald Trump's state visit](https://petition.parliament.uk/petitions/171928) invitation.\n\nIf you were building an IT system to collect large-scale public petitions, you would have no idea how popular each was to be or how much computing power you'd need to make the site performant. The \"serverless\" paradigm is compelling in this use-case, as you only pay for the traffic you generate.\n\n## Let's build a large-scale petition system\n\nModelling our system on the UK model, our petition system will have a public-facing website with a simple form detailing the issue being \"signed\". The user supplies their name, location, email addresss, confirms that they are a resident of the country in question, and submits the form. Our app saves the data in a Cloudant database and sends a verification email to the signatory.\n\n![workflow1](/img/workflow1.png)\n\nWhen the email arrives, the user clicks on the link to confirm and sign the petition\n\n\n![workflow2](/img/workflow2.png)\n\nAt some future date, we can count the confirmed records in the database to see if the petition has reached the 100,000 records needed to trigger a debate in parliament.\n\nWe want our *whole infrastructure* to be \"serverless\" i.e. we won't stand up our own servers to run the website - the hosting, form handling, email sending, and database will be run by others with minimal or ideally no fixed costs.\n\n## Serverless hosting\n\nAn obvious choice for \"serverless\" web hosting is [GitHub Pages](https://pages.github.com/). Sign up for GitHub, create a repository, and put some HTML/JavaScript/CSS in a `gh-pages` branch. Your website will be served out at `http://USERNAME.github.io/REPO/`. You can even point your  custom domain name to GitHub and it serves out your pages on that domain e.g. `mypetition.com`.\n\n## Serverless computing\n\nUse [IBM Cloud Functions](https://www.ibm.com/cloud-computing/bluemix/openwhisk) to handle form submissions and email verification requests. OpenWhisk lets you write your code in JavaScript, Swift, or Python and have it run in response to incoming events--in this case the submission of a web form or the clicking of a link in an email.\n\n## Serverless transactional email\n\nIn order to send emails to signatories, you need a transactional email service. I set up an email template with placeholders for the content that changes between recipients. The email provider ensures that the emails are sent, avoiding each users' spam filter. I used [SendGrid](https://sendgrid.com/) because they let you send a templated email via a simple API call.\n\n## Serverless database storage\n\nUltimately we need to store our data *somewhere*. So *someone* is going to have to manage some servers and disks. By using the [Cloudant](https://console.ng.bluemix.net/catalog/cloudant-nosql-db) database-as-a-service, we don't have to have fixed servers dedicated to us; we can simply consume a database service on a shared cluster and pay only for the requests and storage that we use. \n\n## Designing our data flows\n\nOpenWhisk computing tasks are built up from *actions*. You can combine many *actions* into *sequences* of actions and activate them with incoming data (such as an API call).\n\nCreate two API calls that activate OpenWhisk computing resources:\n\n- `POST /petition/submit` - to verify a form submission, save the data to Cloudant, and dispatch the email.\n- `POST /petition/confirm` - to confirm that the user has clicked on a link from the email. Another GitHub pages page renders on the browser, which makes an API call to OpenWhisk. The database record updates to mark the signature as \"confirmed\".\n\n![workflow3](/img/workflow3.png)\n\nThe `POST /petition/submit` is actually three separate OpenWhisk actions chained in a sequence, whereas `POST /petition/confirm` is configured as a single OpenWhisk action.\n\nOpenWhisk uses IBM's [API Connect](https://console.ng.bluemix.net/docs/services/apiconnect/index.html) to map actions and action sequences to public-facing API calls.\n\n## Sign up for the services\n\nGoing serverless means you don't have to deal with equipment, virtual servers, operating systems or networking, but you still have to sign up for some accounts:\n\n- sign up for an [IBM Bluemix](https://bluemix.net) account\n- follow [Getting started with OpenWhisk](https://console.ng.bluemix.net/docs/openwhisk/index.html#getting-started-with-openwhisk) to get the `wsk` tool installed and authenticated\n- inside Bluemix, sign up for a Cloudant service. Make a note of the Cloudant URL, including username and password.\n- setup a [SendGrid](https://sendgrid.com/) account, create an API key that has permissions to send transactional emails and create an email template\n- create a [Github](https://github.com/) account create a repository with your web site in it and branch to a `gh-pages` branch\n\n## Deploying Cloud Functions code\n\nIBM Cloud Functions has nice dashboard where you can paste your code, build sequences, and try your code. This is fine when taking your first few steps with \"serverless\" but very soon you'll want to script the deployment of your code so that it can be automated and reproduced easily.\n\nAssuming you have the `wsk` tool installed and it is authenticated against your Bluemix service, then installing our code is a breeze. Just clone the *git* repository 'https://github.com/ibm-cds-labs/online-petition.git' and run the `deploy.sh` script from the `openwhisk` directory.\n\nThis script assumes that you have the following environment variables set up:\n\n- `COUCH_URL` = the URL of the Cloudant service e.g. https://u:p@host.cloudant.com\n- `COUCH_DBNAME` = the name of the Cloudant database e.g. petition\n- `SENDGRIDBEARER` = the SendGrid API key \n- `SENDGRIDSENDER` = the email address that emails will appear to come from\n- `SENDGRIDTEMPLATEID` = the SendGrid email template id to email \n\nGIST! https://gist.github.com/glynnbird/2a80a059f4002e327f42e83feebfef7f\n\nIt's worth understanding each step in the deployment script. Notice how the credentials are encapsulated into a 'package' called 'petition' to which each of the actions is added. Then three of the actions are combined into a sequence before they are finally exposed as public API calls using the `api-experimental` command. Each API call also has an 'OPTIONS' method - this is quirk of CORS restrictions (the rules that prevent a website from making API calls to other servers). We need the dummy 'OPTIONS' API call to convince the browser that the main 'POST' request is permitted.\n\n## Try it yourself\n\nYou can sign our demo petition yourself [here](https://ibm-cds-labs.github.io/online-petition/). The code that appears on that page is in [this GitHub repository](https://github.com/ibm-cds-labs/online-petition). The IBM Cloud Functions \"actions\" and the deployment script can be found in the [openwhisk directory](https://github.com/ibm-cds-labs/online-petition/tree/master/openwhisk).\n\n## Who's paying the bill?\n\nSo I have a totally \"serverless\" system with Github serving the static content, IBM Cloud Functions handling the form submissions, SendGrid sending emails, and Cloudant storing the data. But is serverless free? \n\n> Not free - it's pay-as-you-go.\n\n- GitHub reserves the right to [disable or throttle](https://help.github.com/articles/github-terms-of-service/) your site's usage if your content is very popular\n- SendGrid has a [range of plans](https://sendgrid.com/pricing/) that start at free for the first month but typically allow you to send tens of thousands of emails from $10 per month.\n- [IBM Cloud Functions has a free tier](https://console.ng.bluemix.net/openwhisk/learn/pricing) and then takes payment depending on the volume, memory, and execution time of your actions.\n- Cloudant's default [Lite Plan](https://console.ng.bluemix.net/docs/services/Cloudant/offerings/bluemix.html#ibm-bluemix) offers a limited API call limit for free, or increased capacity for additional dollars per month.\n\nSo it's possible to create something like this and run it for free for a while (until your free trial runs out!) and up to a certain amount of traffic (depending on whether you run out of hosting, email capacity, or database storage first).\n\nThe important point is that you can setup an IT system with $0 fixed costs, paying extra money to add extra capacity. IBM Cloud Functions lets you  scale your application to deal with spikes in traffic but have no fixed costs for when there is no demand.\n\n\n",
    "url": "/2017/02/27/Online-petition-system.html",
    "tags": "Serverless",
    "id": "13"
  },
  {
    "title": "Moving data from DynamoDB",
    "description": "Using the dynamodbexport npm module",
    "content": "\n\n\nIf you have data in an Amazon DynamoDB service and want to move it to IBM Cloudant or Apache CouchDB, how would you go about it? First of all, DynamoDB has a peculiar form of JSON. A single temperature measurement would be expressed like this:\n\n```js\n{\n\t\"temperature\": {\n\t\t\"N\": \"8391\"\n\t},\n\t\"time\": {\n\t\t\"S\": \"2017-03-09T01:38:11+0000\"\n\t},\n\t\"id\": {\n\t\t\"S\": \"1489023491\"\n\t}\n}\n```\n\nInstead of the more straightforward JSON:\n\n```js\n{\n\t\"temperature\": 8391,\n\t\"time\": \"2017-03-09T01:38:11+0000\",\n\t\"id\": \"1489023491\"\n}\n```\n\nCloudant and CouchDB can store any JSON documents with nested objects of arbitrary complexity, whereas DynamoDB stores a series of key values at the top of the JSON tree.\n\n## Getting the data out\n\nThe [AWS SDK](https://www.npmjs.com/package/aws-sdk) provides a comprehensive toolkit for interacting with AWS services. You need to to \"scan\" a whole table, performing a chain of API calls to pull back records in batches until you've consumed them all. I've written a script to do this for you: [dynamodbexport](https://www.npmjs.com/package/dynamodbexport).\n\nFirst, install the tool:\n\n```sh\n$ npm install -g dynamodbexport\n```\n\nDefine a couple of environment variables with your Amazon API credentials:\n\n```sh\n$ export AWS_ACCESS_KEY_ID=\"OGIIWJGNWNIITJHWTHSO\"\n$ export AWS_SECRET_ACCESS_KEY=\"YRPHIIIWJJJYwKLGV28JJuiuwnjiiqq06ASn\"\n```\n\nThen simply run *dynamodbexport*, supplying the name of the table to export and the AWS region it is hosted in:\n\n```sh\n$ dynamodbexport --table iot --region us-east-1\n{\"temperature\":30730,\"time\":\"2017-03-09T02:21:48+0000\",\"id\":\"1489026108\"}\n{\"temperature\":17072,\"time\":\"2017-03-09T02:15:22+0000\",\"id\":\"1489025722\"}\n{\"temperature\":18177,\"time\":\"2017-03-08T21:27:23+0000\",\"id\":\"1489008443\"}\nExport complete { iterations: 1, records: 3, time: 0.145 }\n```\n\nThe tool makes as many API calls as it needs to extract the data, converting the JSON to a more compact form as it goes.\n\n## Importing into CouchDB/Cloudant\n\nI already have a tool to import data into CouchDB: [couchdbimport](https://www.npmjs.com/package/couchdbimport), which you can install in a similar way:\n\n```sh\n$ npm install -g couchimport\n```\n\nSet an environment variable with your target Cloudant/CouchDB service's URL:\n\n```sh\n$ export COUCH_URL=\"https://MYUSERNAME:MYPASSWORD@MYHOST.cloudant.com\"\n```\n\nThen run both the `dynamodbexport` and `couchimport` commands together, piping the output of the former into the latter:\n\n```sh\n$ dynamodbexport --table iot --region us-east-1 | couchimport --db iot --type jsonl\n```\n\nThe `--type jsonl` parameter tells couchimport that it is to expect one JSON document per line and `--db iot` defines the name of the target database. (Make sure your target database already exists, since couchimport will not create new databases.)\n\nIt's that simple! You can now use Cloudant's awesome [MapReduce](https://console.ng.bluemix.net/docs/services/Cloudant/api/creating_views.html#views-mapreduce-) tools to aggregate the data or [replicate](https://console.ng.bluemix.net/docs/services/Cloudant/api/replication.html#replication) it to other devices.\n\n## It works for local databases too\n\nThe *dynamodbexport* tool also works for [local DynamoDB](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBLocal.html) databases. Just leave out the `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` environment variables, and it will assume a local database. \n\nMoving data from local DynamoDB to local CouchDB is as simple as:\n\n```sh\n$ dynamodbexport --table iot | couchimport --db iot\n```\n\nYou'll find more details on command-line usage and programmatic access for [dynamodbexport on npm](https://www.npmjs.com/package/dynamodbexport).",
    "url": "/2017/03/24/Moving-data-from-DynamoDB.html",
    "tags": "DynamoDB",
    "id": "14"
  },
  {
    "title": "Moving data from DocumentDB",
    "description": "Using the documentdbexport npm module",
    "content": "\n\n\nIn this article we're going to look at extracting data from Microsoft Azure's DocumentDB service and then importing the data into IBM Cloudant.\n\n![documentdb]({{< param \"image\" >}})\n\n## Getting the data out\n\nOnce again, I've written a script to do this for you: [documentdbexport](https://www.npmjs.com/package/documentdbexport).\n\nFirst, install the tool:\n\n```sh\n$ npm install -g documentdbexport\n```\n\nDefine a couple of environment variables with your Azure credentials:\n\n```sh\n$ export AZURE_ENDPOINT=\"https://mydocumentdb.documents.azure.com:443/\"\n$ export AZURE_KEY=\"GeIZysnonvgpk2\"\n```\n\nThen, simply run *documentdbexport*, supplying the name of the database and collection to export:\n\n```sh\n$ documentdbexport --database iot --collection temperaturereadings\n{\"temperature\":30730,\"time\":\"2017-03-09T02:21:48+0000\",\"id\":\"1489026108\"}\n{\"temperature\":17072,\"time\":\"2017-03-09T02:15:22+0000\",\"id\":\"1489025722\"}\n{\"temperature\":18177,\"time\":\"2017-03-08T21:27:23+0000\",\"id\":\"1489008443\"}\nExport complete { records: 3, time: 0.145 }\n```\n\nThe tool makes as many API calls as it needs to extract the data, converting the JSON to a more compact form as it goes.\n\n## Importing into CouchDB/Cloudant\n\nWe can use [couchdbimport](https://www.npmjs.com/package/couchdbimport) to do the import stage for us. Install it with:\n\n```sh\n$ npm install -g couchimport\n```\n\nSet an environment variable with your target Cloudant/CouchDB service's URL:\n\n```sh\n$ export COUCH_URL=\"https://MYUSERNAME:MYPASSWORD@MYHOST.cloudant.com\"\n```\n\nThen, run both the `documentdbexport ` and `couchimport` commands together, piping the output of the former into the latter:\n\n```sh\n$ documentdbexport --database iot --collection temperaturereadings | couchimport --db iot --type jsonl\n```\n\nThe `--type jsonl` parameter tells couchimport that it is to expect one JSON document per line and `--db iot` defines the name of the target database.\n\nIt's that simple! You’ll find more details on command-line usage and programmatic access for [documentdbexport on npm](https://www.npmjs.com/package/documentdbexport). \n",
    "url": "/2017/03/28/Moving-data-from-DocumenDB.html",
    "tags": "DocumentDB",
    "id": "15"
  },
  {
    "title": "PouchDB",
    "description": "The Swiss Army Knife of databases",
    "content": "\n\n\n[PouchDB](https://pouchdb.com/) is a database. It's a JSON document store to be precise, allowing you to create, read, update, delete and query your documents with a simple JavaScript API. It's most commonly used *in a browser*, to keep data on the client side. Storing data *within* the browser allows your web applications to keep working, even when the network connection is flaky or non-existant - this is called an __offline first__ approach. Offline first brings 100% uptime to web applications together with performance gains, better user experience and powers the *Progressive Web App (PWA)* movement.\n\nPouchDB goes to great lengths to store data in a format that allows the client side to be disconnected from the server side, for both copies to be updated (even when updated in different ways) and for the data to *synced* without the loss of data. This is no mean feat - syncing data across a distributed systems is not a problem you want to tackle yourself and PouchDB solves it for you at a stroke.\n\nPouchDB stands on the shoulders of the [Apache CouchDB](http://couchdb.apache.org/) project which defines the protocol and the storage mechanism that allows the seamless syncing of data between occasionally connected replicas. PouchDB to CouchDB, PouchDB to Cloudant, PouchDB to PouchDB - you decide. \n\n## PouchDB - the in-browser database\n\nStoring data in a web application couldn't be simpler. Get [PouchDB into your web page's code](https://pouchdb.com/guides/setup-pouchdb.html) by which ever means you prefer and then write some JavaScript:\n\n```js\nvar db = new PouchDB('mydb');\nvar doc = { \n  name: 'Glynn', \n  team: 'blue', \n  date: '2017-03-24', \n  verified: true \n};\ndb.put(doc);\n```\n\nRead documents back:\n\n```js\ndb.allDocs();\n```\n\nor [run queries](https://github.com/nolanlawson/pouchdb-find), [calculate aggregations](https://pouchdb.com/api.html#query_database) and [lots more](https://pouchdb.com/api.html#overview).\n\n## PouchDB - the server-side database\n\nYou can also use PouchDB as the storage mechanism for your single-server, client-side Node.js application. Just `npm install pouchdb` and \"require\" the library into your code:\n\n```js\nvar PouchDB = require('pouchdb');\nvar db = new PouchDB('mydb');\n```\n\nand the rest of the code as the same as the client-side example.\n\n## PouchDB - the client library for CouchDB/Cloudant\n\nIf you want to write client-side code that speaks to a remote CouchDB or Cloudant server, then you can use PouchDB as a client library:\n\n```js\nvar remotedb = new PouchDB('https://MYUSERNAME:MYPASSWORD@MYHOST.cloudant.com/mydb');\nremotedb.put(doc);\n```\n\nBy providing a URL instead of database name, PouchDB makes API calls to the remote database, using the same code as if it were a local database.\n\n## PouchDB - that syncing feeling\n\nSyncing data from client to server side copies is also painless. On the client side:\n\n```js\ndb.sync(remotedb);\n```\n\nThat's it. Two-way sync from your in-browser database to a remote copy. See [the documentation](https://pouchdb.com/api.html#sync) for more examples, one-way sync and receiving update notifications.\n\n## PouchDB - the HTTP server\n\nIf you want to run PouchDB as if it were an Apache CouchDB service, with HTTP API, dashboard and all, then you're only a couple of commands away:\n\n```sh\nnpm install -g pouchdb-server\npouchdb-server\n```\n\nThen visit http://127.0.0.1:5984/_utils/ in your browser to see the dashboard - apart from the colour scheme, you'd be hard-pushed to notice the difference between this and full-fat CouchDB.\n\n![pouchdb server](/img/pouchdb-server.png)\n\n## PouchDB - the app data layer\n\nIf you are building [Apache Cordova](https://cordova.apache.org/)-based native mobile applications or [Electron](https://electron.atom.io/)-based desktop apps, then PouchDB can be included in the project and used to provide the data storage API. Storing data with PouchDB unlocks your app's ability to sync its data to the server allowing your app to uncouple from the network and roam free, as apps were intended to do!\n\n## PouchDB - plug in for extra functionality\n\nBy adding other plugins, PouchDB can be extended to perform [peer-to-peer sync](https://github.com/natevw/PeerPouch), [social media authentication](https://www.npmjs.com/package/superlogin), [framework integration](https://github.com/angular-pouchdb/angular-pouchdb) and [much more](https://pouchdb.com/external.html).\n\n## PouchDB - where's the catch?\n\nHow much does this cost? What are the licensing restrictions? Where do I enter my credit card details? PouchDB is a free, open-source project maintained by volunteers. You are free to use it in your own projects within the generous provisions of the [Apache-2.0](https://github.com/pouchdb/pouchdb/blob/master/LICENSE) license, so what are you waiting for?\n\n## Further reading\n\n* [Installing Web Apps with Electron](https://medium.com/ibm-watson-data-lab/installing-web-apps-with-electron-7a8fa1b12744)\n* [Offline First](http://offlinefirst.org/)\n* [Scaling Offline First with Envoy](https://medium.com/offline-camp/scaling-offline-first-with-envoy-ada42d130cfc)\n* [PouchDB](https://pouchdb.com/)",
    "url": "/2017/05/04/PouchDB-swiss-army-knife.html",
    "tags": "PouchDB",
    "id": "16"
  },
  {
    "title": "couchdiff",
    "description": "Compare two databases on the command line",
    "content": "\n\n\nThe Unix `diff` command-line utility has been around since the 1970s. It compares two text files line by line and tells you the differences between them.\n\n![diff]({{< param \"image\" >}})\n> Photo by [Hermes Rivera](https://unsplash.com/photos/OX_en7CXMj4) on Unsplash\n\nIf I `diff` two files [a.txt](https://gist.github.com/glynnbird/03ea265a3a670e8b514512e36fd63582) and [b.txt](https://gist.github.com/glynnbird/c0705e06d633cdbc40b680b2d8e69c96) containing versions of the same poem I can find the differences between them with the command:\n\n```sh\n$ diff a.txt b.txt\n2c2\n< T.S. Eliot 1888-1965\n",
    "url": "/2017/05/10/Diff-your-database-couchdiff.html",
    "tags": "CLI",
    "id": "17"
  },
  {
    "title": "Choosing a Cloudant library",
    "description": "Which level of abstraction is right for you?",
    "content": "\n\n\nThe beauty of Apache CouchDB and Cloudant is that you don't need to a library to be able to start using it. Some databases require a \"driver\" module to be installed to handle communication between your application and your database, but when your database speaks HTTP then you only need `curl`, a web browser, or anything that can make web requests. For example:\n\n- your Raspberry Pi could write IoT data to a remote database by making PUT requests from curl\n- your web page could fetch data directly from the database by making in-page HTTP calls\n- your PHP code could read and write from its data store without any third-party add-on code\n\nSometimes developers need a little help. To avoid repeating the same low-level code, to abstract the API calls into more semantically meaningful methods, and to *make life easier* we often employ libraries.\n\n![library]({{< param \"image\" >}})\n> Photo by [Tobias Fischer](https://unsplash.com/photos/PkbZahEG2Ng) on Unsplash\n\nIn this article we'll explore some options that a JavaScript/Node.js developer could choose when writing code against CouchDB or Cloudant, from the lowest level to the highest.\n\n## Level 0 - no libraries\n\nIf you want to learn the HTTP in detail, then you can choose to use no libraries whatsoever:\n\n```js\nconst url = process.env.CLOUDANT_URL;\nconst db = 'appointments';\nconst https = require('https');\nconst req = https.request(url + '/' + db + '/_all_docs?include_docs=true', (res) => {\n  res.on('data', (chunk) => {\n    process.stdout.write(chunk);\n  });\n});\n\nreq.on('error', (e) => {\n  console.error(e.message);\n});\nreq.end();\n```\n\nThis approach uses the [Node.js https](https://nodejs.org/api/https.html) library to make a single API call. It leaves you to formulate your own URL and to join the separate chunks of reply data into a complete JSON response.\n\n## Level 1 - an HTTP request library\n\nTo help with formulating HTTP requests, there a several third-party HTTP libraries to choose from. I usually go for [request](http://npmjs.com/package/request), but others are available.\n\n```js\nvar request = require('request');\nconst url = process.env.CLOUDANT_URL;\nconst db = 'appointments';\nvar r = {\n  method: 'get',\n  url: url + '/' + db + '/_all_docs',\n  qs: {\n    include_docs: true\n  },\n  json: true\n};\nrequest(r, function(err, response, body) {\n  console.log(body);\n}); \n```\n\nThe *request* module makes it simpler to deal with HTTP requests, and if you ask it nicely, it will parse the JSON response for you too. You still get to learn the CouchDB API, but the mechanics of making the HTTP call are simplified.\n\n## Level 2 - the Nano library\n\n[Nano](https://www.npmjs.com/package/nano) is an open-source project that was donated to the Apache Software Foundation and has become the official Node.js library for CouchDB.\n\nIt doesn't actually do much &mdash; it is a thin wrapper around CouchDB's API &mdash; but it does make your code a little easier to write and to maintain:\n\n```js\nvar nano = require('nano');\nconst url = process.env.CLOUDANT_URL;\nconst db = 'appointments';\nvar mydb = nano(url).db.use(db);\nmydb.list({include_docs:true}, function(err, data) {\n  console.log(data);\n});\n```\n\nUsing Nano allows you to abstract the API calls away. In this case, the `list` function makes a `GET /db/_all_docs` API call. You can use the Nano library and not know what API calls are being made on your behalf. \n\n## Level 3 - the Cloudant SDK\n\nThe [Cloudant](https://www.npmjs.com/package/cloudant) library is the officially support method of interacting with your Cloudant database:\n\n```js\nconst { CloudantV1 } = require('@ibm-cloud/cloudant')\nconst service = CloudantV1.newInstance()\nconst DB = 'appointments'\nconst list = await service.postAllDocs({\n  db: DB\n  includeDocs: true\n})\nconsole.log(list)\n```\n\n## Level 4 - PouchDB\n\nYou can use [PouchDB](https://www.npmjs.com/package/pouchdb-http) as an HTTP-only client too:\n\n```js\nvar PouchDB = require('pouchdb-http');\nconst url = process.env.CLOUDANT_URL;\nvar db = new PouchDB(url);\ndb.allDocs({include_docs:true}).then(console.log);\n```\n\nIt has its own naming convention for functions, but function calls result in the equivalent API call. If you're developing with PouchDB on the client side, it may be easier to stick with the same API to deal with your server-side CouchDB or Cloudant database.\n\n\n## Alternatives\n\nThe great thing about open-source is that you aren't limited to \"official\" products. If you don't like the tools, help improve the open-source offerrings by raising issues or submitting code &mdash; find alternative tools or build your own! You can choose whether you're looking for a library that can do callbacks or Promises and whether it allows you to learn the CouchDB API or hides it from you.\n\nIn my opinion, it makes sense to get started with a high-level library that abstracts and hides details from you at first. It allows you to build more quickly with less distraction. As you get more serious in your work, you may find you need to see more detail and switch to a lower-level of abstraction.",
    "url": "/2017/07/12/Choosing-a-Cloudant-library.html",
    "tags": "Library Node.js",
    "id": "18"
  },
  {
    "title": "Querying Cloudant with SQL",
    "description": "Converting SQL to Cloudant Query JSON",
    "content": "\n\n\nCloudant and its Apache CouchDB stable-mate are \"NoSQL\" databases&mdash;that is, they are schemaless JSON document stores. Unlike a traditional relational database, you don't need to define your schema before writing data to the database. Just post your JSON to the database and change your mind as often as you like!\n\nOne of the appealing things about relational databases is the query language. [Structured Query Language or SQL](https://en.wikipedia.org/wiki/SQL) was developed by IBM in the 1970s and was widely adopted across a host of databases ever since. In its simplest form, SQL reads like a sentence:\n\n```sql\nSELECT name, colour, price \n\tFROM animals\n\tWHERE type='cat' OR (price > 500 AND price < 1000) \n\tLIMIT 50\n```\n \nThis statement translates to:\n\n> \"Fetch me the name, colour and price from the animals database, but only the records that are cats, or ones which are more expensive than 500 but cheaper than 1000. And I only want a maximum of 50 records returned.\"\n\nIt is a convenient way of expressing the fields you want to fetch, the filter you wish to apply to the data, and the maximum number of records you want in reply.\n\n![sorting]({{< param \"image\" >}})\n> Photo by [H E N G S T R E A M](https://unsplash.com/photos/pjJdOE2XBRU) on Unsplash\n\nUnfortunately, NoSQL databases don't generally support the SQL language. Cloudant and Apache CouchDB™ have their own form of query language where the query is expressed as a JSON object: \"[Cloudant Query](https://console.bluemix.net/docs/services/Cloudant/api/cloudant_query.html#query)\" (CQ) and \"[Mango](https://blog.couchdb.org/2016/08/03/feature-mango-query/),\" in their respective contexts. The CQ or Mango equivalent of the above SQL statement is:\n\n```js\n\t{\n\t \"fields\": [\n\t  \"name\",\n\t  \"colour\",\n\t  \"price\"\n\t ],\n\t \"selector\": {\n\t  \"$or\": [\n\t   {\n\t    \"type\": {\n\t     \"$eq\": \"cat\"\n\t    }\n\t   },\n\t   {\n\t    \"$and\": [\n\t     {\n\t      \"price\": {\n\t       \"$gt\": 500\n\t      }\n\t     },\n\t     {\n\t      \"price\": {\n\t       \"$lt\": 1000\n\t      }\n\t     }\n\t    ]\n\t   }\n\t  ]\n\t },\n\t \"limit\": 50\n\t}\n```\n\nIt's a world of curly brackets! If you're happier expressing your query in SQL, then there is a way.\n\n## cloudant-quickstart + SQL\n\nThe latest version of the [cloudant-quickstart](https://www.npmjs.com/package/cloudant-quickstart) Node.js library can now accept SQL queries. It will convert the SQL into a Cloudant Query and deliver the results.\n\nSimply install the `cloudant-quickstart` library:\n\n```sh\nnpm install -s cloudant-quickstart\n```\n    \nAnd add it to your Node.js app by passing your Cloudant URL to the library:\n\n```js\n  var db = require('cloudant-quickstart')('https://USER:PASS@HOST.cloudant.com/DATABASE');\n```\n\nWe can then start querying our database with an SQL statement:\n\n```js\n  db.query('SELECT name FROM mydb').then(function(data) {\n    // data!\n  });\n```\n\nHere are some other sample queries:\n\n```sql\n// fetch all fields\nSELECT * FROM mydb\n\n// fetch selected fields\nSELECT name, colour, price FROM mydb\n\n// fetch data with WHERE clause\nSELECT name FROM mydb WHERE colour = 'tabby'\n\n// fetch data with a more complex WHERE clause\nSELECT name FROM mydb WHERE type!='cat' OR (price > 500 AND price < 1000) \n\n// limit the number of items returned\nSELECT name FROM mydb LIMIT 10\n\n// limit the number of items and skip records\nSELECT name FROM mydb LIMIT 20,10\n\n// ordering ascending\nSELECT name FROM mydb ORDER BY price\n\n// ordering descending\nSELECT name FROM mydb ORDER BY price DESC\n\n// multiple field ordering descending\nSELECT name FROM mydb ORDER BY type,price\n\n// all together\nSELECT name,colour,price FROM mydb WHERE type!='cat' OR (price > 500 AND price < 1000) ORDER BY type, price LIMIT 20,10\n```\n\n`cloudant-quickstart` achieves this by converting your SQL query into the equivalent Cloudant Query object. If you'd like to see that data yourself, then call the `explain` function instead of `query` to be returned by the query that would have been used:\n\n```js\nvar q = db.explain(\"SELECT name FROM mydb WHERE type!='cat' OR (price > 500 AND price < 1000)\");\nconsole.log(JSON.stringify(q));\n// {\"fields\":[\"name\"],\"selector\":{\"$or\":[{\"type\":{\"$ne\":\"cat\"}},{\"$and\":[{\"price\":{\"$gt\":500}},{\"price\":{\"$lt\":1000}}]}]}}\n```\n\n## Limitations\n\nBefore we get carried away, this feature doesn't suddenly make Cloudant support joins, unions, transactions, stored procedures etc. It's just a translation from [SQL to Cloudant Query](https://github.com/ibm-watson-data-lab/sqltomango). \n\nIt doesn't support aggregations or grouping either, but you can use `cloudant-quickstart`'s `count`, `sum`, and `stats` functions to generate performant grouped aggregation without any fuss.\n\nThis feature simply makes it easier to explore data sets if you already have SQL language experience.\n\n\n",
    "url": "/2017/07/18/Querying-Cloudant-with-SQL.html",
    "tags": "SQL Query",
    "id": "19"
  },
  {
    "title": "Piecemeal, Bulk or Batch",
    "description": "Cloudant write modes discussed",
    "content": "\n\n\nThere are many different deployment models for CouchDB-style databases, but thankfully [CRUD operations](https://en.wikipedia.org/wiki/Create,_read,_update_and_delete) work the same across all of them. Apache CouchDB&trade; is a database, specifically a JSON document store, with an HTTP API. IBM Cloudant is Apache CouchDB with a few extra bells and whistles run as-a-service in pay-as-you go, dedicated and local configurations.\n\nIn this article, I'm going to explain your options for writing data using the CouchDB API, the different calls, and the tradeoffs therein. First, however, it will help to understand the basics of CouchDB as a distributed system, and what the database means when it says your writes are written.\n\n## CouchDB clusters 101\n\nA CouchDB cluster is a distributed system that exposes a single API&mdash;you treat your CouchDB cluster as a single data store, but behind the scenes your database is divided into *shards* and multiple copies of your documents are stored on separate machines.\n\n![diagram](/img/couchdb_cluster_1.png)\n\nThe larger the number of nodes in the cluster, the greater the volume of data and the number of concurrent requests it can handle.\n\nWhen you write data to a CouchDB cluster (or a Cloudant service), by default, two or more copies of your document are persisted **on disk** (for example, on 2 or more machines in a 3-node cluster). Other database systems may give you the thumbs-up to your write requests *before* the data is written to disk as a speed optimisation&mdash;behaviour that risks data loss in the event of a node failure.\n\nNow that you know the basics of what's happening behind the scenes, I'll cover the API calls that allow you to write data to CouchDB and the options you have that trade-off storage guarantees and performance.\n\n## Piecemeal writes\n\nWriting data to CouchDB is simply an HTTP POST request:\n\n```sh\ncurl -v -X POST \\\n     -H 'Content-type: application/json' \\\n     -d '{\"name\": \"Mittens\", \"type\": \"cat\"}' \\\n     \"$COUCH_URL/animals\"\n\nHTTP/1.1 201 Created\nCache-Control: must-revalidate\nContent-Length: 95\nContent-Type: application/json\nDate: Fri, 02 Jun 2017 06:33:08 GMT\nLocation: http://localhost:5984/animals/\n\n{\"ok\":true,\"id\":\"7bff55e2a7f9fa3a999c1f76bd00044b\",\"rev\":\"1-76558a77771fb4c1f81d4d91144dc83f\"}\n```\n\nHere, I POST a JSON document to my database, and the reply indicates the auto-generated `id` of the document that was created. The \"201\" response code indicates success and guarantees that the document was stored on a [quorum of servers](https://en.wikipedia.org/wiki/Quorum_(distributed_computing)) in the cluster (at least 2 of 3, in this case).\n\n![diagram](/img/couchdb_cluster_2.png)\n\nThis process is important for mission-critial data. It means that if the servers were abruptly powered off, your data would be safe on disk on mulitple machines. \n\n## Bulk writes\n\nIf you have lots of data to write to the database, then a single bulk API request is more efficent than making several individual API calls. More efficicent in terms of fewer HTTP round trips, and more efficient for the database cluster too:\n\n```sh\ncurl -v -X POST \\\n     -H 'Content-type: application/json' \\\n     -d '{\"docs\": [{\"name\": \"Snowy\", \"type\": \"cat\"},{\"name\": \"Patch\", \"type\": \"dog\"}]}' \\\n     \"$COUCH_URL/animals/_bulk_docs\"\n\nHTTP/1.1 201 Created\nCache-Control: must-revalidate\nContent-Length: 192\nContent-Type: application/json\nDate: Fri, 02 Jun 2017 06:44:45 GMT\n\n[{\"ok\":true,\"id\":\"7bff55e2a7f9fa3a999c1f76bd001d39\",\"rev\":\"1-263fbfee100b3417c513b14f4dacd776\"},{\"ok\":true,\"id\":\"7bff55e2a7f9fa3a999c1f76bd00202b\",\"rev\":\"1-591fadc21c08df0ba8efa5c5912c1cfb\"}]\n```\n\n![diagram](/img/couchdb_cluster_3.png)\n\nIn this case I supply an object containing an array of documents and, in reply, I receive an array of objects. The body can contain inserts, updates, and deletes:\n\n```js\n{\n  \"docs\": [\n    { \"name\": \"Paws\", \"type\": \"cat\" },\n    { \"_id\": \"7bff55e2a7f9fa3a999c1f76bd001d39\", \"_rev\": \"1-263fbfee100b3417c513b14f4dacd776\", \"name\": \"Snowie\", \"type\": \"cat\"},\n    { \"_id\": \"7bff55e2a7f9fa3a999c1f76bd00202b\", \"_rev\": \"1-591fadc21c08df0ba8efa5c5912c1cfb\", \"_deleted\": true} \n  ]\n}\n```\n\n![diagram](/img/couchdb_cluster_4.png)\n\nIs there a limit to how many documents should be posted in a single bulk request? There isn't a limit *per se*, but 500 small documents is a reasonable rule of thumb.\n\n> Note: The pay-as-you-go Cloudant plans limit the size of POST requests to 1MB.\n\n## Batch writes\n\nIn some circumstances, it is not possible to combine writes into fewer bulk requests. If your application is running on a serverless platform such as [OpenWhisk](http://openwhisk.org/), then your code has no visibility into the other serverless actions that are performing similar requests concurrently.\n\nThis is where [batch mode](http://docs.couchdb.org/en/2.0.0/api/database/common.html#api-doc-batch-writes) may be of use. By supplying `?batch=ok` to a single write request, you are indicating to the server that you permit CouchDB to buffer the document in memory before writing it to disk in batches:\n\n```sh\ncurl -v -X POST \\\n     -H 'Content-type: application/json' \\\n     -d '{\"name\": \"Tiddles\", \"type\": \"cat\"}' \\\n     \"$COUCH_URL/animals?batch=ok\"\n\nHTTP/1.1 202 Accepted\nCache-Control: must-revalidate\nContent-Length: 52\nContent-Type: application/json\nDate: Fri, 02 Jun 2017 07:01:50 GMT\n\n{\"ok\":true,\"id\":\"7bff55e2a7f9fa3a999c1f76bd002cec\"}\n```\n\nIn this case, I get a \"202\" response, indicating that the document is accepted but not written to disk (yet). This behavoir is faster and more efficient than piecemeal \"write\" performance, but doesn't provide any persistance guarantees. Batch mode should not be used for writing critical data to the database but may be useful for some applications.\n\n## References\n\nIf you want to read a more scholarly article on how CouchDB handles writes, then [this blog by Mike Rhodes](\nhttps://dx13.co.uk/articles/2015/10/19/couchdb-20s-read-and-write-behaviour-in-a-cluster.html) is a great place to start.",
    "url": "/2017/07/25/Piecemeal-Bulk-or-Batch.html",
    "tags": "Serverless Replication",
    "id": "20"
  },
  {
    "title": "Custom Replication",
    "description": "With Cloud Functions",
    "content": "\n\n\nCloudant has first-class replication built in. A database can be replicated to another local database or to a remote Cloudant instance&mdash;or to any other database that speaks the same replication protocol, such as [Apache CouchDB&trade;](http://couchdb.apache.org/) or [PouchDB](https://pouchdb.com/). A replication process can be one-shot or continuous, and replication streams can be \"filtered\", i.e. the documents that are replicated can be a subset of the total.\n\nNot all applications need replication. It is essential when there are multiple, disconnected copies of the data where *the data can be modified on either side*. Cloudant solves the problem by never throwing away conflicting revisions of the same document and allows [your app to decide how to resolve the situation](https://developer.ibm.com/dwblog/2015/cloudant-document-conflicts-one/).\n\n### One-way street\n\nOther apps *look* like they need replication but only really involve the movement of data from one place to another in one direction. This is a much simpler problem to solve and lets us get creative with replication.\n\nLet's take the example of a transport system. A central database contains a continuously growing collection of events:\n\n- Bus AB12XJK has begun its journey on route X1 from Newcastle-upon-Tyne at 13:08 on 4th August 2017\n- Bus AB12XJK has stopped for a break at 15:08 on 4th August 2017\n- Bus AB12XJK has resumed its journey at at 15:30 on 4th August 2017\n- Bus AB12XJK has arrived at its destination (Victoria Coach Station) at 19:10 on on 4th August 2017\n\nThis trip could be modelled in a Cloudant database with the following document structure:\n\n```js\n{\n  \"_id\": \"3007166d-3fd3-4e3f-be0d-43aa9c054a48\",  // auto-generated id\n  \"_rev\": \"1-16e262673ed141f0b711f33e6bb0fdc1\", // revision token\n  \"route\": \"X1\",\n  \"name\": \"Newcastle to London Express\",\n  \"start\": \"Newcastle-upon-Tyne\",\n  \"end\": \"Victoria, London\",\n  \"scheduled_start\": \"2017-08-04 13:05:00 Z\",\n  \"actual_start\": \"2017-08-04 13:08:00 Z\",\n  \"scheduled_arrival\": \"2017-08-04 13:05:00 Z\",\n  \"estimated_arrival\": \"2017-08-04 13:08:00 Z\",\n  \"actual_arrival\": null,\n  \"stops\": [\n    {\n      \"type\": \"comfort_break\",\n      \"location\": \"Woodall services\",\n      \"start\": \"2017-08-04 15:00:00 Z\",\n      \"actual_start\": null,\n      \"end\": \"2017-08-04 15:30:00 Z\",\n      \"actual_end\": null\n    }\n  ],\n  \"driver\": {\n    \"name\": \"Sheila Davies\",\n    \"employee_num\": \"SD_1552\"\n  },\n  \"vehicle\": {\n    \"model\": \"Volvo 9700\",\n    \"registration\": \"PQ89MGW\"\n  }\n}\n```\n\nSome things to note about this document structure:\n\n- The document's id is auto-generated by the database, although you could provide your own.\n- The revision token contains a number (`1`), a hyphen (`-`), and a hash of the contents of the document (`16e262...`). If the document changes, the number will be incremented and the system will compute a new hash.\n- The other fields are up to us. Our data model allows for multiple \"stops\" on a journey (hence the array of objects).\n- This document contains everything we need to get a snapshot of the progress of this journey. Although it contains references to other data structures&mdash;route id, employee id, vehicle registration&mdash;Cloudant is not a relational database. So it is ok to take copies of related data in our object to allow us to get a useful view of the data without de-referencing everything.\n\nWe can use this data to show the progress of a particular journey on our company website, to perform analytics (e.g., statistics of how many journeys were late on arrival) and to power public displays at each end.\n\n![board](/img/board.jpg)\n> Image courtesy of [Flickr user Carlbob.com](http://bit.ly/2uZAwrk)\n\n### Moving the data\n\nLet's imagine we were building such a display. We would need:\n\n- A display!\n- The arrivals and departures data for the station in question.\n- A local data store so we can cache the data locally. If the remote database becomes disconnected, we can still render the most recent information.\n\nWe can use PouchDB or CouchDB within the display. Both are small enough to be incorporated into an embedded system, but we want to keep data volumes to a minimum. A single display only needs to know about journeys that list *it* as the starting or destination point. It would be overkill to replicate the *entire* database to each display.\n\n![all screens get all data](/img/busstation2.png)\n\nWe could use [Filtered Replication](https://console.bluemix.net/docs/services/Cloudant/api/advanced_replication.html#filtered-replication). This approach involves sending a JavaScript function to Cloudant&mdash;a function that decides which documents are replicated and which are not. It would be simple to create a filter by bus station (i.e., a function which passed any document whose start _or_ destination matches the display's station). But there are drawbacks to this approach.\n\n![filtered replication](/img/busstation3.png)\n\nBecause our database contains all documents back to the beginning of time, a first-time replication will begin at zero and have to spool through every document in turn. It would work eventually but is increasingly inefficient as the data size grows. \n\nAnother solution is to have a *copy* of each station's data in its own database:\n\n![bus station schematic](/img/busstation4.png)\n\nThis \"one database per station\" approach has some advantages:\n\n- The station display can replicate the data easily from its paired database without filtering and with a reduced data size.\n- The per-station databases can be destroyed and recreated daily, keeping the replicatable data sizes much smaller because the display boards are only interested in today's data and the data only pertains to *its* station.\n- The per-station databases could contain only a subset of the original document&mdash;just the bare minimum required for the display boards, keeping the document sizes small.\n\n### Building a custom Cloudant replication\n\nMy solution doesn't use Cloudant replication to feed the per-station databases. Instead, it uses the OpenWhisk serverless platform. An OpenWhisk Node.js function is called with each change on the main database. The code identifies which per-station databases need to be fed (the start and destination stations and any calling points along the way), prunes the document structure, and makes the writes to the relevant \"per station\" databases.\n\n![bus station schematic](/img/busstation1.png)\n\nHere's the [sample code](https://github.com/ibm-watson-data-lab/custom-replication). It includes the [OpenWhisk action](https://github.com/ibm-watson-data-lab/custom-replication/blob/master/onchange.js) that is called with every change and a [deployment script](https://github.com/ibm-watson-data-lab/custom-replication/blob/master/deploy.sh) that deploys it to OpenWhisk and creates the Cloudant changes feed trigger.\n\n## Building your own\n\nFork [the code](https://github.com/ibm-watson-data-lab/custom-replication) and build your own logic to decide how data is routed from your primary database to the secondary database(s).\n\nHopefully this article provided some new ideas for thinking about data movement in your own applications. If you enjoyed it, the clap button awaits you below.",
    "url": "/2017/08/22/Custom-Cloudant-replication.html",
    "tags": "Serverless Replication",
    "id": "21"
  },
  {
    "title": "Custom Indexers",
    "description": "With Cloud Functions and Redis",
    "content": "\n\n\nCloudant offers highly available storage and retrieval of JSON documents across a cluster of computing which includes a primary index on the documents' `_id` field. You can also use the same cluster to power secondary indexes built to provide [selection & aggregation using MapReduce](https://console.bluemix.net/docs/services/Cloudant/api/creating_views.html#views-mapreduce-), [full-text search using Apache Lucene](https://console.bluemix.net/docs/services/Cloudant/api/search.html#search), or [GeoSpatial queries](https://console.bluemix.net/docs/services/Cloudant/api/cloudant-geo.html#cloudant-geospatial).\n\nWhat if you want something a little different that isn't supported by the built-in indexers? You can build you own!\n\nIn this article, I'll build a custom in-memory index using Redis whose data is fed from a Cloudant database's changes feed.\n\n## The data\n\nLet's say we're storing documents in Cloudant that represent a page view on our website. Each document represents a single page view:\n\n```js\n{\n  \"_id\": \"96f898f0f6ff4a9baac4503992f31b01\",\n  \"_rev\": \"1-ff7b85665c4c297838963c80ecf481a3\",\n  \"path\": \"/blog/post-1.html\",\n  \"date\": \"2017-07-04 17:15:59 +00:00\",\n  \"mobile\": true,\n  \"browser\": \"Chrome\",\n  \"ip\": \"85.25.222.52\"\n}\n```\n\nThe documents are never modified. Instead, we keep writing new documents as events arrive. The data set just keeps growing.\n\n## Custom indexes \n\nWe are going to build two custom indexes in Redis that would be tricky or inefficient to achieve in Cloudant with its built-in indexers. In general, they are queries that do not lend themselves well to key range lookups or text searches:\n\n- Retrieve the top ten pages viewed\n- Find the number of unique IP addresses used to access our site\n\nWe may also want these statistics broken down by month. Let's examine each query in a little more detail.\n\n### Top ten pages\n\nIt's easy to count things in Cloudant. Simply create a MapReduce view with a JavaScript map function:\n\n```js\nfunction(doc) {\n  emit(doc.path, null);\n}\n```\n\nThe `map` function emits the path (e.g., `\"/blog/post-1.html\"`) with a null value. Then we can use the built-in [`_count`](https://console.bluemix.net/docs/services/Cloudant/api/creating_views.html#reduce-functions) reducer which will calculate counts of each value:\n\n```\n/blog/post-1.html --> 5028\n/blog/post-2.html --> 12025\n/blog/post-3.html --> 885\n```\n\nWhat it *doesn't* do for you is sort the list by count (it is sorted by the key instead). To get the most popular, you'd have to query *all* the totals and sort the results yourself. This isn't a big deal if you have a few distinct pages, but if you have tens of thousands, then it makes the query relatively expensive&mdash;especially if you need this answer frequently and quickly.\n\nTo solve this problem with Redis, we can store the blog post name and the count in a [Redis sorted set](https://redis.io/topics/data-types#sorted-sets):\n\n```\nZINCRBY 'leaderboard' '/blog/post-1.html' 1\n```\n\nThen it is easy to retrieve the top ten items quickly from the in-memory store:\n\n```\nZREVRANGEBYSCORE 'leaderboard' +inf -inf WITHSCORES LIMIT 0 10\n```\n\n### Distinct counts of IP addresses\n\nCounting the number of unique IP addresses visiting our site is an example of the [count-distinct problem](https://en.wikipedia.org/wiki/Count-distinct_problem). It uses more memory the more unique things you're counting, and with over 4 billion possible IP addresses, this operation has the potential to get tricky.\n\nRedis offers a solution to this problem with its [HyperLogLog data structure](http://antirez.com/news/75). It allows distinct counts to be performed with a fixed memory footprint of 12kb as long as you can accept an approximate answer (<1% error). \n\nData is added to the structure with `PFADD`:\n\n```\nPFADD ipcount \"85.25.222.52\"\n```\n\nAnd the count is retrieved with `PFCOUNT`:\n\n```\nPFCOUNT ipcount\n```\n\n## Streaming the changes \n\nWe can write a simple Node.js script that glues together Cloudant and Redis. Here's what we want it to do:\n\n- Listen to a Cloudant database's [changes feed](https://console.bluemix.net/docs/services/Cloudant/api/database.html#get-changes)\n- Update the totals in the Redis database for each change\n\n![customindex1](/img/customindex1.jpg)\n\nFirst we need to define some environment variables containing the URLs of our cloud-based Cloudant and Redis services; otherwise, local services are assumed (e.g., local CouchDB on port 5984 and local Redis on port 6739). For example:\n\n```sh\nexport CLOUDANT_URL=\"https://USER:PASS@HOST.cloudant.com\"\nexport DBNAME=\"pageviews\"\nexport REDIS_URL=\"redis://x:PASS@HOST:PORT\"\n```\n\nThen we can run a Node.js process that listens to the Cloudant changes feed and writes updates to Redis for each incoming change. Here's the code:\n\n```js\n// connect to Cloudant\nvar cloudant = require('cloudant');\nvar url = process.env.CLOUDANT_URL || 'http://localhost:5984';\nvar dbname = process.env.DBNAME || 'pageviews';\nvar db = cloudant({url:url , plugin: 'promises'}).db.use(dbname);\n\n// connect to Redis\nvar redis = require(\"redis\");\nvar redisurl = process.env.REDIS_URL || 'redis://localhost:6379';\nvar client = redis.createClient(redisurl);\nvar redis_leaderboard = 'leaderboard';\nvar redis_ipcount = 'ipcount';\n\n// listen to changes \nvar feed = db.follow({since: 'now', include_docs:true});\nfeed.on('change', function(change) {\n  if (change.doc && change.doc.path && change.doc.ip) {\n    \n    // do multiple Redis commands in 1\n    client.multi()\n      // increment the leaderboard sorted set\n      .zincrby(redis_leaderboard, 1, change.doc.path)\n      // add the IP to the HyperLogLog data set\n      .pfadd(redis_ipcount, change.doc.ip)\n      // execute both actions together\n      .exec();\n  }\n});\nfeed.follow();\n```\n\nAs documents are added to the Cloudant database, the Redis `leaderboard` and `ipcount` are updated too!\n\nWe can then log into the Redis command-line interface:\n\n```sh\nredis-cli -h HOST -p PORT -a PASSWORD\n# or redis-cli for a local service\n```\n\nAnd query the data like so:\n\n```\n127.0.0.1:6379> PFCOUNT ipcount\n(integer) 375608\n127.0.0.1:6379> ZREVRANGEBYSCORE 'leaderboard' +inf -inf WITHSCORES LIMIT 0 5\n 1) \"/blog/post-160.html\"\n 2) \"606\"\n 3) \"/blog/post-152.html\"\n 4) \"597\"\n 5) \"/blog/post-30.html\"\n 6) \"593\"\n 7) \"/blog/post-35.html\"\n 8) \"589\"\n 9) \"/blog/post-191.html\"\n10) \"589\"\n```\n\n## Monthly reporting\n\nAn enhancement to this approach is to have a monthly leaderboard and monthly unique IP address counts. We can easily enable this feature by parsing the date string in the Node.js code and writing to Redis keys with the month included (e.g., `\"leaderboard_2017-07\"`). On a month boundary, new data will be automatically fed into the next month's key:\n\n```js\n// connect to Cloudant\nvar cloudant = require('cloudant');\nvar url = process.env.CLOUDANT_URL || 'http://localhost:5984';\nvar dbname = process.env.DBNAME || 'pageviews';\nvar db = cloudant({url:url , plugin: 'promises'}).db.use(dbname);\n\n// connect to Redis\nvar redis = require(\"redis\");\nvar redisurl = process.env.REDIS_URL || 'redis://localhost:6379';\nvar client = redis.createClient(redisurl);\n\n// pad\nfunction pad(number) {\n  return (\"0\"+number).slice(-2); \n}\n\n// listen to changes \nvar feed = db.follow({since: 'now', include_docs:true});\nfeed.on('change', function(change) {\n  if (change.doc && change.doc.date && change.doc.path && change.doc.ip) {\n    \n    var d = new Date(change.doc.date);\n    var datestr = d.getUTCFullYear() + '-' + (pad(d.getUTCMonth()+1));\n    var leaderboard = 'leaderboard_' + datestr;\n    var ipcount =  'ipcount_' + datestr;\n\n    // do multiple Redis commands in 1\n    client.multi()\n      // increment the leaderboard sorted set\n      .zincrby(leaderboard, 1, change.doc.path)\n      // add the IP to the HyperLogLog data set\n      .pfadd(ipcount, change.doc.ip)\n      // execute both actions together\n      .exec();\n    console.log('change', change.doc.path, change.doc.ip);\n  }\n});\nfeed.follow();\n```\n\nNow look at the Redis keys being generated:\n\n```\n127.0.0.1:6379> SCAN 0\n1) \"0\"\n2) 1) \"ipcount_2017-06\"\n   2) \"leaderboard_2017-06\"\n   3) \"ipcount_2017-07\"\n   4) \"leaderboard_2017-07\"\n```\n\nWe have an `ipcount` HyperLogLog and a `leaderboard` sorted set for each month.\n\nAnother feature of the HyperLogLog data type is that we can combine multiple monthly instances to get an approximate unique count across all the counters. For example:\n\n```\n127.0.0.1:6379> PFCOUNT ipcount_2017-01 ipcount_2017-02 ipcount_2017-03 ipcount_2017-04 ipcount_2017-05 ipcount_2017-06 ipcount_2017-07 ipcount_2017-08 ipcount_2017-09 ipcount_2017-10 ipcount_2017-11 ipcount_2017-12\n```\n\nThis query gives us an annual unique IP count from the monthly data structures.\n\n## Going serverless\n\nSo far we've created Node.js processes that listen to the Cloudant changes feed. There's another way: if we create an OpenWhisk action that processes a single change, then we can trigger it from a Cloudant changes feed automatically. This passes the responsibility of the changes-feed-handling to OpenWhisk. We only need to to worry about the data processing.\n\n![customindex1](/img/customindex2.jpg)\n\nHere is the [OpenWhisk action](https://github.com/ibm-watson-data-lab/custom-indexes) and the [deployment script](https://github.com/ibm-watson-data-lab/custom-indexes/blob/master/deploy.sh).\n\nAs well as handling the changes feed logic, we can also deploy additional OpenWhisk actions that interrogate the Redis database and expose the aggregations as HTTP APIs.\n\nYou can access the APIs at the following URLs:\n\n- https://openwhisk.ng.bluemix.net/api/v1/web/NAMESPACE/leaderboard/getipcount.json\n- https://openwhisk.ng.bluemix.net/api/v1/web/NAMESPACE/leaderboard/getleaderboard.json\n\nHere, `NAMESPACE` is a combination of your Bluemix username and space. You can find yours by doing `wsk namespace list`.\n\n## Serverless vs. App\n\nWhich approach is better? The serverless approach leaves us with less infrastructure to worry about, but there are advantages for the app-based deployment in this case.\n\n- OpenWhisk doesn't allow subscription to the changes feed with `\"include_docs=true\"` set, so the OpenWhisk action has to call Cloudant to fetch the document body.\n- OpenWhisk's stateless nature means that each invocation of the action requires a connection and disconnection to both Cloudant and Redis&mdash;the app will reuse the connections again and again.\n- With further refinements to the app, we could reduce the writes to Redis by buffering some of them in the app and only writing to Redis periodically (say every 10 seconds). This approach would be impossible to achieve in OpenWhisk.\n\nOpenWhisk offers a pay-as-you go approach. Each invocation would cost you the [OpenWhisk cost](https://console.bluemix.net/openwhisk/learn/pricing?env_id=ibm:yp:us-south) plus [one read to the Cloudant database](https://www.ibm.com/analytics/us/en/technology/cloud-data-services/cloudant/pricing/) and a [fixed monthly cost for Redis](https://www.compose.com/pricing/#redis). An always-on app approach would incur the cost of a [24x7 Bluemix instance](https://www.ibm.com/cloud-computing/bluemix/pricing) and a [fixed monthly cost for Redis](https://www.compose.com/pricing/#redis).\n\nThe best solution depends on the volume of data being processed and which Cloudant & Bluemix plan you are on.\n\n## Custom indexes vs. Built-in indexes\n\nCloudant already has a number of built-in indexing engines:\n\n- The primary index on the `_id` field\n- Secondary [Map/Reduce indexes](https://docs.cloudant.com/creating_views.html)\n- Secondary [Search indexes](https://docs.cloudant.com/search.html)\n- Secondary [Geospatial indexes](https://docs.cloudant.com/geo.html)\n\nThe secondary indexes are defined via the creation of [Design Documents](https://docs.cloudant.com/design_documents.html), or by creating a [Cloudant Query index](https://console.bluemix.net/docs/services/Cloudant/api/cloudant_query.html). Cloudant takes responsibility of the initial build of the index and will keep each index up-to-date automatically as data is added, updated, or modified.\n\nThe custom indexes fed from Cloudant's changes feed are not managed by Cloudant. It is your responsibility to create the index from a standing start and to keep it up-to-date as the data changes over time. If you were starting with pre-existing data, you would have to spool through all the changes feed from zero to build up a first-cut of the index. If your custom indexer went offline for a time, you'd have to be careful that you hadn't missed some changes! \n\n## Conclusion\n\nWe've seen how we can write code to process a Cloudant database's changes feed, writing running totals and counts to an in-memory store. It's a use-case for problems that don't suit Cloudant's built-in indexing engines. We have also seen how the code can be deployed to the OpenWhisk serverless platform.\n\nFor more, see the [code on Github](https://github.com/ibm-watson-data-lab/custom-indexes). Thanks for reading!",
    "url": "/2017/09/26/Custom-indexers-for-Cloudant.html",
    "tags": "Serverless Indexing",
    "id": "22"
  },
  {
    "title": "Cloudant Envoy",
    "description": "Serverless Edition",
    "content": "\n\n\n[Cloudant Envoy](https://www.npmjs.com/package/cloudant-envoy) is software that sits between mobile users and a Cloudant database. It allows the apps to follow the CouchDB \"one-database-per-user\" model on mobile devices, while storing the data in a single server-side database. Without Envoy, each mobile user would have their own server-side database, which makes querying data across users very tricky.\n\n![envoy2](/img/envoy-serverless1.png)\n\nEnvoy's trick is to act as a proxy, seamlessly modifying the HTTP API calls heading for the Cloudant service:\n\n![envoy3](/img/envoy-serverless2.png)\n\nBy handling the subset of the Cloudant API that is required for replication, Envoy becomes indistinguishable from the Cloudant database it is paired with. It invisibly adds data to documents as they are saved and strips it out when data is retrieved. This hidden metadata marks up the ownership of each document so that each mobile user only has access to their own data.\n\nEnvoy can be baked into your own app to allow your web application to sync from an in-browser data store, or it can be run as a stand-alone proxy. In either flavour, it can also handle user authentication as described in [this tutorial which shows how to install Facebook authentication](https://medium.com/ibm-watson-data-lab/authentication-for-cloudant-envoy-apps-part-one-e6dba285b0d1). \n\nRunning Envoy all the time is fine, but it means you need to pay for a virtual machine (or two, or three) to be running 24x7 whether users are syncing their data or not. It would be ideal if we could run Envoy in a \"serverless\" configuration, i.e., to only consume computing resources if there were traffic. It turns out that you can!\n\n## Introducing Envoy serverless\n\nEnvoy Serverless is a new way of deploying Cloudant Envoy. Instead of standing up fixed, 24/7 machines, the code is deployed to [Apache OpenWhisk&trade;](https://developer.ibm.com/openwhisk/), which is branded as [Cloud Functions on IBM's Bluemix platform](https://console.bluemix.net/openwhisk/). Incoming API calls are handled by OpenWhisk as they arrive, scaling up to meet with the demand and costing nothing when the traffic falls off to zero. For low-traffic applications, the hosting costs could be zero because of IBM Cloud Function's generous [free tier](https://console.ng.bluemix.net/openwhisk/learn/pricing).\n\nOpenWhisk is an example of a \"serverless\" platform. Instead of worrying about servers, operating systems, queues, containers, networking, and infrastructure uptime, you simply deploy the *functions* you wish to run. By pairing these functions as HTTP API calls, you can stand up a RESTful API service without managing any infrastructure yourself.\n\n![schematic](/img/envoy-serverless-schematic.png)\n\nEach API call is a separate OpenWhisk *action*, in its own JavaScript file, and is usually tasked with handling a single API call. Sometimes the same action handles variants of the same API call, such as the GET and POST versions of the same call, or handles a range of paths (e.g., GET `/db/*`).\n\nEach action is independent of the others, is stateless (apart from the data that is stored in the Cloudant database), and can be deployed and updated separately.\n\nThe API Gateway handles authentication and routes the HTTP calls through to the corresponding action.\n\n## Deploying Envoy Serverless\n\nYou'll need a [Bluemix](https://bluemix.net) account, and then follow the [Getting started with Cloud Functions](https://console.ng.bluemix.net/docs/openwhisk/index.html#getting-started-with-openwhisk) instructions to download and install the `wsk` tool.\n\nCreate a Cloudant instance in Bluemix, and make note of its URL. In your Cloudant dashboard, create a new database called \"envoydb\".\n\nClone the `envoy-serverless` project:\n\n```sh\ngit clone https://github.com/glynnbird/envoy-serverless\ncd envoy-serverless\n```\n\nCreate an environment variable containing your Cloudant URL:\n\n```sh\nexport COUCH_HOST=\"https://USERNAME:PASSWORD@HOST.cloudant.com\"\n```\n\nAnd deploy:\n\n```sh\n./deploy.sh\n```\n\nThe script will output the URL of your service. It will look something like this:\n\n```\nhttps://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/YOURSERVICEID/api/envoydb\n```\n \nIn the OpenWhisk dashboard under the [API Management tab](https://console.ng.bluemix.net/openwhisk/apimanagement) you should see your deployed API calls:\n\n![envoy auth](/img/envoy-deployed.png)\n\nBelow this you will need to secure your API by ticking the \"Require consuming applications to authenticate via API Key\" box and clicking \"Save\":\n\n![envoy auth](/img/envoy-auth.png')\n\nYou should now only be able to access your API with a valid API Key (which can be created in the \"Sharing\" tab):\n\n```sh\n> curl 'https://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/YOURSERVICEID/api/envoydb/_all_docs'\n{\"status\":401,\"message\":\"Error: Unauthorized\"}\n\n> curl -H 'x-ibm-client-id: VALID_API_KEY' 'https://service.us.apiconnect.ibmcloud.com/gws/apigatewa/api/YOURSERVICEID/api/envoydb/_all_docs'\n{\n  \"total_rows\": 0,\n  \"offset\": 0,\n  \"rows\": []\n}\n```\n\nSee the [source code](https://github.com/glynnbird/envoy-serverless) for further examples.\n\n## Pros and cons of serverless deployment\n\nIs an OpenWhisk deployment *better* than a virtual machine for an Envoy deployment? The answer is nuanced.\n\nA 24x7 virtual machine may have slightly better performance because it can re-use HTTP connections, saving the effort of performing DNS lookups and establishing a new socket. It can also use streaming to provide an efficient flow of moving data through the proxy, whereas OpenWhisk cannot.\n\nThe OpenWhisk version has fewer lines of code because it delegates authentication and the HTTP server infrastructure to the OpenWhisk stack. It is also easier to scale as OpenWhisk handles the deployment of computing power needed to meet the demand of the incoming API calls. \n\nThat's it! If you're using Cloudant in an application architecture that follows the CouchDB one-database-per-user model, let me know in the response below. Deploying Cloudant Envoy in either the traditional or serverless fashion could be a good option.\n\n",
    "url": "/2017/10/05/Cloudant-Envoy-serverless-edition.html",
    "tags": "Serverless",
    "id": "23"
  },
  {
    "title": "Replication topologies",
    "description": "Designing for Cloudant failover",
    "content": "\n\n\n[Cloudant's (and CouchDB's) replication](https://console.bluemix.net/docs/services/Cloudant/guides/replication_guide.html#replication) feature allows you to keep databases in sync across countries and continents. However, sometimes it's not obvious how to use this basic pair-wise feature in order to create more complicated replication topologies, like three or more geographical replicas, and then how to do disaster recovery between them. Let's discuss these in turn.\n\nThroughout the following, it's important to remember that replication is an asynchronous, best-effort process in which a change is propagated to peers sometime after the client receives the response to its write request. This means that longer replication chains don't directly affect document write latency, but also that discrepancies between peers will exist for some small period of time (typically low single digit seconds maximum) after a write to one peer completes.\n\n## More complicated topologies\n\nFirstly it's important to understand that Cloudant's replication creates synchronised copies of databases between peers once a two-way replication is set up. Each replication flows changes only in one direction, which means that a two-way replication involves setting up two separate replications, one in each direction. Visualising this as a directed graph works well. Each node in the graph is a Cloudant database and the arrows are directed edges showing which way changes are flowing.\n\nUsing this style, a basic two-peer setup between databases A and B looks like this:\n\n![topology 1](/img/top1.png)\n\nThere are two arrows because to have synchronised peers a replication needs to be set up in each direction. After a very short time -- milliseconds to seconds -- peer A will know about any changes to B, and, similarly, B will know about any changes to A.\n\nThese peers can then further replicate with other peers to support more complicated scenarios than a single pair of peers. The key point is that, by setting up replications, one is setting up a graph that changes traverse to get from one peer to another. In order for a change to get from peer A to peer Z, at least one directed link must exist between A and Z. This is the foundation piece to creating more complicated topologies because either A or B can now replicate the changes elsewhere.\n\nSo A can propagate the changes from B to another database, C. In this scenario, A is the primary database and the others could be considered replicas.\n\n![topology 2](/img/top2.png)\n\nThis obviously introduces a single point of failure. The replication process is smart enough that you can add a replication from B to C to this set, which means that A is no longer a single point of failure. In graph terms, it's safe to set up a cyclic graph.\n\n![topology 3](/img/top3.png)\n\nAs more database replicas are added to the set, however, having a fully connected mesh starts to add undue load to each database, and it's not necessary as each peer is able to act as a \"stepping stone\" to push changes through the network.\n\nHere, a change at peer B is replicated to E via A then C:\n\n![topology 4](/img/top4.png)\n\nIn the diagram I only have one stepping stone in each path for simplicity of diagramming, but one could add redundant steps to ensure at least two paths through the network for any given change.\n\n## Using One-Way Replications for Synchronisation between Peers\n\nFinally, it's worth revisiting the point that all we require for synchronisation is that there is a directed path for a change to follow from one peer to another. This means that two-way replications between peers are not strictly required. Instead, one alternative is to set up a circle topology:\n\n![topology 5](/img/top5.png)\n\nHere, there is still a path for changes to follow between any pair of nodes. Again, setting up redundant links to provide two paths may be useful. Using one-way replications in this way further allows you to decrease the load upon each peer database while still maintaining acceptable replication latency.\n\n## Failing over between databases\n\nAfter setting up the synchronised peers in whatever topology works for your needs, you're ready to set up failover between the database replicas.\n\nThe important takeaway point in this section is that, while you might be tempted to manage failover elsewhere, the only way to reliably failover is from within the application itself. The reason for this is simple: the application is the only place you can be sure whether a given replica is contactable from the application.\n\nAn application may be unable to contact a database peer for several reasons, such as:\n\n- The database servers are actually offline.\n- The application cannot route to the preferred database servers because of network disruption.\n- There is very high latency suddenly introduced on the network path between the application and database clusters.\n- The application's DNS cache is out of date so it's resolving the database location to the wrong IP address.\n- Key database requests made by the application have a latency above a given threshold.\n\nThe last condition is an example of how the application can use its own measurements to ensure failover happens before users become aware of the problem and how a failover strategy can benefit from being application performance-indicator aware.\n\nThe only thing you care about is whether the application can reach the database; not whether, for example, the third-party health-checking service you might use can contact it, or your DNS provider.\n\nThe basic steps are:\n\n- Configure each application instance with a prioritised list of database peers.\n- Use an approach like the circuit breaker pattern to guide failover.\n- Progress down the prioritised list.\n- Have a background task checking for recovery of higher priority peers, failing back when they are reliably available to the application again.\n\nThis approach is simple to understand, not too complicated to implement and gives your application the best chance of surviving the most number of possible failure modes.",
    "url": "/2017/11/07/Cloudant-replication-topologies.html",
    "tags": "Architecture Replication",
    "id": "24"
  },
  {
    "title": "Tracking cryptocurrencies",
    "description": "Using Cloud Functions and Cloudant",
    "content": "\n\n\n# Tracking your cryptocurrency portfolio with serverless functions & Cloudant\n\n> Monitor the value of your bitcoin hoard with serverless Cloud Functions writing data to Cloudant\n\nThe value of crypto-currencies can vary greatly over time. If you're lucky, some coins bought for a handful of dollars a few years ago can be worth thousands of dollars now.\n\n![bitcoin price](/img/bitcoin_price.png)\n \nThere are many websites and apps that can help you monitor your portfolio, but what's the fun in using a ready-made solution when you can build your own?!\n\n## Crypto pricing API\n\nWe can use an API from [CryptoCompare](https://www.cryptocompare.com/api/) to get the latest price of Bitcoin and Ether, two popular coins, in US dollars. We can call the API passing the symbols `BTC`, `ETH` and `USD` as parameters. For example:\n\n```sh\ncurl 'https://min-api.cryptocompare.com/data/pricemulti?fsyms=BTC,ETH&tsyms=USD'\n{\n  \"BTC\": {\n    \"USD\": 6166.19\n  },\n  \"ETH\": {\n    \"USD\": 308.09\n  }\n}\n```\n\nFor a single currency, we can use an even simpler call:\n\n```sh\ncurl 'https://min-api.cryptocompare.com/data/price?fsym=BTC&tsyms=USD'\n{\n  \"USD\": 6165.54\n}\n```\n\n## Retrieving the data every minute\n\nWe can write a script to fetch this data every minute, writing a document to a Cloudant database each time.\n\nThe Node.js script to do this is very simple:\n\n```js\nconst cloudant = require('cloudant');\nconst request = require('request');\n\nconst readBitcoinPrice = function () {\n  return new Promise(function(resolve, reject) {\n    var req = {\n      url: 'https://min-api.cryptocompare.com/data/pricemulti?fsyms=BTC&tsyms=USD',\n      json: true \n    };\n    request(req, function (err,r,data) {\n      if (err) {\n        return reject(err);\n      }\n      resolve(data);\n    });\n  });\n};\n\nconst main = function(args) {\n  return readBitcoinPrice().then(function(data) {\n    console.log(data);\n    var obj = {\n      _id: new Date().getTime().toString(),\n      btcinusd: data.BTC.USD\n    };\n    if (args && args.url) {\n      const c = cloudant({url: args.url, plugin: 'promises'});\n      const db = c.db.use('bitcoin');\n      return db.insert(obj);\n    } else {\n      return { ok: true }\n    } \n\n  })\n};\n\nexports.main = main;\n```\n\nThe `main` function expects the Cloudant URL to be supplied as the `url` key in its `args` parameter. It calls the API and creates a JSON document which is written to Cloudant:\n\n```js\n{ \n  \"_id\": \"1509444081742\",\n  \"btcinusd\": 6165.54\n}\n```\n\nThe `_id` is the unique key for each Cloudant document. Here, it is actually a timestamp (the number of milliseconds since 1970) which will be unique and in time order for our \"once per minute\" polling of the API.\n\nAll we need now is to call the \"main\" function every minute.\n\n## Going serverless\n\nWe have a number of options here:\n\n- We could write a Node.js script that is on 24/7 and use a JavaScript `setInterval` timer to call the `main` function every 60000 milliseconds.\n- We leave the script doing a single execution as it does now and call it every minute using [cron](http://www.adminschoice.com/crontab-quick-reference) &mdash; a task scheduler found on most Unix-like systems.\n- We could push our script to IBM Cloud Functions, IBM's serverless platform, and instruct *it* to run it for us every minute.\n\nWe put the URL of our Cloudant service in the environment variable  `CLOUDANT_URL` and run a series of commands to deploy our Node.js file to IBM Cloud Functions:\n\n```sh\n# deploy to OpenWhisk\nwsk package update blockchain --param url \"$CLOUDANT_URL\"\nwsk action update blockchain/bitcoin bitcoin.js\n\n# run every minute\nwsk trigger create every-minute --feed /whisk.system/alarms/alarm --param cron \"* * * * *\"\nwsk rule update every-minute-rule every-minute blockchain/bitcoin\nwsk rule enable every-minute-rule\n```\n\nThe first two commands deploy the code, the last three trigger the code to be run every minute using the [alarm package](https://console.bluemix.net/docs/openwhisk/openwhisk_alarms.html#openwhisk_catalog_alarm_fire). \n\nOnce deployed, we should see the documents appearing in our Cloudant `bitcoin` database:\n\n```js\n{\n  \"_id\": \"1509448502820\",\n  \"_rev\": \"1-e96f50965801b81f2cf19fc2d2ad637b\",\n  \"btcinusd\": 6200.42\n}\n```\n\n## Querying the data\n\nOnce we've left our data collection script running for a while, we can query the data using Cloudant's API. Our data is already sorted in time order because we used a timestamp as the key `_id` field.\n\nWe can therefore get the first 2 hours of data:\n\n```sh\ncurl \"$CLOUDANT_URL/bitcoin/_all_docs?limit=120\"\n```\n\nThe most recent hour of data:\n\n```sh\ncurl \"$CLOUDANT_URL/bitcoin/_all_docs?limit=60&descending=true\"\n```\n\nThe most recent data point:\n\n```sh\ncurl \"$CLOUDANT_URL/bitcoin/_all_docs?limit=1&descending=true\"\n```\n\n## Aggregating the data\n\nCloudant's [MapReduce](https://console.bluemix.net/docs/services/Cloudant/api/creating_views.html#views-mapreduce-) system allows custom indexes to be created by supplying a JavaScript function which is executed over each document in a database.\n\nWe can use a function like this:\n\n```js\nfunction (doc) {\n  var ts = parseInt(doc._id);\n  var date = new Date(ts);\n  var y = date.getUTCFullYear();\n  var m = date.getUTCMonth() + 1;\n  var d = date.getUTCDate(); \n  var h = date.getUTCHours();\n  var min = date.getUTCMinutes();\n  emit([y, m, d, h, min], doc.btcinusd);\n}\n```\n\nHere's what it does:\n\n- turn the `_id` field into an integer\n- parse it into a JavaScript Date object\n- extract the year, month, day, hour, and minute\n- `emit` a key/value pair where the key is the array `[year, month, day, hour, minute]` and the value is the bitcoin price\n\n![bitcoin map reduce](/img/bitcoin-mr.png)\n\nCombined with the built-in `_stats` reducer, we can use this index to create hierarchical aggregations of the bitcoin price:\n\n```sh\ncurl \"$CLOUDANT_URL/bitcoin/_design/query/_view/bydate?group_level=2\"\n{\n  \"rows\": [{\n      \"key\": [2017, 8],\n      \"value\": {\n        \"sum\": 88808847.69999999,\n        \"count\": 20770,\n        \"min\": 3613.28,\n        \"max\": 4764.44,\n        \"sumsqr\": 380636509418.7472\n      }\n    },\n    {\n      \"key\": [2017, 9],\n      \"value\": {\n        \"sum\": 175506096.85000002,\n        \"count\": 42849,\n        \"min\": 2982.94,\n        \"max\": 4974.69,\n        \"sumsqr\": 724630560624.3647\n      }\n    },\n    {\n      \"key\": [2017, 10],\n      \"value\": {\n        \"sum\": 233471117.30000004,\n        \"count\": 44238,\n        \"min\": 4143.9,\n        \"max\": 6294.59,\n        \"sumsqr\": 1251129781142.4658\n      }\n    }\n  ]\n}\n```\n\nThe above query asks for all of the data grouped by year and month. Notice how the `_stats` reducer gives us a sum/count/sumsqr (which we can use to calculate mean, variance, and standard deviation) and the maximum and minmum value per month. The `group_level` parameter indicates how many levels into the key we wish to aggregate. For example:\n\n- `group_level=1` - year\n- `group_level=2` - year/month\n- `group_level=3` - year/month/day\n- `group_level=4` - year/month/day/hour\n\nWe can also specify a `startkey` and `endkey` to limit range of data we want to report on:\n\n```sh\ncurl \"$CLOUDANT_URL/bitcoin/_design/query/_view/bydate?group_level=3&startkey=[2017,10,1]&endkey=[2017,11,1]\"\n```\n\nThe above command asks for year/month/day data for the month of October 2017.\n\n## What's next?\n\nWe could do more than just record the data. We could calculate [technical indicators](https://www.npmjs.com/package/technicalindicators) and use them to predict whether the market is heading upwards or downwards. We could send emails or alerts when certain market conidtions prevail. We could write an algorithm that decides when it's a good time to buy or sell and call a third-party API to actually make crypto-currency trades.\n\nI'm not that brave, so I wrote a trading simulator that allows you to simulate crypto-currency trading using the last 2 hours of bitcoin data. You start with $5000 of cash and $5000 of bitcoin. Trade as many times as you like and see if you end up with more than $10000 at the end of the game.\n\n![trader](/img/bitcoin-trader.png)\n\nTry it for yourself: https://glynnbird.github.io/markettrader/public/index.html\n\n\n\n \n",
    "url": "/2017/11/16/track-cryptocurrency-with-serverless.html",
    "tags": "Serverless",
    "id": "25"
  },
  {
    "title": "Elasticsearch hybrid",
    "description": "Moving Cloudant data to ES",
    "content": "\n\n\nOn the face of it, IBM Cloudant and Elasticsearch seem to be very similar products. They both:\n\n- Store JSON documents without you needing to define a schema up-front.\n- Have an HTTP API.\n- \"Shard\" each collection into pieces and store multiple copies across a distributed cluster.\n- Can be used to perform \"free text\" search, i.e., query which documents best match a supplied string.\n\nCloudant is a resilient database that guarantees that data is stored on disk on multiple nodes when you write your data. Elasticsearch is not a database as such &mdash; it prioritises speed over resiliance and is designed to be used as a secondary index rather than as your primary source of truth. \n\nIn some cases, it may be desirable to take a hybrid approach:\n\n- Store your primary data in Cloudant, directing all Create/Read/Update/Delete (CRUD!) requests to this database.\n- Store a copy of the data in Elasticsearch and use it to handle all free-text search queries.\n\nSuch a setup needs a means of moving data from Cloudant to Elasticsearch as it changes. In this article, we'll see how to automatically keep the data in sync using a \"serverless\" function.\n\n![schematic](/img/cloudant-es-bridge.png)\n\n## Cloudant-to-Elasticsearch bridge\n\nThe Cloudant database has a [changes feed](https://console.bluemix.net/docs/services/Cloudant/api/database.html#get-changes), an HTTP stream that spools out each add/update/delete that happens to the dataset. By consuming that feed, we can trigger other actions as each change arrives, in this case, writing to an Elasticsearch index. The [IBM Cloud Functions](https://console.bluemix.net/openwhisk/) platform can be configured to listen to the Cloudant changes feed on your behalf, and execute custom code on each change. As IBM Cloud Functions is a \"serverless\" platform, you don't pay for any fixed computing power to run the actions. You simply pay for the number of invocations of your code, which is proportional to the rate of change of your data.\n\nI've written a script to set up the serverless infrastructure. Clone the code:\n\n```sh\ngit clone https://github.com/ibm-watson-data-lab/cloudant-es-bridge\ncd cloudant-es-bridge\n```\n\nAll it needs is our Cloudant and Elasticsearch authentication credentials entered as commandline environment variables. (Read the project's [README](https://github.com/ibm-watson-data-lab/cloudant-es-bridge) to see how to sign up for Cloudant, Elasticsearch, and IBM Cloud Functions.)\n\n```sh\nexport CLOUDANT_HOST=\"HOST.cloudant.com\"\nexport CLOUDANT_USERNAME=\"CLOUDANTUSERNAME\"\nexport CLOUDANT_PASSWORD=\"CLOUDANTPASSWORD\"\nexport CLOUDANT_DB=\"esbridge\"\nexport ELASTIC_URL=\"https://ESUSERNAME:ESPASSWORD@HOST.composedb.com:PORT/INDEX/TYPE\"\n```\n\nThen run the deployment script (which assumes you have the [bx wsk](https://console.bluemix.net/openwhisk/learn/cli) tool installed and configured on your machine):\n\n```sh\n./deploy.sh\n```\n\nThis script performs the following tasks:\n\n- Creates a serverless package with all the configuration you supplied.\n- Creates a serverless \"action\" inside that package.\n- Creates a Cloudant connection with your configuration.\n- Creates a trigger that fires on your Cloudant database's changes feed.\n- Creates a rule that fires your action when the trigger fires.\n\nThe upshot is that every time you create, modify or delete a document in Cloudant, the equivalent action happens in your Elasticsearch cluster!\n\nTry creating documents in your Cloudant dashboard, and then check that they appear in the Compose dashboard's \"Browser\" feature.\n\nThe data can then be queried with the Elasticsearch API:\n\n```sh\ncurl \"$ELASTIC_URL/_search?q=aimee\"\n{\n  \"took\": 110,\n  \"timed_out\": false,\n  \"hits\": {\n    \"total\": 1,\n    \"max_score\": 0.2824934,\n    \"hits\": [\n      {\n        \"_index\": \"myesindex\",\n        \"_type\": \"default\",\n        \"_id\": \"5be1886e8ff5f340947b907ce2ac9e9d\",\n        \"_score\": 0.2824934,\n        \"_source\": {\n          \"firstname\": \"Aimee\",\n          \"lastname\": \"Mann\",\n          \"description\": \"singer songwriter\",\n          \"timestamp\": 241\n        }\n      }\n    ]\n  }\n}\n```\n\n## Do I need separate Cloudant & Elasticsearch?\n\nWhether you use Cloudant on its own, or pick Cloudant for storage and Elasticsearch for search, depends on your application and how it's used. \n\nIf your application relies heavily on search, then you may want to direct the search requests to an Elasticsearch cluster that contains a copy of your searchable data. This would allow you to scale your search capability separately to your storage engine.\n\nIf search is an occasional or secondary activity in your app, or if you prefer the convenience of fewer moving parts, then you may wish to use Cloudant's built-in [free-text search](https://console.bluemix.net/docs/services/Cloudant/api/search.html#search). While not as fully-featured as Elasticsearch, it can easily handle free-text searches, range queries, and faceting as it is built on the same Lucene library that powers Elasticsearch.\n\n## Open-Source ftw\n\nIf you want to build on this utility, the code is [on GitHub](https://github.com/ibm-watson-data-lab/cloudant-es-bridge). It's free and open-sourced via the Apache-2.0 license. You may want to modify the `onchange.js` action to alter the document before it's written to Elasticsearch. You may also need to deploy multiple changes feed listeners for multiple data sources. It's up to you.\n\n    \n",
    "url": "/2017/11/30/Making-a-Cloudant-ElasticSearch-hybird.html",
    "tags": "ElasticSearch Serverless",
    "id": "26"
  },
  {
    "title": "Using your own domain",
    "description": "Using Cloud Functions as a proxy",
    "content": "\n\n\nUntil recently, Cloudant allowed you to bring your own domain name through its \"virtual hosts\" functionality. This feature is now being removed for security reasons.\n\nIf you still want to be able to access the Cloudant API through your own domain name, you can do it by creating a *serverless proxy* that is hosted on [IBM Cloud Functions](https://www.ibm.com/cloud/functions), IBM's serverless platform.\n\n![schematic](/img/serverless-proxy1.png)\n\nA number of serverless \"actions\" are deployed&mdash;one per API call you need to proxy. They are served out by the IBM Cloud Functions API Gateway, which provides authentication and rate-limiting of your service. Finally, you can configure the API gateway to use *your* domain name and to perform the SSL termination so that your API is served out securely.\n\n## Prequisites\n\nFirst we need the source code. You'll need the `git` tool and to be familiar with command-line tools:\n\n```sh\ngit clone https://github.com/ibm-watson-data-lab/serverless-proxy.git\ncd serverless-proxy\n```\n\nIf you haven't done so already, [sign up for an IBM Cloud account](https://bluemix.net) (formerly branded as \"Bluemix\"). Follow the [Getting Started with IBM Cloud Functions guide](https://console.ng.bluemix.net/openwhisk/getting-started) to download the `bx wsk` tool and configure it for your account. \n\nIf you don't already have a Cloudant account, in your IBM Cloud dashboard create a Cloudant service and make a note of its URL. In the Cloudant dashboard, create a new empty database (say, 'mydb'). This is the database we will be using in our proxy deployment.\n\n## Installation\n\nBefore we deploy our code, we need to define the following:\n\n- the URL of the Cloudant service we are using\n- the name of the database we are working with\n\nThese two pieces of information need to be stored in environment variables like so:\n\n```sh\nexport COUCH_HOST=\"https://USERNAME:PASSWORD@HOST.cloudant.com\"\nexport COUCH_DATABASE=\"mydb\"\n```\n\nNow we can run the deployment script:\n\n```sh\n./deploy.sh\n```\n    \nThis will configure a number of serverless actions&mdash;one for each Cloudant endpoint that is supported. It also configures the IBM Cloud Functions [API Gateway](https://console.bluemix.net/docs/apis/management/index.html#index) to map API calls to those actions. We end up with a set of Cloudant-like API calls hosted on IBM Cloud Functions.\n\nIn the IBM Cloud Functions dashboard we can configure that API to: \n\n- limit access to only those that have an API key\n- only allow those who have an OAuth cookie\n- rate-limit requests per user\n\nBy default, however, the API is open, meaning that anyone who has access to this API can read, write, update, or delete Cloudant data.\n\n## Secure your traffic\n\nTo use a custom domain, first you need a domain name. I created a subdomain: `mycloudant.glynnbird.com`. You need to configure the DNS for that domain name using the [instructions here](https://console.bluemix.net/docs/apis/management/manage_apis.html#custom_domains). \n\nThen we need a secure certificate for that domain name. Fortunately, thanks to the folks at [Let's Encrypt](https://letsencrypt.org/), secure certificates are now free. I found the simplest way to get one was to use the [https://www.sslforfree.com](https://www.sslforfree.com) website and download the certifcate to a `mycert.crt` and the private key to `mykey.key` file.\n\nConfigure your IBM Cloud account to accept your domain name and upload your certificate and private key using the [instructions here](https://console.bluemix.net/docs/apis/management/manage_apis.html#custom_domains). \n\nAt this point, your API should be served out on your custom domain name using HTTPS. \n",
    "url": "/2017/12/07/Using-your-own-domain-name.html",
    "tags": "Domain Serverless",
    "id": "27"
  },
  {
    "title": "Detaching attachments",
    "description": "Using Cloud Functions and Object Storage",
    "content": "\n\n\nImagine we have an app that collects geo-coded photographs. We give the app to thousands of students and ask them to collect pictures of storefronts, together with the company name and data from the phone's GPS. Using the latitude and longitude, the company name, and the photograph we intend to crowd-source a business directory.\n\nOur app is based on [Offline First](http://offlinefirst.org/) design principles. It stores its data locally on the mobile device, syncing to the cloud when there is a decent connection. This means our intrepid data collectors need not worry about mobile data charges or visiting areas without cellular coverage&mdash;they can still store their data on their smartphones and upload it later.\n\nWe can use [PouchDB](https://pouchdb.com/) for web apps or the [Cloudant Sync](https://www.ibm.com/analytics/us/en/technology/offline-first/) library for native mobile apps on the client side, and use the IBM Cloudant database service on the server side. This gives us the mobile-to-cloud replication of the business data, including storing the photographs as binary [attachments](https://console.bluemix.net/docs/services/Cloudant/api/attachments.html#attachments).\n\n## Why detach attachments?\n\nStoring binary attachments in a NoSQL document database is handy, but it's not best-practice in the long term. It's a useful means of storing binary data, especially on the client side when there's no network connectivity. In the long term, however, object storage is a natural choice to provide a limitless store of files and is much cheaper than a database per GB of binary data.\n\nWe can implement a best-of-both-worlds approach by storing attachments in the database _initially_, but detaching them later by moving them to object storage as data reaches the cloud.\n\n## How does it work?\n\nWe need to monitor our Cloudant database's changes feed, and as each document is updated we need to move any attachments from the document to object storage.\n\nInitially, our database documents might look like this:\n\n```js\n{\n  \"_id\": \"7\",\n  \"_rev\": \"2-920d8da7eb1a1175fcbc10cf6f989d99\",\n  \"title\": \"Whole Foods\",\n  \"latitude\": 54.5107,\n  \"longitude\": -0.1383,\n  \"_attachments\": {\n    \"storefront.jpg\": {\n      \"content_type\": \"image/jpeg\",\n      \"revpos\": 2,\n      \"digest\": \"md5-N0JXExRZxZaOD3sszjMXzA==\",\n      \"length\": 46998,\n      \"stub\": true\n    }\n  }\n}\n```\n\nHere, attachments are referenced in the `_attachments` object. In this case we have a single attachment called `storefront.jpg`. After moving the document to object storage our document looks like this:\n  \n```js\n{\n  \"_id\": \"7\",\n  \"_rev\": \"3-c3272191e6e94d3bd2a3d72145c7d4fd\",\n  \"title\": \"Whole Foods\",\n  \"latitude\": 54.5107,\n  \"longitude\": -0.1383,\n  \"attachments\": {\n    \"storefront.jpg\": {\n      \"content_type\": \"image/jpeg\",\n      \"revpos\": 2,\n      \"digest\": \"md5-N0JXExRZxZaOD3sszjMXzA==\",\n      \"length\": 46998,\n      \"stub\": true,\n      \"Location\": \"https://detacher.s3.eu-west-2.amazonaws.com/7-storefront.jpg\",\n      \"Key\": \"7-storefront.jpg\"\n    }\n  }\n}\n```\n\nThere are several points to note:\n\n- The `_rev` field has changed because we have written a new version of the document.\n- The `_attachments` object is gone. Cloudant is no longer storing the attachment.\n- There is a new `attachments` object with almost identical data, except that it contains a reference to a `Location` (the URL describing where the attachment is stored in Object Storage) and a `Key` (a concatenation of the document id and the original filename).\n\nThe code to do this runs on [IBM Cloud Functions](https://www.ibm.com/cloud-computing/bluemix/openwhisk), IBM's serverless platform which is based on Apache OpenWhisk. A tiny piece of Node.js code is deployed to IBM Cloud Functions and configured to run against every change that occurs in the Cloudant database. \n\n![schematic](/img/detacher-schematic.png)\n\nIn pseudocode, this is what it does:\n\n```\nLoad the document by its _id\nIF the document contains an _attachments key THEN\n   FOR each _attachment\n     write the attachment to object storage\n   END FOR\n   remove the documents _attachments key\n   replace it with a new attachements object\n   save a new version of the document back to Cloudant\nEND IF\n```\n\n## How do I deploy this myself?\n\nYou'll need a Cloudant account with a database in it, the [IBM Cloud Functions command-line tool](https://console.bluemix.net/docs/cli/reference/bluemix_cli/get_started.html#getting-started) installed, and an object storage bucket in [IBM Cloud Object Storage](https://www.ibm.com/cloud-computing/products/storage/object-storage/) or [Amazon S3](https://aws.amazon.com/s3/).\n\nSimply [clone my detacher source code](https://github.com/ibm-watson-data-lab/detacher), set your service credentials as environment variables, and run my deploy script:\n\n```sh\nexport CLOUDANT_HOST=\"myhost.cloudant.com\"\nexport CLOUDANT_USERNAME=\"myusername\"\nexport CLOUDANT_PASSWORD=\"mypassword\"\nexport CLOUDANT_DATABASE=\"mydatabase\"\nexport AWS_ACCESS_KEY_ID=\"ABC123\"\nexport AWS_SECRET_ACCESS_KEY=\"XYZ987\"\nexport AWS_BUCKET=\"mybucket\"\nexport AWS_REGION=\"eu-west-2\"\nexport AWS_ENDPOINT=\"https://ec2.eu-west-2.amazonaws.com\"\n./deploy.sh\n```\n\nNow every time you create a document with an attachment, the attached files are automatically moved to your object storage bucket in the blink of an eye.\n\n![demo](/img/attachments.gif)\n\nCheck out the [source code](https://github.com/ibm-watson-data-lab/detacher) for yourself, and start giving those document attachments the treatment they deserve!",
    "url": "/2017/12/21/Detaching-Attachments-with-Serverless.html",
    "tags": "Attachments Serverless",
    "id": "28"
  },
  {
    "title": "Partial Indexes",
    "description": "Filter data before it's indexed",
    "content": "\n\n\nIndexing is what makes database queries fast and scalable. Without and index, a database is forced to trawl through *every record* to calculate the answer to a query. A carefully designed index allows a query to be answered with a fraction of a work by jumping to the pertinent portion.\n\n![Organise your data.]({{< param \"image\" >}})\n> Image by [Jay Wennington](https://unsplash.com/photos/OLIcAFggdZE)\n\n## Indexing basics\n\nLet's dive in with the example of [road safety data](https://data.gov.uk/dataset/road-accidents-safety-data) from the UK government's open data portal. There is a record for each traffic accident that looks like this once de-normalised and imported into a Cloudant database:\n\n```js\n{\n  \"_id\": \"200901BS70015\",\n  \"_rev\": \"1-e2a972623f4402f3af7f43ecc376de0a\",\n  \"Longitude\": -0.206779,\n  \"Latitude\": 51.498778,\n  \"Police_Force\": \"Metropolitan Police\",\n  \"Accident_Severity\": \"Slight\",\n  \"Number_of_Vehicles\": 2,\n  \"Number_of_Casualties\": 1,\n  \"Date\": \"2009-01-12\",\n  \"Day_of_Week\": \"Monday\",\n  \"Time\": \"14:00\",\n  \"Road_Type\": \"Single carriageway\",\n  \"Speed_limit\": 30,\n  \"Road_Surface_Conditions\": \"Wet or damp\",\n  \"year\": 2009,\n  \"month\": 1,\n  \"day\": 12,\n  \"Road_Class\": \"A\",\n  \"Road_Number\": 3220\n}\n```\n\nThere are 1,176,672 records in the database, so flying without an index is going to be an expensive operation. In fact, if you attempt to make a query without first creating an index, Cloudant will oblige but issue a warning in the returned data:\n\n```js\n{\n  \"docs\": [ ],\n  \"warning\": \"no matching index found, create an index to optimize query time\"\n}\n```\n\nThe fields you need to index are related to the fields that you use in the *selector* object in your query. If we are going to be making queries between two dates, then it makes sense to create an index on the *Date* field:\n\n```sh\ncurl -X POST \\ \n     -d'{\"index\":{\"fields\":[\"Date\"]}}' \\\n     -H 'Content-type: application/json' \\\n     https://U:P@host.cloudant.com/accidents/_index\n```\n\nWe are making a call to the `_index` endpoint of our Cloudant database, passing in a JSON object that describes the index we wish to create:\n\n```js\n{\n  \"index\": {\n    \"fields\": [\n      \"Date\"\n    ]\n  }\n}\n```\n\nIn this case we only need to index the `Date` field. \n\nOn receiving this request, Cloudant trawls through each document in the database and creates an index where each record is organised into `Date` order. On a large data set this can take some time&mdash;check the progress in the \"Active Tasks\" tab of your Cloudant dashboard or call the [`_active_tasks` endpoint](https://console.bluemix.net/docs/services/Cloudant/api/active_tasks.html#retrieving-a-list-of-active-tasks).\n\n![indexing](/img/indexing.png)\n\nOnce the index is built, we can then query this index, passing in a date or a range of dates, and Cloudant will use the index to fulfill the request:\n\n```sh\n# fetch documents from the 1st January 2015\ncurl -X POST \\\n     -d'{\"selector\":{\"Date\":\"2015-01-01\"}}' \\\n     -H 'Content-type: application/json' \\\n     https://U:P@host.cloudant.com/accidents/_find\n```\n\n```sh\n# fetch documents from the year 2015 only \ncurl -X POST \\\n     -d'{\"selector\":{\"Date\":{ \"$gte\":\"2015-01-01\", \"$lt\":\"2016-01-01\"}}}' \\\n     -H 'Content-type: application/json' \\\n     https://U:P@host.cloudant.com/accidents/_find\n```\n\n## Building an index with a partial filter\n\nThe first index we created stored an entry for each of the 1m+ records. This may be exactly what we intended, but in some cases we may only ever be interested in a subset of the data. \n\nLet's say we are only interested querying accidents that occur during a weekend&mdash;there is little point indexing those that occur during the week. This is where Cloudant's index definitions with a `partial_filter_selector` are useful.\n\n```sh\n# create index with partial filter\ncurl -X POST \\ \n     -d'{\"index\":{\"fields\":[\"Date\"],\"partial_filter_selector\":{\"$or\":[ {\"Day_of_Week\": \"Saturday\"}, {\"Day_of_Week\":\"Sunday\"}]}},\"ddoc\":\"date-index2\"}' \\\n     -H 'Content-type: application/json' \\\n     https://U:P@host.cloudant.com/accidents/_index\n```\n\nThe `partial_filter_selector` parameter instructs the indexer to filter the data *prior to indexing*, meaning that the index no longer holds data for every record in the database, only the ones that match the supplied selector.\n\n![indexing](/img/indexing2.png)\n\nAt query-time, you can further winnow the data on the fields you chose to index:\n\n```sh\n# cloudant query\ncurl -X POST \\\n     -d'{\"selector\":{\"Date\": {\"$gte\":\"2015-01-01\", \"$lt\":\"2015-02-01\"}},\"ddoc\":\"date-index2\"}' \\\n     -H 'Content-type: application/json' \\\n     https://U:P@host.cloudant.com/accidents/_find\n```\n\nOur query for 2015 accidents using this partial index now only contains accidents that occurred during the weekend.\n\nFor extra code readability, you can also include the original partial selector in your query-time selector:\n\n```js\n{\n  \"$and\": {\n    \"Date\": {\n      \"$gte\": \"2015-01-01\",\n      \"$lt\": \"2015-02-01\"\n    },\n    \"$or\": [\n      { \"Day_of_Week\": \"Saturday\"}, \n      { \"Day_of_Week\": \"Sunday\"}]\n  }\n}\n```\n\nCloudant will only use a partial index to answer a query if you specify either the `ddoc` or `use_index` fields at query time. See [the documentation for full details](https://console.bluemix.net/docs/services/Cloudant/api/cloudant_query.html#creating-an-index-with-a-selector).\n\n## Why partial indexes?\n\nPartial indexes pre-filter the data before it is written to the index. This can make your indicies smaller, leaner and quicker for Cloudant to work with. Also, as Cloudant pay-as-you-go services charge per GB of data (which includes the data used to store your core JSON and index data), smaller indexes can help keep your costs down too!\n",
    "url": "/2018/01/10/Partial-Indexes.html",
    "tags": "Indexing",
    "id": "29"
  },
  {
    "title": "Local documents",
    "description": "When you need to leave a bit of config behind",
    "content": "\n\nThe Apache CouchDB&trade; family has a JSON document database for every application:\n\n- [Apache CouchDB](http://couchdb.apache.org/) can be installed on your own desktop or servers in single node or clustered installations\n- [IBM Cloudant](https://www.ibm.com/cloud/cloudant) is a hosted version of CouchDB running on IBM's cloud\n- [PouchDB](https://pouchdb.com/) can be installed in [many flavours](https://medium.com/ibm-watson-data-lab/pouchdb-the-swiss-army-knife-of-databases-c5429f3db21f) but is commonly used as an in-browser database, to store client-side data with or without network connectivity\n\nAll three members of the family can sync data between each other to create hybrid, multi-homed applications where data lives on a mobile device, or in the cloud, or both.\n\nThe act of replicating a database moves JSON documents from the source database to the target, but some documents are left behind. These are *local documents*.\n\n![A lone flamingo ornament in the sand.]({{< param \"image\" >}})\n> Photo by [Jerry Kiesewetter](https://unsplash.com/photos/QdlZY4ofH4c) on [Unsplash](https://unsplash.com/).\n\n## What are CouchDB local documents?\n\nCouchDB local documents are JSON documents whose `_id` value starts with `_local/`:\n\n```js\n{\n  \"_id\": \"_local/config\",\n  \"code\": \"red\",\n  \"defcon\": 2,\n  \"timestamp\": \"2018-02-10\"\n}\n```\n\n## What are local CouchDB documents used for?\n\nIf you are replicating data between two members of the CouchDB family (e.g., between your PouchDB-powered web app and hosted Cloudant), you may need to store some configuration on the client side that is:\n\n- never to be replicated\n- not counted in queries, views or aggregations\n\nI use local documents for storing configuration or other application state that is to remain on the machine it was created on. It saves you from the work of creating a separate client-side database just to store a tiny amount of configuration data.\n\nThe CouchDB replicator uses local documents to keep track of where it got to by writing state in local documents to the source and target database.\n\n## CRUD operations for CouchDB local documents\n\nLocal documents are created in a similar way to normal JSON documents by using an HTTP POST:\n\n```sh\nURL=\"https://username:password@host.cloudant.com/mydatabase\"\nHEAD=\"Content-type: application/json\"\nDOC='{\"_id\":\"_local/config\",\"code\":\"red\"}'\ncurl -X POST -H \"$HEAD\" -d \"$DOC\" \"$URL\"\n# {\"ok\":true,\"id\":\"_local/config\",\"rev\":\"0-1\"}\n```\n\nOr HTTP PUT:\n\n```sh\n# note that the document _id is now in the URL\nURL=\"https://username:password@host.cloudant.com/mydatabase/_local/config\"\nHEAD=\"Content-type: application/json\"\nDOC='{\"code\":\"red\"}'\ncurl -X PUT -H \"$HEAD\" -d \"$DOC\" \"$URL\"\n# {\"ok\":true,\"id\":\"_local/config\",\"rev\":\"0-1\"}\n```\n\nOr in JavaScript using PouchDB:\n\n```js\nvar doc = {\"_id\":\"_local/config\",\"code\":\"red\"};\ndb.insert(doc).then(...);\n```\n\nOne important difference between local documents and regular documents is that you don't need to supply a `_rev` token when updating or deleting a local document. As local documents are never going to be replicated, the `_rev` token is not used and is fixed at a value \"0-1\" and can effectively be ignored.\n\nUpdating a local document is a case of simply repeating the POST or PUT with a new document (note the lack of `_rev` token):\n\n```sh\nDOC='{\"code\":\"orange\"}'\ncurl -X PUT -H \"$HEAD\" -d \"$DOC\" \"$URL\"\n```\n\nDeleting a document requires a HTTP DELETE (note the lack of `_rev` token):\n\n```sh\ncurl -X DELETE \"https://username:password@host.cloudant.com/mydatabase/_local/config\"\n# {\"ok\":true,\"id\":\"_local/config2\",\"rev\":\"0-0\"}\n```\n\nThat's all for this quick tip on `_local/` documents in the CouchDB ecosystem. Hopefully it comes in handy for your next replication job!\n\n<hr>\n\n_Apache®, [Apache CouchDB™, CouchDB™](http://couchdb.apache.org/), and the red couch logo are either registered trademarks or trademarks of the [Apache Software Foundation](http://www.apache.org/) in the United States and/or other countries._\n\n",
    "url": "/2018/02/14/Local-Documents.html",
    "tags": "Local",
    "id": "30"
  },
  {
    "title": "couchreplicate",
    "description": "Managing replication from the command-line",
    "content": "\n\n\nOne of Apache CouchDB&trade;'s killer features is replication. JSON data is easily replicated between data centers, from a mobile device to the cloud, or vice versa. The CouchDB replication protocol is shared by [Apache CouchDB](http://couchdb.apache.org/) itself, the [IBM Cloudant](https://www.ibm.com/cloud/cloudant) database-as-a-service, Cloudant Sync libraries for [iOS](https://github.com/cloudant/CDTDatastore) and [Android](https://github.com/cloudant/sync-android) and the [PouchDB](https://pouchdb.com/) in-browser database. \n\nSetting up a single replication is as easy as filling in a form in the Replication tab of your Cloudant or CouchDB dashboard:\n\n![replication-screenshot]({{< param \"image\" >}})\n\nBut what if you have hundreds of databases to replicate? What if you want to move lots of small databases, but only want to replicate 5 at any one time? This is where *couchreplicate* is here to help.\n\n*couchreplicate* is a command-line tool that allows you to kick off and monitor replications with minimal effort.\n\n## Installing couchreplicate\n\nAssuming you already have [Node.js and npm](https://nodejs.org/en/) installed on your machine, *couchreplicate* is installed with a single line:\n\n```sh\n$ npm install -g couchreplicate\n```\nYou should now have the `couchreplicate` tool installed on your machine. Check with:\n\n```sh\n$ couchreplicate --help\n```    \n    \n## Replicating a single database\n\nMigrating one database is as easy as running *couchreplicate*, supplying your source URL and target URL.\n\n```sh\n$ couchreplicate -s http://u:p@localhost:5984/mysource -t https://U:P@HOST.cloudant.com/mytarget\ncities [▇▇▇▇▇▇▇▇———————————————————————————] 32% 21.1s triggered\n```\n\nYour replication will start and you should see a progress bar start to fill up. You can also supply the database name as a separate `-d` parameter:\n\n```sh\n$ couchreplicate -d food -s http://u:p@localhost:5984-t https://U:P@HOST.cloudant.com\n```\n    \nAnd that database name will be used at both the source and target ends.\n\n## Replicating multiple databases\n\nThe tool really comes into its own when replicating more than one database. Simply supply a comma-separated list of database names in your `-d` parameter:\n\n```sh\n$ couchreplicate -d food,drink,hardware,software -s http://u:p@localhost:5984-t https://U:P@HOST.cloudant.com\nfood  [▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇]100% 42.5s completed\ndrink [▇▇▇▇▇▇▇▇———————————————————————————] 32% 21.1s triggered\n```\n\nAs one replication completes, another will be scheduled, until the entire list is exhausted.\n\nYou can control the number of simultaneous replications with the `-c` parameter:\n\n```sh\n$ couchreplicate -c 3 -d food,drink,hardware,software -s http://u:p@localhost:5984-t https://U:P@HOST.cloudant.com\nfood     [▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇—————] 82% 34.5s triggered\ndrink    [▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇———————————————] 63% 34.1s triggered\nhardware [▇▇—————————————————————————————————————] 10% 34.1s triggered\n```\n\n## Replicating all of your databases\n\nInstead of supplying a list of database names, the `-a` option can be used to indicate that *all* of the databases in the source cluster are to be migrated. This is handy if you are moving from local CouchDB to Cloudant, or from one Cloudant service to another.\n\n```sh\n$ couchreplicate -c 3 -a -s http://u:p@localhost:5984-t https://U:P@HOST.cloudant.com\n```\n\n## It's all about the open source\n\nThe *couchreplicate* tool is free to use and is open-sourced under the Apache 2.0 license, so you can fork the code yourself and modify as you see fit. If you find a bug or have a contribution, we'd love to hear from you on the project's [Github page](https://github.com/ibm-watson-data-lab/couchreplicate).\n\n## Further reading\n\nIf you like using command-line tools, then there more CouchDB- and Cloudant-compatible tools from the same stable:\n\n- [couchimport](https://www.npmjs.com/package/couchimport) - import your CSV data into CouchDB/Cloudant and export your JSON documents to CSV files\n- [couchbackup](https://www.npmjs.com/package/@cloudant/couchbackup) - backup a single database to a file\n- [couchmigrate](https://www.npmjs.com/package/couchmigrate) - migrate design documents into production without loss of service\n- [couchdiff](https://www.npmjs.com/package/couchdiff) - find the difference between two databases\n- [couchshell](https://www.npmjs.com/package/couchshell) - interact with your database cluster as if it were a file system\n\nDevelopers can use Node.js and Cloudant using the following libraries:\n\n- [cloudant-node-sdk](https://github.com/IBM/cloudant-node-sdk) - the official Cloudant Node.js library\n\n<hr>\n\n_Apache®, [Apache CouchDB™, CouchDB™](http://couchdb.apache.org/), and the red couch logo are either registered trademarks or trademarks of the [Apache Software Foundation](http://www.apache.org/) in the United States and/or other countries._",
    "url": "/2018/02/22/Cloudant-Replication-with-couchreplicate.html",
    "tags": "Replication CLI",
    "id": "31"
  },
  {
    "title": "Cloudant Fundamentals 1/10",
    "description": "The Document",
    "content": "\n\n\n[Cloudant](https://www.ibm.com/cloud/cloudant) is a JSON document store, based on [Apache CouchDB](http://couchdb.apache.org/), running as-a-service in the IBM Cloud. The form of JSON you store in the database is up to you. You don't need to tell the database about the _schema_ you're using ahead of time.\n\nHere's a typical document:\n\n```js\n{\n  \"type\": \"person\",\n  \"born\": \"1743-04-13\",\n  \"name\": \"Thomas Jefferson\",\n  \"potus\": 3,\n  \"diedInOffice\": false,\n  \"address\": {\n    \"street\": \"931 Thomas Jefferson Pkwy\",\n    \"town\": \"Charlottesville\",\n    \"state\": \"Virginia\",\n    \"stateCode\": \"VA\",\n    \"zip\": \"22902\"\n  },\n  \"description\": \"Thomas Jefferson (April 13 [O.S. April 2] 1743 – July 4, 1826) was an American Founding Father who was the principal author of the Declaration of Independence and later served as the third President of the United States from 1801 to 1809. Previously, he was elected the second Vice President of the United States, serving under John Adams from 1797 to 1801. A proponent of democracy, republicanism, and individual rights motivating American colonists to break from Great Britain and form a new nation, he produced formative documents and decisions at both the state and national level. He was a land owner and farmer.\",\n  \"offices\": [\"Governor of Virginia\",\"Secretary of State\", \"Vice President\"]\n}  \n```\n\nJSON allows you to represent your data through key/value pairs that are:\n\n- strings\n- numbers\n- booleans\n- objects\n- arrays of any of the above\n\nCloudant doesn't restrict you on how detailed or \"nested\" your objects are (other database services restrict you to top-level key/values, for instance), so you can have arrays of objects that contain arrays of objects, if that is what you need.\n\n![Citrus and apples, in the same bag]({{< param \"image\" >}})\n> Photo by [Kristina Tripkovic](https://unsplash.com/photos/fvC5KxA5mPw?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText).\n\n## Rules of thumb\n\n### Avoid thinking in joins\n\nCloudant doesn't do joins like a relational database, so the JSON objects tend to be denormalised (i.e., contain repetitions of data held elsewhere)ok . Don't try to  model your data in a relational way and join it yourself in your app. \n\n### Size matters\n\nCloudant will refuse to store documents over 1MB in size, but you shouldn't be going anywhere near that hard limit. The example document above is less than 1KB. \n\n### Schemaless doesn't mean schema-free\n\nAlthough Cloudant won't object if you use a different schema for each document in your database, that's probably not what you want. One of the first things I do when creating a new project is design the schema, in other words sketch the JSON that represents the objects I'm going to save and the data types of each key. I don't have to tell *Cloudant* what the schema is, but that doesn't mean I shouldn't give careful consideration to my data model before I jump into the code.\n\nUnlike other databases, Cloudant doesn't enforce data types or mandatory fields or permitted ranges of values, but it's likely that you still want to do that in your app. \n\n### Multiple object types in the same database\n\nOne widely used pattern that is odd to folks coming from a relational background is storing different object types in the same database. You could have blog posts and authors in the same Cloudant database using their own separate schema. One convention is to use the `type` field to indicate which flavour of object it is.\n\n```js\n{\n  \"type\": \"post\",\n  \"name\": \"Thought Leadership for Beginners\",\n  \"description\": \"How to think yourself to success.\",\n  \"url\": \"https://myblog.com/2018-04-02-thougt-leadership.html\",\n  \"data\": \"2018-04-02\",\n  \"author\": \"John Doe\"\n}\n```\n\n```js\n{\n  \"type\": \"author\",\n  \"name\": \"John Doe\",\n  \"url\": \"https://myblog.com/\",\n  \"registered\": \"2005-09-18\",\n  \"email\": \"john.doe@myspace.com\",\n  \"confirmed\": true\n}\n```\n\n### Write-only document patterns\n\nCloudant allows documents to be updated, but not on a field-by-field basis. For example, your e-commerce site can't do:\n\n```sql\nUPDATE stock SET stock_level = stock_level  - 1 WHERE product_id='45'\n```\n    \nYour app would have to fetch the whole document and write the whole, modified document back to the database. \n\nThink about whether you can adopt a \"write only\" approach. For the e-commerce example, a database could contain:\n\n- a document every time inventory is received into the warehouse\n- a document every time an item is sold\n\nThe sum of the stock levels could then be calculated for each product by aggregating the documents. Using this technique you are only ever *writing* documents, not updating an existing document over and over.\n\nIf you're interested in data modelling, then be sure to read [this guide](https://console.bluemix.net/docs/services/Cloudant/guides/model_data.html#my-top-5-tips-for-modelling-your-data-to-scale) from the Cloudant documentation and watch Joan Touzet's excellent talk [10 Common Misconceptions about CouchDB](https://www.youtube.com/watch?v=BKQ9kXKoHS8).\n\n## Next time\n\nNext time we'll look at a vital component of a Cloudant document: the `_id` field.",
    "url": "/2018/04/27/Cloudant-Fundamentals-1.html",
    "tags": "Fundamentals",
    "id": "32"
  },
  {
    "title": "Cloudant Fundamentals 2/10",
    "description": "The _id",
    "content": "\n\n\n[Last time](/2018/04/27/Cloudant-Fundamentals-1.html) we looked at how to design a JSON document schema that models the data in your application. I didn't mention a vital field: the document's `_id`. \n\nEvery Cloudant document has an `_id` - if you don't supply one when you write a new document then Cloudant will generate one for you. Letting Cloudant make an `_id` for you is the easiest solution, but there are some cases where you might want to keep control of the `_id` field for yourself.\n\nThe `_id` field is a document's unique identifier in a database and as such, it is indexed. This means that Cloudant can retrieve a document from a given `_id` very quickly by consulting the index - without having to page through all the documents in the collection to find the right one. \n\n![boats with their own id]({{< param \"image\" >}})\n> Photo by [Rahul Shanbhag](https://unsplash.com/photos/KsWlqXwEALg?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on Unsplash\n\nThe *primary index* used to write and retrieve documents by the `_id` field also keeps the documents in id-order on disk. We can use this to our advantage when we design your own `_id` values as we can employ the primary index to perform range queries e.g find documents where _id is less than, greater than or between supplied values. See the [GET /db/_all_docs](https://console.bluemix.net/docs/services/Cloudant/api/database.html#get-documents) API endpoint for querying the primary index.\n\nLet's say we are storing user objects in a Cloudant database. It would be perfectly valid to store documents like this:\n\n```js\n{\n  \"type\": \"user\",\n  \"name\": \"Abe Froman\",\n  \"email\": \"abe.froman@aol.com\",\n  \"registered\": \"2018-03-09T11:48:11.491Z\",\n  \"profile\": \"Sausage maker\",\n  \"city\": \"Chicago\"\n}\n```\n\nWe omit the `_id` field and let Cloudant pick one. Something like `e87a03636ee3d9d0943cd1f35f431fe7` will be generated on our behalf.\n\nIf we _know_ something unique about our user, such as their email address, we could modify the document to look like this:\n\n```js\n{\n  \"_id\": \"user:abe.froman@aol.com\",\n  \"type\": \"user\",\n  \"name\": \"Abe Froman\",\n  \"email\": \"abe.froman@aol.com\",\n  \"registered\": \"2018-03-09T11:48:11.491Z\",\n  \"profile\": \"Sausage maker\",\n  \"city\": \"Chicago\"\n}\n```\n\nNow we are storing the user type and the email address in the `_id` field. This means we can use the primary index to add a little value. We can query the primary index toget a list of all documents whose ids start with `user:` and the returned list will be in email address order.\n\n## I want my auto-incrementing values back\n\nIf you're used to relational databases, you may be familiar with auto-incrementing primary keys. The key starts at \"1\" for the first record and the number increments each time - easy! With Cloudant, you either get Cloudant to generate a unique id for you, or you create your own. If you want your document's ids to be \"1\", \"2\", \"3\" etc, it's up to you to keep track of where you're up to! \n\nI'd recommend using Cloudant's auto-generated id's or supplying your own when you know something unique about the object you are saving. \n\n## How do I generate my own unique identifier\n\nThere are libraries to do that can generate unique identifiers for you  such as the [uuid](https://www.npmjs.com/package/uuid) package for Node.js:\n\n```js\nconst uuidv4 = require('uuid/v4');\nuuidv4(); // ⇨ '416ac246-e7ac-49ff-93b4-f7e94d997e6b'\n```\n\nAlternatively you could ask Cloudant to supply a list of ids for you with the [GET /_uuids](https://console.bluemix.net/docs/services/Cloudant/api/advanced.html#-get-_uuids-) API call.\n\n```js\n{\n\t\"uuids\": [\n\t  \"6260efe4dfe1b6fc9b1f65257446080c\", \n\t  \"6260efe4dfe1b6fc9b1f6525744613dd\", \n\t  \"6260efe4dfe1b6fc9b1f6525744616b1\", \n\t  \"6260efe4dfe1b6fc9b1f6525744616c3\", \n\t  \"6260efe4dfe1b6fc9b1f652574461d91\"]\n}\n```\n\n## Can I edit an _id once it's in the database?\n\nAlthough you can edit a document body, you can't change a document's id. There's nothing stopping you deleting the unwanted document and creating a new one. You can even do both the delete and the insert operations at the same time usint a `POST /db/_bulk_docs` request.\n\n## Next time\n\nIn the next post, we'll unlock the mysteries of the `_rev` token.\n",
    "url": "/2018/05/14/Cloudant-Fundamentals-2.html",
    "tags": "Fundamentals",
    "id": "33"
  },
  {
    "title": "Cloudant Fundamentals 3/10",
    "description": "The _rev token",
    "content": "\n\n\nIn [part one](/2018/04/27/Cloudant-Fundamentals-1.html) of this series we looked at Cloudant JSON, and in [part two](/2018/05/14/Cloudant-Fundamentals-2.html) we saw how an `_id` is made. In this part we'll focus on the humble `_rev` token.\n\nWhen you first create a document, you don't need to worry about the `_rev` token &mdash; it is generated for you and returned to you in the receipt.\n\nIf we [create a new document](https://console.bluemix.net/docs/services/Cloudant/api/document.html#create) with a body of `{\"a\":1,\"b\":2}`, we get a reply from the database of:\n\n```js\n{\n  \"ok\":true,\n  \"id\":\"4245507c8acee2f2986298688244708c\",\n  \"rev\":\"1-25f9b97d75a648d1fcd23f0a73d2776e\"\n}\n```\n\nWe can see the `_id` and the `_rev` if we [fetch the document](https://console.bluemix.net/docs/services/Cloudant/api/document.html#read):\n\n```js\n{\n  \"_id\":\"4245507c8acee2f2986298688244708c\",\n  \"_rev\":\"1-25f9b97d75a648d1fcd23f0a73d2776e\",\n  \"a\":1,\n  \"b\":2\n}\n```\n\nFields starting with the underscore character `_` are reserved for Cloudant-specifc purposes. You can't add your own custom `_name` field, for example.\n\n![Pine trees in a forest, with sun shining through the canopy]({{< param \"image\" >}})\n> Photo by [Sergei Akulich](https://unsplash.com/photos/HyEtBCPlgmY?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/search/photos/versions?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n\n## What is the _rev token?\n\nThe `_rev` token consists of two parts separated by a hyphen character `-`:\n\n- a number that increments with each version of the document\n- a 32-character string that is a [cryptographic hash](https://simple.wikipedia.org/wiki/Cryptographic_hash_function) of the document's body.\n\n## Why does Cloudant have a _rev token?\n\nThe `_rev` token keeps track of the revisions that a document goes through in its life:\n\n1. First revision 1-25f9b97d75a648d1fcd23f0a73d2776e\n2. Second revision 2-524e981baaeec9bbecf92c4c01242308\n3. Third revision 3-e95ca5ca4dc5407fd09b8e0e0acf25fd\n\nCloudant actually stores revisions in a tree data structure, the simplest form being an ever-growing list of revisions:\n\n![revision tree](/img/revtree.png)\n\nThings can get much more complicated than this when we talk about [conflicts](https://developer.ibm.com/dwblog/2015/cloudant-document-conflicts-one/) but that is for another time.\n\nAs to why data is stored like this, it's because Cloudant was built to work as a distributed database with the data stored across many nodes in a cluster. Distributed systems are complicated, and the revision tree allows the database to handle conflicting writes without losing data, rather like Git would not lose data in a conflicting merge. The revision tree is also essential when replicating data from one location to another. Two databases in any state could be replicated in either direction without loss of data, thanks to the revision tree.\n\n## Can I use the revsion tree as a version control system for my documents?\n\nNo.\n\nCloudant doesn't keep the _bodies_ of old revisions (they are destroyed in a process called \"compaction\"), but the history of revision *tokens* is retained.\n\n\n## Deleting a document creates another revision\n\nA Cloudant document can never really be deleted. When you do a [delete API call](https://console.bluemix.net/docs/services/Cloudant/api/document.html#delete) another revision is added to the end of the tree:\n\n1. First revision 1-25f9b97d75a648d1fcd23f0a73d2776e\n2. Second revision 2-524e981baaeec9bbecf92c4c01242308\n3. Third revision 3-e95ca5ca4dc5407fd09b8e0e0acf25fd\n4. Fourth revision 4-d0b8f4e0375c952eb957de7dc1947aef\n\nThe last revision will look like this:\n\n```js\n{\n  \"_id\", \"4245507c8acee2f2986298688244708c\",\n  \"_rev\":\"4-d0b8f4e0375c952eb957de7dc1947aef\",\n  \"_deleted\": true\n}\n```\n\nDeleting a document leaves this final revision and the tree of revision tokens behind.\n\n## I don't care about revision tokens - make them go away\n\nYou can't really make revision tokens go away, but there are libraries aimed at new starters that hide them from you so you can get on with building your app. Take a look at [cloudant-quickstart](https://www.npmjs.com/package/cloudant-quickstart) which does just that.\n\nEven if you are putting your fingers in your ears and pretending that revision tokens don't exist, they are still being recorded in the database so you should try to avoid modifying the same document over and over if possible and be aware that a deleted document leaves a piece of data behind.\n\n## Next time\n\nIn the next part we'll take a look at using Cloudant's HTTP API using the `curl` command-line tool.\n",
    "url": "/2018/05/22/Cloudant-Fundamentals-3.html",
    "tags": "Fundamentals",
    "id": "34"
  },
  {
    "title": "Date formats",
    "description": "Storing date & time in JSON",
    "content": "\n\n\n[Apache CouchDB](http://couchdb.apache.org/) and IBM Cloudant are JSON document stores and as such don't have a native _date_ type - only the data primitives allowed in the JSON specification. \n\nBefore discussing date storage formats, we should first tackle the issue of timezones. Timezones are [utterly baffling](https://www.youtube.com/watch?v=cGXp34c_o48&feature=youtu.be&t=12s) so best practice is to store dates in the **UTC timezone** in the database even if your data originated from many places across the globe. Storing data in the same univeral timezone in the database means all the dates and times in our date store are in the same \"units\". There's nothing stopping your front end app converting these dates into a format according to the locale of each of your users. Using UTC also neatly sidesteps the issue of Daylight Savings Time!\n\nAs to storage formats, there are three options in common use.\n\n## 1. Store date as a single ISO-8601 string\n\nThere's an [international standard](https://www.iso.org/iso-8601-date-and-time-format.html) for storing date and time as a human and machine readable string:\n\n```js\n{\n  \"name\": \"Glynn\",\n  \"datetime\": \"2018-05-02T15:02:40.628Z\"\n}\n```\n\nIn JavaScript you can create this format with:\n\n```js\nnew Date().toISOString()\n```\n\nIt consists of:\n\n- year, month and day separated by hyphens.\n- a \"T\" character to separate the date and time elements.\n- hour, minute and second (to microsecond precision) separated by colons.\n- UTC timezone indicated by `Z`, which refers to the military timezone [Zulu](https://en.wikipedia.org/wiki/List_of_military_time_zones).\n\nThis is a good general-purpose format that is compact, sorts in date/time order and can be returned to a date object in the constructor of the JavaScript _Date_ object e.g when manipulating dates in a [MapReduce view](https://console.bluemix.net/docs/services/Cloudant/api/creating_views.html#views-mapreduce-):\n\n```js\nfunction(doc) {\n  var d = new Date(doc.datetime);\n  var month = d.getMonth() + 1; // 0 - 11\n  var year = d.getFullYear();\n  emit([year, month], doc.name);\n}\n```\n\nwhich when packaged into a [Design Document](https://console.bluemix.net/docs/services/Cloudant/api/design_documents.html#design-documents) with a built-in reducer can be queried with:\n\n```sh\ncurl \"$URL/_design/mydesigndoc/_view/myview?group=true\"\n```\n\nThis produces time-ordered, hierarchical, grouped aggregations of your data.\n\nWhat is less obvious is that if you had chosen to store the data in different format, such as `2018-05-02 15:02:40`, then the call to `new Date(doc.datetime)` would have failed. This is due to the older version of the SpiderMonkey JavaScript engine used by CouchDB & Cloudant being unable to parse this unrecognised date format.\n\n## 2. Store date as a timestamp\n\nInstead of storing your date and time as a string, you could opt to store an integer timestamp instead - specifically, the number of milliseconds since 1 January 1970 00:00:00 UTC.  \n\n```js\n{\n  \"name\": \"Glynn\",\n  \"timestamp\": 1525273360628\n}\n```\n\nAlthough this is not as human-readable as the ISO-8601 string, it is still machine-readable, sorts in date & time order and can be easily converted back into a _Date_ object in a map function:\n\n```js\nfunction(doc) {\n  var d = new Date(doc.timestamp);\n  var month = d.getMonth() + 1; // 0 - 11\n  var year = d.getFullYear();\n  emit([year, month], doc.name);\n}\n```\n\nThis format is not really suitable for storing dates before 1970 (although negative timestamps do work!) but it may be useful for simple date arithmetic as one timestamp can be subtracted from another to calculate the time difference.\n\n```js\n// calculate days since presidential inauguration\nvar a = new Date('2017-01-20T17:00:00.000Z');\nvar b = new Date();\nvar diff = b.getTime() - a.getTime();\nvar days = diff / 1000*60*60*24;\n// 467.9\n```\n\n## 3. Store date and time components in separate fields\n\nThe third option is to store each date and time component separately:\n\n```js\n{\n  \"name\": \"Glynn\",\n  \"year\": 2018,\n  \"month\": 5,\n  \"day\": 2,\n  \"hour\": 15,\n  \"minute\": 2,\n  \"second\": 40,\n  \"millsecond\": 628\n}\n```\n\nThis is more verbose than the previous two solutions but has the advantage that the data is ready for querying and indexing without any pre-processing. This is particularly important when using [Cloudant Query](https://console.bluemix.net/docs/services/Cloudant/api/cloudant_query.html#query) - MapReduce views are commonly used to pre-process data before it is *emitted* into the index but there is no such facility when using Cloudant Query. For Cloudant Query, the data *has* to be in the document and in the correct format. \n\nCreate an index with the `/db/_index` endpoint or through the dashboard:\n\n![date ui](/img/date1.png)\n\nand query the index using the `/db/_find` endpoint or again, using the dashboard:\n\n![date ui](/img/date2.png)\n\nIf the data were stored in ISO-8601 format, it would not be possible to index or query a single component (e.g. the year) on its own using Cloudant Query.\n\n## Date & time gotchas\n\n- Remember that the JavaScript `Date.getMonth()` and `Date.setMonth()` functions use number 0-11 to represent months January to December.\n- When extracting data from a Javascript _Date_ object, remember it's `getDate()`, `getMonth()+1` and `getFullYear()`, not the function names you might expect!\n- The JavaScript engine used in the MapReduce engine can't parse many date formats in its constructor. It can deal with ISO-8601 format and milliseconds  since 1970, and that's about it.\n- If you use string manipulation to split up a date you may be run into this conundrum in your map function\n\n```js\nfunction(doc) {\n  // split up the date 2018-08-09\n  var bits = doc.date.split('-');\n  var year = parseInt(bits[0]);\n  var month = parseInt(bits[1]);\n  var day = parseInt(bits[2]);\n  emit([year, month, day], doc.temperature);\n}\n```\n\nIn the above code we've elected to store only the date as a string. We're splitting the string by the `-` character, turning the year/month/day pieces into integers and using them to create a hierarchical index. What's the problem with this approach?\n\nTake the date `2018-08-09` (9th of August). In this case the data calculated would be:\n\n- year - 2018\n- month - 0\n- day - 1\n\nWhy? Because the indexer's SpiderMonkey JavaScript engine interprets the leading zero on `08` and `09` to indicate that you wish the date to be parsed as [Octal](https://en.wikipedia.org/wiki/Octal) numbers! It can be remedied with:\n\n```js\n  var month = parseInt(bits[1], 10);\n  var day = parseInt(bits[2], 10);\n```\n\nThis indicates that your strings are in decimal.\n",
    "url": "/2018/05/22/Date-formats.html",
    "tags": "Date",
    "id": "35"
  },
  {
    "title": "Cloudant Fundamentals 4/10",
    "description": "Using the API with curl",
    "content": "\n\n\nThe [curl](https://curl.haxx.se/) command-line tool allows you to make HTTP requests from a terminal:\n\n```sh\n$ curl 'http://www.website.com/'\n<html>\n<h1>This is a web page</h1>\n</html>\n```\n\n[Cloudant's API](https://console.bluemix.net/docs/services/Cloudant/api/index.html#api-reference-overview) is entirely HTTP. You don't need any special software drivers or to understand a bespoke protocol &mdash; it's just web requests. You can access the database from a browser, a mobile app, any programming language or, in this case, from the command line.\n\nThe first thing you'll need is your Cloudant service's URL. It'll look something like this:\n\n```\nhttps://username:password@hostname.cloudant.com\n```\n\n- all traffic between you and Cloudant is sent over HTTPS\n- the administrator's username and password are encoded into a URL using \"Basic\" authentication (other forms of [authentication](https://console.bluemix.net/docs/services/Cloudant/api/authentication.html#authentication) are available)\n- the domain name `hostname.cloudant.com` is the address of your Cloudant cluster. \n\n\n## Put your URL in an environment variable\n\nWe don't want to have to type that URL every time, so a nice shortcut is to put it into an environment variable:\n\n```sh\n$ export URL=\"https://username:password@hostname.cloudant.com\"\n```\n\nWe can then access our the `$URL` variable using `curl` which comes pre-installed on Macs and Linux machines and is available [to download](https://curl.haxx.se/) on other platforms:\n\n```sh\n$ curl $URL\n{\"couchdb\":\"Welcome\",\"version\":\"2.1.1\",\"vendor\":{\"name\":\"IBM Cloudant\",\"version\":\"6656\",\"variant\":\"paas\"},\"features\":[\"geo\",\"scheduler\",\"iam\"]}\n```\n\nIf you see some JSON, you're in!\n\n![A dewy spider web]({{< param \"image\" >}})\n> Photo by [michael podger](https://unsplash.com/photos/jpgRztEuaV4?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on Unsplash\n\n## Let's create a database\n\nDatabases are created with a `PUT` API call.\n\n```sh\n$ curl -X PUT \"$URL/newdb\"\n{\"ok\":true}\n```\n\nthe `-X` parameter defines the HTTP method. If omitted it is assumed to be `GET`, but `PUT`, `POST` and `DELETE` (and others) are allowed. We are asking a database called `newdb` to be created and as we got `{\"ok\":true}` back, then it worked!\n\nIf we tried to do the same thing again, we'd get an error because it already exists:\n\n```sh\n$ curl -X PUT \"$URL/newdb\"\n{\"error\":\"file_exists\",\"reason\":\"The database could not be created, the file already exists.\"}\n```\n\nSometimes it's good to see the HTTP status code in the response. Adding `-i` to the command will do that:\n\n```sh\n$ curl -i -X PUT \"$URL/newdb\"\nHTTP/2 412\ncontent-type: application/json\n\n{\"error\":\"file_exists\",\"reason\":\"The database could not be created, the file already exists.\"}\n```\n\nIn the above example `412` is our error code. There's a [full list here](https://console.bluemix.net/docs/services/Cloudant/api/http.html#http-status-codes), but in general:\n\n- 20x - good\n- 30x - nothing happened\n- 40x/50x - your request failed\n\n## Adding data\n\nWe can add data to our database by posting a JSON object. We have to specify the data *is* JSON by specifying a \"Content-type\" header:\n\n```sh\n$ curl -X POST \\\n    -H \"Content-type: application/json\" \\\n    -d '{\"x\":1}' \\\n    \"$URL/newdb\"\n{\"ok\":true,\"id\":\"2ded8ec775b6728227143ac575613060\",\"rev\":\"1-0785e9eb543380151003dc452c3a001a\"}\n```\n\nThis time we're using a `POST` endpoint and passing our JSON using the `-d` parameter. Cloudant has auto-generated an ID for us which we see in the response.\n\nIf the JSON you want to add is in a file, you can do the following instead:\n\n```sh\n$ curl -X POST \\\n    -H \"Content-type: application/json\" \\\n    -d@myfile.json \\\n    \"$URL/newdb\"\n```\n\n## Reading the document back\n\nWe can read the document back again with a `GET` request:\n\n```sh\n$ curl \"$URL/newdb/2ded8ec775b6728227143ac575613060\"\n{\"_id\":\"2ded8ec775b6728227143ac575613060\",\"_rev\":\"1-0785e9eb543380151003dc452c3a001a\",\"x\":1}\n```\n\nWhere `2ded8ec775b6728227143ac575613060` was the auto-generated ID (it will be a different ID in your case).\n\n## Modifying a document\n\nTo create another revision we need to do a new POST request, passing in the new document body *including the old document's revision token*:\n\n```sh\n$ curl -X POST \\\n       -H \"Content-type: application/json\" \\\n       -d '{\"_id\":\"2ded8ec775b6728227143ac575613060\",\"_rev\":\"1-0785e9eb543380151003dc452c3a001a\",\"x\":2}' \\\n       \"$URL/newdb\"\n{\"ok\":true,\"id\":\"2ded8ec775b6728227143ac575613060\",\"rev\":\"2-8c49edca19d786e747fb5bea32c4cb91\"}\n```\n\nWe are free to add any new fields we want in the next revision of the document. Your schema can evolve over time to reflect the data your application needs to store. Cloudant doesn't allow you to modify individual attributes of the document &mdash; you must present the entire document you want to write with each revision.\n\n## Deleting a document\n\nThe act of deleting a document causes a final revision to the document to be added with a `_deleted: true` flag added. Cloudant needs to know the ID of the document and the revision token that is to be deleted.\n\n```sh\n$ curl -X DELETE \"$URL/newdb/2ded8ec775b6728227143ac575613060?rev=2-8c49edca19d786e747fb5bea32c4cb91\"\n{\"ok\":true,\"id\":\"2ded8ec775b6728227143ac575613060\",\"rev\":\"3-fb567087695adb203ba116e130794a84\"}\n```\n\nThis time our HTTP method is \"DELETE\" and the URL contains the document ID. The revision token is passed in as a \"rev\" parameter. Notice how you get another revision token in reply.\n\n## Reducing the keyboard strain\n\nIf you're sick of typing long curl commands, you can reduce the strain by creating an \"alias\".\n\n```sh\n$ alias acurl='curl -i -H \"Content-type: application/json\"'\n```\n\nUsing your `acurl` alias conjunction with your `URL` environment variable, you get:\n\n```sh\n$ acurl -X POST -d@myfile.json \"$URL/newdb\"\n```\n\nA more comprehensive `acurl` can be found [here](https://console.bluemix.net/docs/services/Cloudant/guides/acurl.html#authorized-curl-acurl-) which exchanges your credentials for a token and uses this in subsequent requests.\n\n## Next time\n\nNow we are able to create, read, update and delete single documents, next time we'll find out how to do the same thing in bulk.\n\n\n\n",
    "url": "/2018/05/29/Using-API-with-curl.html",
    "tags": "Fundamentals API",
    "id": "36"
  },
  {
    "title": "Emoji in Cloudant",
    "description": "🤔 ?",
    "content": "\n\n\nCloudant and Apache CouchDB databases store their data as JSON documents so there's only a handful of data types to choose from: strings, booleans, numbers, objects and arrays.\n\nHere's a document representing a person in a JSON:\n\n```js\n{\n  \"name\": \"Glynn\",\n  \"startDate\": \"2018-05-11T15:29:31.354Z\",\n  \"verified\": true,\n  \"mood\": \"tired\",\n  \"nationality\": \"British\",\n  \"favouriteFood\": \"Eggs\",\n  \"musicalInstruments\": [\"Guitar\",\"Piano\",\"Voice\"],\n  \"phobia\": \"Spiders\",\n  \"profile\": \"Does computers at IBM.\"\n}\n```\n\nLong gone are the days when we were limited to the [ASCII](https://www.asciitable.com/) character set in our code. In recent years, the characters we use to form sentences have go a lot more colourful.\n\n![emoji1](/img/emoji5.jpg)\n> Photo by iabzd on [Unsplash](https://unsplash.com/photos/YfpP8_IxKmQ)\n\nLet's try remodelling our document by using Emoji:\n\n```js\n{\n  \"name\": \"Glynn\",\n  \"startDate\": \"2018-05-11T15:29:31.354Z\",\n  \"verified\": true,\n  \"mood\": \"😫\",\n  \"nationality\": \"🇬🇧\",\n  \"favouriteFood\": \"🍳\",\n  \"musicalInstruments\": [\"🎸\",\"🎹\",\"🎤\"],\n  \"phobia\": \"🕷\",\n  \"profile\": \"Does 🖥 at IBM.\"\n}\n```\n\nIs this valid? Will Cloudant accept documents like this? It turns out we're good to go!\n\n![emoji1](/img/emoji1.png)\n\nCloudant is happy to store any valid JSON containing Unicode strings. Sometimes the Emojis render differently across devices depending on the character set used. \n\n## Can I use an Emoji as a key field?\n\nYes you can:\n\n![emoji2](/img/emoji2.png)\n\n## Can I use an Emoji as a document attribute?\n\nYes you can:\n\n![emoji2](/img/emoji6.png)\n\n## What about in Map functions?\n\nIn JavaScript map functions, values of fields containing Emoji can be tested and Emoji keys and values can be emitted:\n\n![emoji3](/img/emoji3.png)\n\n## Can I query using Emoji?\n\nYes you can:\n\n![emoji4](/img/emoji4.png)\n\n## Why would I want an Emoji in a JSON document?\n\nJust because you can do something, doesn't mean you should. \n\nIn the above examples, I'm using Emojis like an enumerated data type where the symbols represent one of a set of allowed data values \n\n- Nationality: 🇬🇧 🇫🇷 🇩🇪 🇨🇳 🇯🇵\n- Mood: 😀 🤔 😐 ☹\n- Weather:  ☀ ⛅ 🌧 🌪\n\nAnother use-case is the storage of Twitter, Slack, SMS or other strings that happen to contain Emoji in a JSON database. The data will be indexed correctly and can be queried and aggregated like any other data.",
    "url": "/2018/05/30/Emoji-in-Cloudant-documents.html",
    "tags": "Emoji",
    "id": "37"
  },
  {
    "title": "Cloudant Fundamentals 5/10",
    "description": "Using the Bulk API",
    "content": "\n\n\nIn the [last blog post](https://medium.com/ibm-watson-data-lab/cloudant-fundamentals-using-the-api-with-curl-4c4a4f278104) we saw how to do CRUD (Create/Read/Update/Delete) operations with the Cloudant database using the curl command line tool. In this post we'll use just two API calls to achieve the same thing, but with the capability of working on multiple documents at the same time.\n\nIf you have two or five or a hundred documents to add to Cloudant, then you need to look at [bulk operations API](https://console.bluemix.net/docs/services/Cloudant/api/document.html#bulk-operations). If your app is going to go to the trouble making an HTTP call (doing a DNS lookup, creating a connection, negotiating a HTTPS handshake etc), then it may as well do as much work as it can with that connection while it can. It is much more efficient for you to bulk upload 100 documents in a single bulk request than send them in 100 separate API calls.\n\nThere's only two API calls you need to know about:\n\n- `GET or POST /db/_all_docs` - for reads\n- `POST /db/_bulk_docs` - for creates, updates and deletions\n\n## Creating documents in bulk\n\nLet's create a file called `bulk.json` that contains the documents we want to write:\n\n```js\n{\n  \"docs\": [\n    {\"name\": \"Ferris Bueller\", \"actor\": \"Matthew Broderick\", \"dob\": \"1962-03-21\"},\n    {\"name\": \"Sloane Peterson\", \"actor\": \"Mia Sara\", \"dob\": \"1967-06-19\"},\n    {\"name\": \"Cameron Frye\", \"actor\": \"Alan Ruck\", \"dob\": \"1956-07-01\"}\n  ] \n}\n```\n\nWe can write the three documents in a single API call:\n\n```sh\n$ curl -X POST \\\n       -H 'Content-type: application/json' \\\n       -d@bulk.json \\\n       \"$URL/newdb/_bulk_docs\"\n[{\"ok\":true,\"id\":\"6545abac34ff08ea39aaafb5ca1765c4\",\"rev\":\"1-974e44505640c47cd31db6d4949aaff5\"},{\"ok\":true,\"id\":\"6545abac34ff08ea39aaafb5ca176920\",\"rev\":\"1-9bcc8585a4cb3144fcffe8201f3c56d4\"},{\"ok\":true,\"id\":\"6545abac34ff08ea39aaafb5ca177037\",\"rev\":\"1-8e73b2f79fcf8ef1cafa37d196808ecd\"}]\n```\n\nCloudant replies back with an array, with one element for each document inserted telling you the auto-generated id and the calculated rev token.\n\nIf we'd wanted to specify the _id fields we could have simply included them in the document objects in the submitted `bulk.json` file.\n\n## Reading the documents back in bulk\n\nAs well as reading back single documents:\n\n```sh\n$ curl \"$URL/newdb/6545abac34ff08ea39aaafb5ca1765c4\"\n{\"_id\":\"6545abac34ff08ea39aaafb5ca1765c4\",\"_rev\":\"1-974e44505640c47cd31db6d4949aaff5\",\"name\":\"Ferris Bueller\",\"actor\":\"Matthew Broderick\",\"dob\":\"1962-03-21\"}\n```\n\nwe can use the `GET /db/_all_docs` endpoint to fetch multiple documents at once if we supply an array of document ids:\n\n```sh\n$ curl \"$URL/newdb/_all_docs\"\n{\"total_rows\":3,\"offset\":null,\"rows\":[\n{\"id\":\"6545abac34ff08ea39aaafb5ca1765c4\",\"key\":\"6545abac34ff08ea39aaafb5ca1765c4\",\"value\":{\"rev\":\"1-974e44505640c47cd31db6d4949aaff5\"}},\n{\"id\":\"6545abac34ff08ea39aaafb5ca176920\",\"key\":\"6545abac34ff08ea39aaafb5ca176920\",\"value\":{\"rev\":\"1-9bcc8585a4cb3144fcffe8201f3c56d4\"}},\n{\"id\":\"6545abac34ff08ea39aaafb5ca177037\",\"key\":\"6545abac34ff08ea39aaafb5ca177037\",\"value\":{\"rev\":\"1-8e73b2f79fcf8ef1cafa37d196808ecd\"}}\n]}\n```\n\nBut wait! Where are the document bodies? Cloudant has only returned the id of the documents (twice!) and the revision token.\n\nIf you want the document bodies too, you have specify `include_docs=true` in your request:\n\n```sh\n$ curl \"$URL/newdb/_all_docs?include_docs=true\"\n{\"total_rows\":3,\"offset\":0,\"rows\":[\n{\"id\":\"6545abac34ff08ea39aaafb5ca1765c4\",\"key\":\"6545abac34ff08ea39aaafb5ca1765c4\",\"value\":{\"rev\":\"1-974e44505640c47cd31db6d4949aaff5\"},\"doc\":{\"_id\":\"6545abac34ff08ea39aaafb5ca1765c4\",\"_rev\":\"1-974e44505640c47cd31db6d4949aaff5\",\"name\":\"Ferris Bueller\",\"actor\":\"Matthew Broderick\",\"dob\":\"1962-03-21\"}},\n{\"id\":\"6545abac34ff08ea39aaafb5ca176920\",\"key\":\"6545abac34ff08ea39aaafb5ca176920\",\"value\":{\"rev\":\"1-9bcc8585a4cb3144fcffe8201f3c56d4\"},\"doc\":{\"_id\":\"6545abac34ff08ea39aaafb5ca176920\",\"_rev\":\"1-9bcc8585a4cb3144fcffe8201f3c56d4\",\"name\":\"Sloane Peterson\",\"actor\":\"Mia Sara\",\"dob\":\"1967-06-19\"}},\n{\"id\":\"6545abac34ff08ea39aaafb5ca177037\",\"key\":\"6545abac34ff08ea39aaafb5ca177037\",\"value\":{\"rev\":\"1-8e73b2f79fcf8ef1cafa37d196808ecd\"},\"doc\":{\"_id\":\"6545abac34ff08ea39aaafb5ca177037\",\"_rev\":\"1-8e73b2f79fcf8ef1cafa37d196808ecd\",\"name\":\"Cameron Frye\",\"actor\":\"Alan Ruck\",\"dob\":\"1956-07-01\"}}\n]}\n```\n\nNow we can see the whole document in a `doc` attribute of each element of the rows array.\n\n## Updating documents in bulk\n\nLet's update our `bulk.json` file to prepare it for a bulk update. We need to:\n\n - add the `_id`/`_rev` of each document \n - add the data we want to add, in this case the IMDB URL of each actor\n\n```js\n{\n  \"docs\": [\n    {\n      \"_id\": \"6545abac34ff08ea39aaafb5ca1765c4\",\n      \"_rev\": \"1-974e44505640c47cd31db6d4949aaff5\",\n      \"name\": \"Ferris Bueller\",\n      \"actor\": \"Matthew Broderick\",\n      \"dob\": \"1962-03-21\",\n      \"imdb\": \"http://www.imdb.com/name/nm0000111/?ref_=tt_cl_t1\"\n    },\n    {\n      \"_id\": \"6545abac34ff08ea39aaafb5ca176920\",\n      \"_rev\": \"1-9bcc8585a4cb3144fcffe8201f3c56d4\",\n      \"name\": \"Sloane Peterson\",\n      \"actor\": \"Mia Sara\",\n      \"dob\": \"1967-06-19\",\n      \"imdb\": \"http://www.imdb.com/name/nm0000214/?ref_=tt_cl_t3\"\n    },\n    {\n      \"_id\": \"6545abac34ff08ea39aaafb5ca177037\",\n      \"_rev\": \"1-8e73b2f79fcf8ef1cafa37d196808ecd\",\n      \"name\": \"Cameron Frye\",\n      \"actor\": \"Alan Ruck\",\n      \"dob\": \"1956-07-01\",\n      \"imdb\": \"http://www.imdb.com/name/nm0001688/?ref_=tt_cl_t2\"\n    }\n  ]\n}\n```\n\nUpdating these three documents is simply a matter of posting this JSON to `POST /db/_bulk_docs`\n\n```sh\n$ curl -X POST -H 'Content-type: application/json' -d@bulk.json \"$URL/newdb/_bulk_docs\"\n[{\"ok\":true,\"id\":\"6545abac34ff08ea39aaafb5ca1765c4\",\"rev\":\"2-3fe304e13f53719d577846b93a7fa865\"},{\"ok\":true,\"id\":\"6545abac34ff08ea39aaafb5ca176920\",\"rev\":\"2-fa0d94cb2361136c666267746ae4b682\"},{\"ok\":true,\"id\":\"6545abac34ff08ea39aaafb5ca177037\",\"rev\":\"2-bccbb9d28769bb4bf79116cf59c01c45\"}]\n```\n\nand Cloudant gives back a new revision token for each document.\n\n## Bulk deletions\n\nBulk deletions is similar to bulk updates, except that we don't need to supply a document body, only a `_deleted: true` flag for each id/rev pair:\n\n```js\n{\n  \"docs\": [\n    {\n      \"_id\": \"6545abac34ff08ea39aaafb5ca1765c4\",\n      \"_rev\": \"2-3fe304e13f53719d577846b93a7fa865\",\n      \"_deleted\": true\n    },\n    {\n      \"_id\": \"6545abac34ff08ea39aaafb5ca176920\",\n      \"_rev\": \"2-fa0d94cb2361136c666267746ae4b682\",\n      \"_deleted\": true\n    },\n    {\n      \"_id\": \"6545abac34ff08ea39aaafb5ca177037\",\n      \"_rev\": \"2-bccbb9d28769bb4bf79116cf59c01c45\",\n      \"_deleted\": true\n    }\n  ]\n}\n```\n\nwhich we post to `_bulk_docs`:\n\n```sh\n$ curl -X POST \\\n       -H 'Content-type: application/json' \\\n       -d@bulk.json \\\n       \"$URL/newdb/_bulk_docs\"\n[{\"ok\":true,\"id\":\"6545abac34ff08ea39aaafb5ca1765c4\",\"rev\":\"3-6f40626814e930dbcb0d17ad4a82e9eb\"},{\"ok\":true,\"id\":\"6545abac34ff08ea39aaafb5ca176920\",\"rev\":\"3-b0d3619a0af1d356b67dd15e7309e1a6\"},{\"ok\":true,\"id\":\"6545abac34ff08ea39aaafb5ca177037\",\"rev\":\"3-1f4738f25cf3da86f1646f69740d88fb\"}]\n```\n\nto get another set of revision tokens.\n\nNote you can combine inserts, updates and deletes in the same `bulk_docs` call.\n\nDon't forget we can use the `acurl` alias we created in the last post to shorten these commands:\n\n```sh\n$ acurl -X POST -d@bulk.json \"$URL/newdb/_bulk_docs\"\n```\n\nIf you're not keen on command-line tools but want to learn the API, then you could also look at the [Postman](https://chrome.google.com/webstore/detail/postman/fhbjgbiflinjbdggehcddcbncdddomop?hl=en) Chrome extension, which allows low-level API calls to constructed in a graphical user interface.\n\n## Next time\n\nIn the next blog we'll look at the programmatic equivalents of these Cloudant create/read/update/delete and bulk operations.",
    "url": "/2018/06/04/Cloudant-Fundamentals-The-Bulk-API.html",
    "tags": "Fundamentals API",
    "id": "38"
  },
  {
    "title": "Cloudant Fundamentals 6/10",
    "description": "Programmatic CRUD",
    "content": "\n\n\nIn the previous two posts we saw how how the command-line tool `curl` is all that is required to do basic read and write operations with Cloudant, and how two API calls can be used for bulk commands.\n\nIn this post we'll look at equivalent tasks using programmatic means.\n\nYou don't _need_ a special library to work with Cloudant -  just something capbable of making HTTP requests. The libraries do help, however, with authentication and parameter encoding. There are four officially supported libraries for four programming languages:\n\n- [Node.js](https://github.com/IBM/cloudant-node-sdk)\n- [Python](https://github.com/IBM/cloudant-python-sdk)\n- [Java](https://github.com/IBM/cloudant-java-sdk)\n- [Go](https://github.com/IBM/cloudant-go-sdk)\n\nFor these examples I'm going to use the [Node.js](https://github.com/IBM/cloudant-node-sdk) library.\n\n## Connect to the database\n\nLet's set up our Cloudant credentials in environment variables:\n\n```sh\n# the URL of the Cloudant service\nexport CLOUDANT_URL='https://mycloudantservice.cloudant.com'\n# the IAM API key\nexport CLOUDANT_APIKEY='myapikey'\n```\n\nThen we can include the library and get it set up:\n\n```js\nconst { CloudantV1 } = require('@ibm-cloud/cloudant')\nconst client = CloudantV1.newInstance()\n```\n\n## Create a the database\n\nCreating a database is as simple as calling the `putDatabase` function:\n\n```js\nawait client.putDatabase({\n  db: 'mydatabasename',\n  partitioned: false\n})\n// { ok: true }\n```\n\n## Creating documents\n\nLet's say we want to add an array of documents to our database:\n\n```js\nconst docs = [\n    {\"name\": \"Ferris Bueller\", \"actor\": \"Matthew Broderick\", \"dob\": \"1962-03-21\"},\n    {\"name\": \"Sloane Peterson\", \"actor\": \"Mia Sara\", \"dob\": \"1967-06-19\"},\n    {\"name\": \"Cameron Frye\", \"actor\": \"Alan Ruck\", \"dob\": \"1956-07-01\"}\n ] \n```\n\nThe `insert` document can be used to write them to the database:\n\n```js\nawait client.postBulkDocs({\n  db: 'mydatabasename',\n  bulkDocs: { docs: docs }\n})\n// [ { id: 'a', ok: true, rev: '1-123'}, { id: 'b', ok: true, rev: '1-456'}]\n```\n\nThe same goes for single documents:\n\n```js\nconst newdoc = { name: \"Ed Rooney\", actor: \"Jeffrey Jones\", dob: \"1946-09-28\"}\nawait client.postDocument({\n  db: 'mydatabasename',\n  document: newdoc\n})\n// { ok: true, id: '95b51f94118ae', rev: '1-678' }\n```\n\n## Reading documents\n\nDocuments can be read back singly by specifying the id of the document you want:\n\n```js\nawait client.getDocument({\n  db: 'mydatabasename',\n  docId: '95b51f94118ae2d852393c63edacf462'\n})\n// { _id: '95b51f94118ae2d852393c63edacf462',\n//  name: 'Ed Rooney',\n//  actor: 'Jeffrey Jones',\n//  dob: '1946-09-28' }\n```\n\nYou may also fetch a list of ids:\n\n```js\nconst ids = ['95b51f94118ae2d852393c63edacf462', 'c30959f23a9feb3abbfd40e7e848fde4']\nawait client.postAllDocs({\n  db: 'mydatabasename',\n  includeDocs: true,\n  keys: ids\n})\n// [ {...}, {...} ]\n```\n\nor ask for all the documents:\n\n```js\nawait client.postAllDocs({\n  db: 'mydatabasename',\n  includeDocs: true\n})\n```\n\n## Updating documents\n\nThe Node.js library allows documents to be updated by passing a document with its last revision (`_rev`) in place:\n\n```js\nconst modifiedDoc = { \n  _id: \"95b51f94118ae2d852393c63edacf462\",\n  _rev: '1-678'\n  name: \"Edward Rooney\", \n  actor: \"Jeffrey Duncan Jones\",\n  residence: \"Los Angeles, CA\",\n  dob: \"1946-09-28\"\n}\nawait client.postDocument({\n  db: 'mydatabasename',\n  document: modifiedDoc\n})\n// { ok: true, rev: '2-456' }\n```\n\n## Deleting a document\n\nA documents can be removed from the database by passing an id `delete` function:\n\n```js\nawait client.deleteDocument({\n  db: 'mydatabasename',\n  docId: '95b51f94118ae2d852393c63edacf462',\n  rev: '2-456'\n})\n// { ok: true, rev: '3-789' }\n```\n\n## Next time\n\nIn the next post we'll look at querying data.",
    "url": "/2018/06/12/Cloudant-Fundamentals-Programmatic-CRUD.html",
    "tags": "Fundamentals Node.js",
    "id": "39"
  },
  {
    "title": "Optimising Cloudant for Serverless",
    "description": "Reusing your connection to Cloudant",
    "content": "\n\n\nIBM Cloudant is a great fit for serverless applications - build a JSON document in the schema of your choice and post it to a Cloudant database using the HTTP API. Both [IBM Cloud Functions](https://www.ibm.com/cloud/functions) and [IBM Cloudant](https://www.ibm.com/uk-en/marketplace/database-management) can scale to deal with your application's workload without you worrying about operating systems, machine reboots, queues, networking etc.\n\nBuilding a serverless application makes you think differently about how to optimise for performance:\n\n- each incoming event is handled separately, so you cannot minimise HTTP requests to the database by bundling multiple requests into a single [bulk operation](https://console.bluemix.net/docs/services/Cloudant/api/document.html#bulk-operations).\n- slow performance of your action results in increased bills because you pay per millisecond of execution time.\n- without using the [Composer](https://www.ibm.com/blogs/bluemix/2017/10/serverless-composition-ibm-cloud-functions/) tooling to allow state to be retained between actions in sequences, each action starts with no state other than the incoming data and any parameters that were configured at deploy-time.\n\nLet's take a look at a very simple Cloud Function that writes some data to Cloudant.\n\n## How Cloud Functions talks to Cloudant\n\nWe'll use a Node.js runtime with the latest version of the official [Cloudant Node.js library](https://www.npmjs.com/package/@cloudant/cloudant). \n\nIn a blank directory we can create a new npm project:\n\n```sh\nnpm init\n```\n\nand install the library\n\n```sh\nnpm install --save @cloudant/cloudant\n```\n\nThen we write our own code into a `index.js` file:\n\n```js\n// this is our Cloud Function\nconst main = function(args) { \n\n  // configure the @cloudant/cloudant library\n  const opts = {url: args.url, plugins: ['cookieauth', 'promises']};\n  const cloudant = require('@cloudant/cloudant')(opts);  \n  const db = cloudant.db.use(args.dbname);\n  \n  // write a new document to the database\n  const doc = {\n    timestamp: new Date().getTime(),\n    num: Math.random()\n  };\n  return db.insert(doc);\n};\n\nexports.main = main;\n```\n\nThe above Node.js action simply writes a document to Cloudant when it is invoked. It is deployed with:\n\n```sh\n# zip up our action and dependencies\nzip -r action.zip index.js node_modules\n# upload the zip file to IBM Cloud Functions\nbx wsk action update justtesting action.zip \\\n    --kind nodejs:8 \\\n    --param url \"https://USER:PASS@HOST.cloudant.com\" \\\n    --param dbname justtesting\n```\n\nThe credentials of your Cloudant service and database name are baked-in to the action as parameters - you'll need to replace `\"https://USER:PASS@HOST.cloudant.com\"` with your own Cloudant service's admin credentials if you want to run this yourself. \n\nThe action is invoked from the command-line with:\n\n```sh\nbx wsk action invoke --blocking justtesting\n```\n\nThe Cloudant library translates your call to `db.insert` into an HTTP POST passing a new JSON object to Cloudant. You should see the `id` and `rev` of the newly created document output in the terminal.\n\n## What's happening under the hood?\n\nWhen our action is invoked the Cloudant library is initialized and has some work to do:\n\n- do a DNS lookup to translate your Cloudant hostname into an IP address.\n- make a secure HTTPS connection to the Cloudant service which involves performing a [TLS handshake](https://www.ibm.com/support/knowledgecenter/en/SSFKSJ_7.1.0/com.ibm.mq.doc/sy10660_.htm)\n- authenticating against the Cloudant service by exchanging your `username` & `password` for a [session cookie](https://console.bluemix.net/docs/services/Cloudant/api/authentication.html#cookie-authentication).\n\nThe library handles all of this for us but nevertheless, it is an overhead. Our action is making *two* HTTP requests in series for each invocation of the cloud function. \n\nYou should be concerned about this because *you* are being charged for the execution time of each invocation! \n\n## Re-using the connection\n\nTo avoid making an authentication request each time your action is invoked, we can make use of some inside knowledge of how Cloud Functions works. When you deploy your action, the Cloud Functionsplatform turns your code into a _container_. The platform re-uses the same container again and again when invocations happen often, retaining the code's global variable space. We can use this to our advantage to reuse the Cloudant connection. \n\nIf we store our Cloudant object in a *global* variable (as opposed to the local variable we are using now), our second and subsequent invocations will be able to re-use its data, which includes the authentication cookie. Our code now looks like this:\n\n```js\n// global Cloudant database object\nvar db = null\n\n// this is our Cloud Function\nconst main = function(args) { \n  \n  // if db isn't set we need to set up the library\n  if (!db) {\n    const opts = {url: args.url, plugins: ['cookieauth','promises']};\n    const cloudant = require('@cloudant/cloudant')(opts);  \n    db = cloudant.db.use(args.dbname);\n  }\n  \n  // write a new document to the database\n  const doc = {\n    timestamp: new Date().getTime(),\n    num: Math.random()\n  };\n  return db.insert(doc);\n};\n\nexports.main = main;\n```\n\n- there is a global variable called `db` which is to hold the Cloudant library object\n- when the `main` function is called for the first time, the `db` object is created with the Cloudant configuration. An `if` statement ensures that the `db` object is only created once.\n- on the first write to the database, the Cloudant library will first exchange its credentials for a cookie, storing it in the `db` object.\n- subsequent invocations that reuse the same container will re-use the cookie inside `db`, and if consequitive innvocations are close enough in time, they will also reuse the same _HTTP_ connection, because the socket is [kept alive](https://en.wikipedia.org/wiki/Keepalive).\n\nThe sample principle can be applied to code using the [iam](https://github.com/cloudant/nodejs-cloudant#the-plugins) plugin, IBM Cloud's Identity and Access Management system for authentication.\n\n## I thought global variables were bad?\n\nThey are in general, but in this case they allow state to be retained between invocations of our Cloud Function, making our application faster and saving us money!\n",
    "url": "/2018/06/14/Optimising-Cloudant-for-Serverless.html",
    "tags": "Serverless",
    "id": "40"
  },
  {
    "title": "Cloudant Fundamentals 7/10",
    "description": "Querying",
    "content": "\n\n\nIn previous posts we've looked add adding and retrieving documents from a Cloudant database by their key fields - the `_id` field. There's a good chance that you want your database to be able to do more than that which is where querying comes in.\n\n## Making a query\n\nA [Cloudant Query](https://console.bluemix.net/docs/services/Cloudant/api/cloudant_query.html#query) allows questions to be asked of your Cloudant data, questions such as:\n\n- get me all the documents where the `dob` field is less than `1970-01-01` \n- get me all the documents where the `dob` field is less than `1970-01-01` and the `actor` field is `Marlon Brando`\n- get the first fifty films staring Matthew Broderick in date order\n- get the next 50 films matching the previous query\n\nQueries are expressed as JSON documents such as:\n\n```js\n{\n  \"selector\": {\n    \"dob\": {\n      \"$lt\": \"1970-01-01\"\n    }\n  }\n}\n```\n\nThe `selector` object is the equivalent of the \"WHERE\" part of relational database query. It defines the values or ranges of fields that you are looking for - in this case, the `$lt` (\"less than\") operator is used to perform our first query ([other operators are available](https://console.bluemix.net/docs/services/Cloudant/api/cloudant_query.html#operators))\n\nTo perform the query we simply POST the JSON to the `/db/_find` endpoint:\n\n```sh\n$ curl -X POST \\\n    -H'Content-type:application/json' \\\n    -d@query.json \\\n    \"$URL/newdb/_find\"\n```\n\nThe returned data will have the following form:\n\n```js\n{\n  \"docs\":[ ],\n  \"bookmark\": \"\",\n  \"warning\": \"no matching index found, create an index to optimize query time\"\n}\n```\n\n- `docs` is an array of matching documents\n- `bookmark` unlocks access to the [next page of matches](https://console.bluemix.net/docs/services/Cloudant/api/cloudant_query.html#pagination)\n- `warning` is cautioning us that we are performing a query which foreces Cloudant to scan the whole database to answer. We can improve performance with an [index](https://console.bluemix.net/docs/services/Cloudant/api/cloudant_query.html#creating-an-index), a feature we'll meet later.\n\n## More complex clauses\n\nOur second query needs to use the `$and` operator which is fed an array of clauses. The first clause is the same as our first query and we add on a second clause to match by actor name.\n\n```js\n{\n  \"selector\": {\n    \"$and\": [\n      {\n        \"dob\": {\n          \"$lt\": \"1970-01-01\"\n        }\n      },\n      {\n        \"actor\": \"Marlon Brando\"\n      }\n    ]\n  }\n}\n```\n\nOnly documents matching all of the `$and` clauses will make it to the result set. \n\n## Sorting\n\nThe third query adds a `sort` attribute to the query object:\n\n\n```js\n{\n  \"selector\": {\n    \"actor\": \"Matthew Broderick\"\n  }\n  \"sort\": {\n    \"date\": \"asc\"\n  }\n}\n```\n\nWe can [sort](https://console.bluemix.net/docs/services/Cloudant/api/cloudant_query.html#sort-syntax) by one or more fields in ascending (asc) or descending (desc) order.\n\n## Next time\n\nIn the next post we'll do all this again, but programmatically in Node.js and  introduce the prospect of expressing our queries in SQL.\n\n",
    "url": "/2018/06/18/Cloudant-Fundamentals-Querying.html",
    "tags": "Fundamentals Query",
    "id": "41"
  },
  {
    "title": "Cloudant Fundamentals 8/10",
    "description": "Querying in Node.js",
    "content": "\n\n\n\nIn the previous part of this series we discovered the `_find` endpoint which allows us to formulate queries in JSON and ask Cloudant to answer them.\n\nIn this post, we'll look to doing the same thing but using Node.js code. Again we'll lean on the [cloudant-quickstart](https://www.npmjs.com/package/cloudant-quickstart) library.\n\n## Making queries\n\nUsing the query we made last time, we can pass the `selector` directly to the  `query` function of *cloudant-quickstart* object to get an array of matching documents back.\n\n```js\nvar q = {\n  \"dob\": {\n    \"$lt\": \"1970-01-01\"\n  }\n}\n\ndb.query(q).then(console.log)\n// outputs an array of documents\n```\n\nWe can do the same queries with additional clauses: \n\n```js\nvar q = {\n  \"$and\": [\n    {\n      \"dob\": {\n        \"$lt\": \"1970-01-01\"\n      }\n    },\n    {\n      \"actor\": \"Marlon Brando\"\n    }\n  ]\n}\ndb.query(q).then(console.log)\n// outputs an array of documents\n```\n\n## Sorting\n\nIf we need to sort, we can pass the sort options in as a second parameter:\n\n```js\nvar q = {\n  \"$and\": [\n    {\n      \"dob\": {\n        \"$lt\": \"1970-01-01\"\n      }\n    },\n    {\n      \"actor\": \"Marlon Brando\"\n    }\n  ]\n}\ndb.query(q, { sort: { 'date': 'asc'} }).then(console.log)\n// outputs an a sorted array of documents\n```\n\n## SQL too\n\nIf you prefer to express your queries in Structured Query Language (SQL), then the *cloudant-quickstart* library can understand that too:\n\n```js\nvar q = \"SELECT * from newdb WHERE dob < '1970-01-01' AND actor = 'Marlon Brando' SORY BY date\"\ndb.query(q).then(console.log)\n```\n\nThe *cloudant-quickstart* converts your SQL into the equivalent Cloudant Query object for you. You can see the object by calling the `explain` function:\n\n```js\nconsole.log(db.explain(q))\n// { selector: { \"$and\": [ { \"dob\":  { \"$lt\": \"1970-01-01\" },  { \"actor\": \"Marlon Brando\" } ] }, sort: { 'date': 'asc'} }\n```\n\nTo be clear, this SQL-to-query conversion only works for non-aggregating SELECT queries, but it is useful way to express your query and to understand what the equivalent Cloudant Query looks like.\n\n## Next time\n\nQuerying is only one half of the story. In order to have your queries run efficiently, your database needs its data *indexing* too. We'll see what that means in the next installment.\n\n",
    "url": "/2018/06/25/Cloudant-Fundamentals-Querying-in-Nodejs.html",
    "tags": "Fundamentals Query",
    "id": "42"
  },
  {
    "title": "Modelling with TypeScript",
    "description": "Making TypeScript objects to store in Cloudant",
    "content": "\n\n\n[TypeScript](https://www.typescriptlang.org/) is a programming language that is a super-set of JavaScript written by Microsoft. TypeScript code compiles to various flavours of JavaScript to run in the browser, in Node.js or in concert with frameworks such as React, Angular, Vue.js etc. Programming in TypeScript brings you the luxury of:\n\n- type checking for function parameters, return values etc\n- default and optional values for function parameters\n- interfaces, for defining the shape of objects being passed in and out of functions\n- classes with constructors, inheritance & access modifiers\n- enumerations\n- iterators\n- module import/export \n- and lots more\n\nUltimately your TypeScript code is transpiled into JavaScript, targetted for your destination platform, so it can't _do_ any more than JavaScript. The advantage of TypeScript code is that you get to apply greater rigour in your code, have access to lots of \"grown up\" programming features and a receive lots of help in the code editor. \n\n![typescript0](/img/typescript0.png)\n\nIn the above example, the red line indicates that there is no 'postcode' property of the `Address` class. Catching mistakes like this in the editor saves lots of time later, chasing down errors and figuring out why the code isn't doing what is expected. \n\nTypescript is [open-sourced](https://github.com/Microsoft/TypeScript) and Microsoft's popular [VS Code](https://code.visualstudio.com/) editor is itself written in TypeScript!\n\n## Data modelling with Cloudant\n\nLet's say we want to store employee records in Cloudant. As Cloudant stores JSON objects, we can use plain JavaScript objects in our code that are transformed to JSON with `JSON.stringify`:\n\n```js\nvar obj = {\n  name: \"Glynn\",\n  joined: 1529060008135,\n  address: {\n    street: \"15 Front Street\",\n    city: \"Coketown\",\n    state: \"Yorkshire\",\n    zip: \"N1 2BX\",\n    lat: 52.1,\n    long: -1.5\n  },\n  employeeCode: \"101-5523\",\n  tags: [\"tech\",\"uk\"]\n}\n```\n\nTo take advantage of the features of TypeScript, it would be better if we could use classes instead of generic objects. We could build:\n\n- a Date object to represent the time the employee joined.\n- an Address object with custom methods that formats address labels.\n- a TagsCollection object with methods that allow new tags to be added and prevents duplicates.\n\n![modelling]({{< param \"image\" >}})\n\nIdeally, an *Employee* class containing these building-block classes would have a usuable JSON representation that could be stored in Cloudant. In addtion, the JSON string could be returned to a concrete class after retrieving data from the database.\n\nThere is a solution to this, so let's start with a custom *Address* class.\n\n## Address class\n\nWe can create a custom TypeScript class that embodies a postal address and geo-location:\n\n```js\n// Address.ts\n\n// the iAddress Interface - the shape of an address object\nexport interface iAddress {\n  street: string\n  town: string\n  state: string\n  zip: string\n  lat: number\n  long: number\n}\n\n// the Address class\nexport class Address implements iAddress {\n  type: string\n  street: string\n  town: string\n  state: string\n  zip: string\n  lat: number\n  long: number\n\n  constructor() {\n    this.type = 'Address'\n  }\n\n  // return an address label string\n  getLabel() {\n    return [this.street, this.town, this.state, this.zip].join(', ')\n  }\n\n  // turn an object into an Address\n  static fromObject(a: iAddress): Address {\n    let obj = new Address()\n    Object.assign(obj, a)\n    return obj\n  }\n}\n```\n\nIt's worth explaining that [TypeScript Interfaces](https://www.typescriptlang.org/docs/handbook/interfaces.html) define the *forms of objects* - the attributes and types that make up an object. They are useful when defining functions that take objects as parameters - this allows the TypeScript compiler to check the form of the object at compile-time and allows your editor to check your code as you type.\n\nThis *Address* object is pretty simple. It has a number of public attributes that define the address, a `getLabel` function that returns a postage label and a static `fromObject` function that can resurrect an Address object from a generic object that conforms to the _iAddress_ interface.\n\nA simple class like *Address* that only uses JavaScript primitive types turns into a sensible form of JSON - an object which looks like the _iAddress_ interface: \n\n![typescript1](/img/typescript1.png)\n\nWe'll see how to incorporate this into our *Employee* class later, but first let's look at how we can use a similar technique to model a date.\n\n## The CloudantDate class\n\nIn [this blog post](https://medium.com/@glynn_bird/date-formats-for-apache-couchdb-and-cloudant-1c017b7b878b), I discussed various ways of representing a date/time in Cloudant JSON. I concluded that it's good to have the constituent date/time parts (day, month, year) present in the object to allow [Cloudant Query](https://console.bluemix.net/docs/services/Cloudant/api/cloudant_query.html#query) to access them and a \"seconds since 1970\" integer or an ISO-8601 string for sorting purposes. \n\nTo represent a [JavaScript Date](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Date) object in our *Employee* class, we're going to create a wrapper class called *CloudantDate* which has a trick up its sleeve:\n\n```js\n// CloudantDate.ts\n\n// the iCloudantDate interface - the shape the object \nexport interface iCloudantDate {\n  type: string,\n  year: number,\n  month: number,\n  day: number,\n  hour: number,\n  minute: number,\n  second: number,\n  millisecond: number,\n  ts: number\n}\n\n// the CloudantDate class\nexport class CloudantDate {\n  d: Date\n\n  // if a date is supplied, it is used, otherwise 'now' is used\n  constructor(c?: any) {\n    this.d = c ? new Date(c) : new Date()\n  }\n\n  // override toJSON to provide a custon JSON representation\n  toJSON(): iCloudantDate {\n    return {\n      type: 'CloudantDate',\n      year: this.d.getUTCFullYear(),\n      month: this.d.getUTCMonth() + 1,\n      day: this.d.getUTCDate(),\n      hour: this.d.getUTCHours(),\n      minute: this.d.getUTCMinutes(),\n      second: this.d.getUTCSeconds(),\n      millisecond: this.d.getUTCMilliseconds(),\n      ts: this.d.getTime()\n    }\n  }\n\n  // convert an object to a CloudantDate (only the 'ts' is used)\n  static fromObject(o: iCloudantDate): CloudantDate {\n    return new CloudantDate(o.ts)\n  }\n}\n```\n\nThe *CloudantDate* class stores its value in a standard JavaScript *Date* class and like *Address*, implements a static `fromObject` function to allow a class to be reconstituted from an plain object. *CloudantDate*'s real party piece is overridng the `toJSON` function, which is used by `JSON.stringify` to turn an instance of the class into JSON.\n\nIf we create a *CloudantDate* object and `JSON.stringify` it, the resultant string contains all the date and time pieces broken out:\n\n![typescript1](/img/typescript2.png)\n\nThis allows us to have dates that are useable `Date` classes in our code and have the constituent parts broken down in the Cloudant database, where they are accessible to the Cloudant indexing engine.\n\n## TagCollection class\n\nOur final storage class is *TagCollection* which stores an array of strings. It has a couple of helper functions (`add` and `remove`) and also overrides the `toJSON` function to ensure that only an array of strings appears in the JSON:\n\n```js\n// TagCollection.ts\n\n// TagCollection class\nexport class TagCollection {\n  tags: string[]\n\n  constructor() {\n    this.clear()\n  }\n\n  // wipe all tags\n  clear() {\n    this.tags = []\n  }\n\n  // add a tag (with de-dupe)\n  add(s: string):boolean {\n    if (this.tags.indexOf(s) === -1) {\n      this.tags.push(s)\n      return true\n    }\n    return false\n  }\n\n  // remove a tag\n  remove(s: string):boolean {\n    let i = this.tags.indexOf(s)\n    if (i > -1) {\n      this.tags.splice(i, 1)\n      return true\n    }\n    return false\n  }\n\n  // override toJSON to only output the tags array\n  toJSON(): string[] {\n    return this.tags\n  }\n\n  // reconstitue \n  static fromObject(t: string[]): TagCollection {\n    let tc = new TagCollection()\n    tc.tags = t\n    return tc\n  }\n}\n```\n\n![typescript3](/img/typescript3.png)\n\n\n## Assembling the Employee class\n\nNow we have all the pieces in place, we can create a top level \"Employee\" class that uses our custom classes:\n\n```js\n// Employee.ts\n\nimport { Address } from './Address'\nimport { CloudantDate } from './CloudantDate'\nimport { CloudantDocument } from './CloudantDocument'\nimport { TagCollection } from './TagCollection'\n\n// the Employee class\nexport class Employee extends CloudantDocument {\n  name: String\n  employeeCode: String\n  salary: number\n  joined: CloudantDate\n  address: Address\n  tags: TagCollection\n  \n  constructor(name: String, address: Address) {\n    super()\n    this.name = name\n    this.joined = new CloudantDate()\n    this.address = address\n  } \n  \n  // convert plain object to Employee\n  static fromObject(parsed: any): Employee {\n    let obj = new Employee(parsed.name, parsed.address)\n    obj._id = parsed._id\n    obj._rev = parsed._rev\n    obj.joined = CloudantDate.fromObject(parsed.joined)\n    obj.address = Address.fromObject(parsed.address)\n    obj.tags = TagCollection.fromObject(parsed.tags)\n    obj.employeeCode = parsed.employeeCode\n    return obj\n  }\n\n  // parse JSON and then convert to Employee\n  static fromJSON(json: string): Employee {\n    let parsed = JSON.parse(json)\n    return Employee.fromObject(parsed)\n  }\n}\n```\n\nWe have a base class that handles the Cloudant-specific fields (`_id`, `_rev` etc) which we can reuse across all of our Cloudant storage classes:\n\n```\n// CloudantDocument.ts\n\nimport * as uuid from 'uuid'\n\nexport interface CloudantAPIResponse {\n  ok: boolean\n  id: string\n  rev: string\n}\n\nexport class CloudantDocument {\n  _id: string\n  _rev: string\n  _deleted: boolean\n  _attachments: object\n\n  constructor() {\n    this.clear()\n  }\n\n  private clear() {\n    this._id = undefined\n    this._rev = undefined\n    this._deleted = undefined\n    this._attachments = undefined\n  }\n\n  generateId() {\n    this.clear()\n    this._id = uuid.v4()\n  }\n\n  processAPIResponse(response: APIResponse) {\n    if (response.ok === true) {\n      this._id = response.id\n      this._rev = response.rev\n    }\n  }\n}\n```\n\nWe can then use the *Employee* class in our own code and pass instances of *Employee* to the [official Node.js Cloudant library](https://www.npmjs.com/package/@cloudant/cloudant) to be stored in the database:\n\n```js\nimport { Employee } from './Employee'\nimport { Address } from './Address'\nimport { TagCollection } from './TagCollection'\nimport * as Cloudant from '@cloudant/cloudant'\n\n// make an address object\nlet a = new Address()\na.street = '19 Front Street'\na.town = 'Coketown'\na.state = 'Lancashire'\na.zip = 'L1 6GC'\na.lat = 52.56\na.long = -2.7\n\n// create employee\nlet newEmployee = new Employee('Glynn', a)\nnewEmployee.generateId()\nnewEmployee.employeeCode = '101-5523'\n\n\n// tags\nnewEmployee.tags = new TagCollection()\nnewEmployee.tags.add('engineering')\nnewEmployee.tags.add('cloudant')\n\n// initialise Cloudant library\nlet client = Cloudant(process.env.CLOUDANT_URL)\nlet db = client.db.use('employees')\ndb.insert(newEmployee, (err, data) => {\n  if (err) throw new Error(err)\n  newEmployee.processAPIResponse(data)\n})\n```\n\nThe Cloudant library converts the our custom object to JSON, which invokes our custom `toJSON` overrides:\n\n![typescript4](/img/typescript4.png)\n\nThe Cloudant document can be restored into an *Employee* class later, if we use our static `Employee.fromObject` method to reconstitute the class from its JSON form:\n\n```js\nlet client = Cloudant(process.env.CLOUDANT_URL)\nlet db = client.db.use('employees')\nlet e:Employee = null\ndb.get('0ef9a2bd-e738-43bb-9bb6-06385e5ac541', (err, data) => {\n  if (err) throw new Error(err)\n  e = Employee.fromObject(data)\n})\n```\n\n## Typescript conclusions\n\nWriting in TypeScript adds some of the rigour of writing Java code to JavaScript, including strong typing and classes with inheritance & access control. This allows the developer to build neat, consistent code with the code editor advising at every turn. TypeScript is transpiled, whether your target is a minified package for a web app, a server-side Node.js module or a native Electron app. \n\nOverriding the `toJSON` function in your custom TypeScript classes allows you to use classes in your code and pass them to the Cloudant Node.js library to be saved in a form of JSON of your choice. ",
    "url": "/2018/06/29/Modelling-Cloudant-Data-in-TypeScript.html",
    "tags": "TypeScript Modelling",
    "id": "43"
  },
  {
    "title": "Cloudant Fundamentals 9/10",
    "description": "Indexing",
    "content": "\n\n\nIn part 7 of this series, we saw a warning in the search results:\n\n> \"no matching index found, create an index to optimize query time\"\n\nThis is Cloudant's polite way of saying that your query is expensive and the database is having to walk through the whole data set to calculate the answer. In small databases this is not a problem but in a production system, with the data growing all the time, an index is essential.\n\n![indexing]({{< param \"image\" >}})\n\n## What is an index?\n\nAn database index is just like an index in a book or a biblical concordance. It is a sorted data structure that allows quick access to a portion of the data. \n\nOur data looks like this:\n\n```js\n{\n  \"_id\": \"0d960fedfb62499abe35557f2d2c7c5e\",\n  \"name\": \"Ferris Bueller\", \n  \"actor\": \"Matthew Broderick\", \n  \"dob\": \"1962-03-21\"\n}\n```\n\nCloudant automatically creates an index on the `_id` field so that it can retrieve data by `_id`. If we are going to be making lots of queries on the `dob` field, then it might make sense to instruct Cloudant to create a secondary index on that field.\n\nThis is acheived by writing to the [POST /db/_index](https://console.bluemix.net/docs/services/Cloudant/api/cloudant_query.html#creating-an-index) endpoint:\n\n```sh\ncurl -X POST \\\n     -H 'Content-type:application/json' \\\n     -d'{\"index\":{\"fields\":[\"dob\"]}}' \\\n     \"$URL/newdb/_index\"\n```\n\nIn the body of the JSON supplied to the `_index` endpoint, we specify the array of fields that are to be indexed. Cloudant then creates the index on disk (ordered by `dob` in this case), so that it can be used by future queries. \n\nIf we repeat our query, we should see the same results, but without the warning:\n\n```sh\n$ curl -X POST \\\n    -H'Content-type:application/json' \\\n    -d@query.json \\\n    \"$URL/newdb/_find\"\n# { \"docs\":[ ], \"bookmark\": \"\" }\n```  \n\nOur query is now using the index and you should see a performance improvement, especially with larger data sets.\n\n## Index types - json & text\n\nCloudant Query has two types of index - `json` and `text`.\n\n- Indexes of `type='json'` are built using Cloudant's [MapReduce](https://console.bluemix.net/docs/services/Cloudant/api/creating_views.html#views-mapreduce-) technology. This creates single-use indexes in the order of the keys you supply i.e. if you index `actor` and `dob` then the index will be in `actor`/`dob` order and useful for queries that involve selectors on those fields in that order.\n- Indexes of `type='text' are powerered by the Cloudant's [Search](https://console.bluemix.net/docs/services/Cloudant/api/search.html#search) technology. This builds separate \"concordances\" for the fields you supply, making it more universal for the database at query time. (see the [$text operator](https://console.bluemix.net/docs/services/Cloudant/api/cloudant_query.html#selector-syntax) for matching arbitrary strings against blocks of text in your documents).\n\nBoth index types have their subtleties, so make sure you've read [the documentation](https://console.bluemix.net/docs/services/Cloudant/api/cloudant_query.html#query) and understand how it works before going into production!\n\n## Indexing strategies\n\nThe job of an index is to help the database to reduce the volume of data it's working with to a managable size. Imagine you have a query like this for a database of movies:\n\n```js\n\"selector\": {\n    \"$and\": [\n        {\n            \"actor\": {\n                \"$eq\": \"Al Pacino\"\n            }\n        },\n        {\n            \"year\": {\n                \"$eq\": 2010\n            }\n        }\n    ]\n}\n```\n\nWhich field or fields should you create the index on to best serve this query? It depends on whether which is the smaller:\n\n- the number of movies in that database for a given year\n- the number of movies that star an actor\n\nIn this case, I'd be inclined to create an index on `actor` because there may be thousands of movies in a year, but an actor might be credited with only a few dozen movies in their entire career. So an index on `actor` winows the database to a smaller dataset than an index on `year`.\n\nOur index on actor reduces a database of tens of thousands of records down to a few dozen. It is then very simple for the database to apply the second clause of the `$and` to this smaller data set.\n\nOther tips:\n\n- Use the [POST /db/_explain](https://console.bluemix.net/docs/services/Cloudant/api/cloudant_query.html#explain-plans) to see how Cloudant picks an index for a specified query.\n- Cloudant allows you to specify the index you want the database to use at query time. This is good practice as you remove any ambiguity in index selection.\n- consider creating [partial Cloudant indicies](https://medium.com/ibm-watson-data-lab/creating-partial-cloudant-indexes-1ebf169c8e15) if you are only ever interested in querying a subset of your database (i.e. you could eliminate *preliminary* blog posts from your index and only include *published* posts in a partial index).\n- There's always Cloudant's [MapReduce](https://console.bluemix.net/docs/services/Cloudant/api/creating_views.html#views-mapreduce-), [Search](https://console.bluemix.net/docs/services/Cloudant/api/search.html#search) and [Geospatial](https://console.bluemix.net/docs/services/Cloudant/api/cloudant-geo.html#cloudant-nosql-db-geospatial) indexes for a lower-level querying experience.\n- Not all queries are \"Cloudant-shaped\". If you are building leader boards or  counting distinct values, you might want to consider [rolling your own custom index](https://medium.com/ibm-watson-data-lab/custom-indexers-for-cloudant-6b7e65186db1) outside of Cloudant.\n\n## Next time\n\nIn the final part of this series, we'll look at performing aggregations using Cloudant.",
    "url": "/2018/07/12/CloudantFundamentals-Indexing.html",
    "tags": "Fundamentals Indexing",
    "id": "44"
  },
  {
    "title": "Conflicts",
    "description": "How to deal with conflicts in Cloudant documents",
    "content": "\n\n\nConflicts occur in Cloudant and Apache CouchDB databases when the same document is written to in different ways in separate copies of the database. This can occur when there is:\n\n- a mobile app and a server-side replica and the data is replicated between them.\n- two databases that are replicated together.\n- a single, multi-node database that receives mulitple writes to the same document at the same time. Although the database will try to prevent conflicts happening in this circumstance by returning an `HTTP 409` response, conflicts may still arise in some circumstancces.\n\nConflicts are not an error condition - they are symptom of the database ensuring that you don't lose data when the same document is modified in different ways by different actors. Sometimes conflicts occur by accident - perhaps some automated process updating the database more frequently than normal. Ideally your code should adopt a design pattern that reduces or eliminates the chance of conflicts occurring, but if conflicts arise your code will need to have an algorithm to fix them.\n\n![conflict]({{< param \"image\" >}})\n\nConflicted documents can be a headache for the performance of the database, even if you make the effort to resolve conflicts as you find them. In some cases, documents can have hundreds or thousands of conflicted revisions. As the database has to keep the bodies of the conflicted documents and revision histories for conflicted branches, conflicted documents eat up storage and make the revision tree costly to navigate when updating or deleting revisions.\n\n## Detecting conflicts\n\nYou can see if a document is conflicted by fetching the document with `?conflicts=true` appended to the URL:\n\n```sh\ncurl http://localhost:5984/mydb/mydoc?conflicts=true\n{\"_id\":\"mydoc\",\"_rev\":\"1-99\",\"a\":98,\"_conflicts\":[\"1-98\",\"1-97\",\"1-96\",\"1-95\"]}\n```\n\nAnother API option is to use `?meta=true` which returns further metadata about your document:\n\n```js\n{\n  \"_id\": \"c956ef1ed08dd48b48ea4b9809665b4f\",\n  \"_rev\": \"2-8a759d1f5a1537bcf775ab7bc947b377\",\n  \"a\": 1,\n  \"b\": 3,\n  \"_revs_info\": [\n    {\n      \"rev\": \"2-8a759d1f5a1537bcf775ab7bc947b377\",\n      \"status\": \"available\"\n    },\n    {\n      \"rev\": \"1-25f9b97d75a648d1fcd23f0a73d2776e\",\n      \"status\": \"available\"\n    }\n  ],\n  \"_deleted_conflicts\": [\n    \"3-f76e0de0cabef8da918d5b747493631a\"\n  ]\n}\n```\n\n## Resolving conflicts by hand\n\nIn the first example, the document has a winning revision `1-99` and four other revisions that are non-winning. If I delete the conflicting revisions the document returns to its normal state.\n\n```sh\ncurl -X DELETE http://localhost:5984/mydb/mydoc?rev=1-98\ncurl -X DELETE http://localhost:5984/mydb/mydoc?rev=1-97\ncurl -X DELETE http://localhost:5984/mydb/mydoc?rev=1-96\ncurl -X DELETE http://localhost:5984/mydb/mydoc?rev=1-95\n```\n\nI don't *have to* delete the conflicted revisions - I could instead have chosen to retain revision `1-96`. Simply deleting the winning revision and the other conflicts would promote `1-96` to be the winner. I can even delete ALL of the revisions and propose a new winner (perhaps a merge of all the conflicted documents). The resolution of a conflict is simply deleting the revisions you don't want and optional creation of a new winner. The [_bulk_docs](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-documents#updating-documents-in-bulk) endpoint can be used to make several modifications to a single document in a single API call.\n\nThis is simple enough when there are only a handful of conflicts but if there are hundreds or thousands, some tooling would help.\n\n## Resolving conflicts with couchdeconflict\n\nI wrote a [simple command-line utility to help clean up CouchDB/Cloudant documents that have become conflicted](https://www.npmjs.com/package/couchdeconflict). It is installed with\n\n```sh\nnpm install -g couchdeconflict\n```\n\nand run by specifying the URL of the document to work on:\n\n```sh\n> couchdeconflict -u http://localhost:5984/mydb/mydoc\noptions: {\"url\":\"http://localhost:5984/mydb/mydoc\",\"keep\":null,\"batch\":100}\nFetching document\n217 conflicts\n  [==================================================] 100% 217/217           \n217 conflicts deleted\n```\n\nBy default, the pre-existing winning revision is retained and the conflicted revisions are deleted. You can nominate a new winning revision with the `--keep` parameter:\n\n```sh\n> couchdeconflict --url http://localhost:5984/mydb/mydoc  --keep 1-111\ncouchdeconflict\n",
    "url": "/2018/07/25/Removing-Conflicts.html",
    "tags": "Conflicts",
    "id": "45"
  },
  {
    "title": "Cloudant Fundamentals 10/10",
    "description": "Aggregation",
    "content": "\n\n\nIt's been an emotional journey through Cloudant's fundamentals but we're nearly at the end. In this final post, we'll discuss data aggregation: counting, summing and statistics.\n\n[Cloudant Query](https://console.bluemix.net/docs/services/Cloudant/api/cloudant_query.html#query) which we used for [part 8]() and [part 9]() of this series does not have the ability to perform aggregations, only selection. i.e. you can do `SELECT * FROM mydb WHERE actor='Al Pacino'` but not `SELECT COUNT(*) FROM mydb WHERE actor='Al Pacino'`.\n\n![wood]({{< param \"image\" >}})\n> Photo by [Sven Scheuermeier on Unsplash](https://unsplash.com/photos/jnU5EivNygE)\n\n## MapReduce\n\nOne option is to write your own [MapReduce](https://console.bluemix.net/docs/services/Cloudant/api/creating_views.html#views-mapreduce-) view. A view is defined as a JavaScript function. Cloudant takes each document in the database, passing it to this function in turn and then storing the emitted  key/value pairs in an index on disk.\n\nTo calculate counts of documents by actor we would create a map function like this:\n\n```js\nfunction(doc) {\n  emit(doc.actor, null)\n}\n```\n\nThis function paired with the `_count` reducer produces counts of each value of actor. The other built-in reducers `_stats` and `sum` can be used calculate statistics on the second field emitted by your map function:\n\n \n```js\nfunction(doc) {\n  emit([doc.year, doc.month], doc.orderValue)\n}\n```\n\nThe above example allows a hierarchical key of `year` and `month` to be used to group the aggregation of the `orderValue` field - ideal for reports and dashboards.\n\n## Facet counts\n\nAnother way of calculating simple count aggregations is using the [faceting feature](https://console.bluemix.net/docs/services/Cloudant/api/search.html#faceting) of Cloudant Search. Like MapReduce, Cloudant Search indexes are configured in JavaScript, this time with an `index` function instead of `emit`:\n\n```js\nfunction(doc) {\n  index(\"category\", doc.category, {\"facet\": true});\n  index(\"price\", doc.price, {\"facet\": true});\n}\n```\n\nThe above example indexes two fields (`category` and `price`) from our document and instructs both to be \"faceted\". This means that values can be counted at query time by supplying a `counts` parameter:\n\n```\n...?q=*:*&counts=[\"category\"]\n{\n    \"total_rows\":100000,\n    \"bookmark\":\"g...\",\n    \"rows\":[...],\n    \"counts\":{\n       \"category\":{\n         \"action\": 55,\n         \"comedy\": 98,\n         \"documentary\": 3\n       }\n    }\n}\n```\n\nSee the [documentation](https://console.bluemix.net/docs/services/Cloudant/api/search.html#faceting) for further examples of facet counts and range faceting.\n\n## Aggregation with cloudant-quickstart\n\nThe simplest way to do aggregation is using the [cloudant-quickstart](https://www.npmjs.com/package/cloudant-quickstart) library.\n\nCounts are performed without defining the index yourself:\n\n```js\n // count documents by year and month\n db.count(['year','month]),then(console.log)\n```\n\nSimply specify the field or array of fields you want to group by and the library will create the appropriate MapReduce view for you.\n\nThe same applies to `stats` and `sum` aggregations\n\n```js\n// get sum of sales grouped by year/month/day\ndb.sum('sales', ['year', 'month', 'day'])\n\n// get stats on weather by state/city\ndb.stats(['temperature', 'airPressure', 'windSpeed'], 'state')\n```\n\n## Conclusion\n\nIn this Cloudant Fundamentals series we've touched on schema design, unique ids, revision tokens, CRUD operations, querying and aggregation. But there's a lot more.\n\n\nThere’s a project called [PouchDB](https://pouchdb.com/) that might strike your fancy. You also can [search our Medium publication](https://medium.com/ibm-watson-data-lab/search?q=cloudant) for more articles on Cloudant and CouchDB. And of course, you can reach the wider open source community on the [Apache CouchDB chat channels](http://couchdb.apache.org/#chat) or get involved via the [CouchDB mailing lists](http://couchdb.apache.org/#mailing-lists). I’ll see you on there!\n\n\n",
    "url": "/2018/08/06/Cloudant-Fundamentals-Aggregation.html",
    "tags": "Fundamentals Aggregation",
    "id": "46"
  },
  {
    "title": "Searching Jekyll Sites",
    "description": "Adding seach facility to a static website",
    "content": "\n\n\nI'm a big fan of [Jekyll](https://jekyllrb.com/) for building static websites. If you're not familiar with Jekyll, it takes a collection of configuration, templates and source files (I write my posts in [Markdown](https://en.wikipedia.org/wiki/Markdown)) and transforms them into static HTML files that can be delivered to the world by any web server. Jekyll is built into [GitHub Pages](https://pages.github.com/) so that you can host the source files for your website or blog in a Git repository and have the resultant static web site served out by GitHub Pages without having to manage any server infrastructure yourself. As of May 2018, [GitHub Pages now supports HTTPS on your custom domains](https://blog.github.com/2018-05-01-github-pages-custom-domains-https/).\n\nStatic sites are fast and easy to manage but without any dynamic server-side components, they may leave your users without features they expect, such as *search*. In a Wordpress-style blog, the content is served out from a MySQL database and site search is powered by querying that data set.\n\n![search]({{< param \"image\" >}})\n> Photo by [Clem Onojeghuo on Unsplash](https://unsplash.com/photos/QBvtgLdmTbQ)\n\nOn a static site, how can you allow your users to search the titles, tags and content of your blog if the data doesn't reside in a database and there's no server-side layer that can render dynamic pages? Here's how it can be done:\n\n![search](/img/atom00.png)\n\n1. Create a static website with Jekyll and serve it out on GitHub Pages, or another static site hosting service.\n2. Write some code to poll the site's Atom feed. A serverless platform like IBM Cloud Functions can be used to run the code periodically.\n3. Write the Atom Feed meta data into an IBM Cloudant database that has a free-text search index configured.\n4. Query the Cloudant database directly from the web page whenever a search is to be performed. \n\nLet's dive into the detail.\n\n## Building a blog with Jekyll\n\nThere are plenty of guides that show you [how to build a Jekyll-powered blog on GitHub Pages](https://www.smashingmagazine.com/2014/08/build-blog-jekyll-github-pages/) or follow the Jekyll documentation's [Quick Start Guide](https://jekyllrb.com/docs/quickstart/).\n\nOnce your blog is setup, make sure it has an Atom feed published at the `/feed.xml` endpoint. This is powered by the [jekyll-feed](https://github.com/jekyll/jekyll-feed) plugin.\n\n## Schema design\n\nIn order to add a search tool to your static blog we're first going to need a database of blog post meta data. Using [Cloudant](https://www.ibm.com/cloud/cloudant) as the database, we can store one JSON document per blog post like this:\n\n```js\n{\n  \"_id\": \"3d8d5d582576d35c984ba7b328190f72\",\n  \"_rev\": \"1-57b755340a3b760b5cef4f3018f0d9fb\",\n  \"title\": \"Cloudant replication with couchreplicate\",\n  \"description\": \"One of Apache CouchDB™’s killer features is replication...\",\n  \"date\": \"2018-02-22T09:00:00.000Z\",\n  \"link\": \"http://myblog.com/2018/02/22/Cloudant-Replication-with-couchreplicate.html\",\n  \"author\": \"Glynn Bird\",\n  \"image\": \"http://myblog.com/assets/img/replication-screenshot.png\",\n  \"tags\": [\n    \"Replication\",\n    \"CLI\"\n  ]\n}\n```\n\nAll of this data can be gleaned from the blog's `feed.xml` Atom feed with two exceptions:\n\n- the [_id field](https://medium.com/ibm-watson-data-lab/cloudant-fundamentals-the-id-f6c7c88fbc75) needs to be unique - we can use a hash of the URL of the blog post.\n- the [_rev field](https://medium.com/ibm-watson-data-lab/cloudant-fundamentals-the-rev-token-fb0fc19a3145) is generated by the database and indicates the revision of the document.\n\nFirst [sign up](https://console.bluemix.net/catalog/services/cloudant-nosql-db) for a Cloudant service and log into the dashboard. Create a new database called `blog`.\n\n![create database](/img/atom1.png)\n\nIn that database we need to define a [Cloudant Search](https://console.bluemix.net/docs/services/Cloudant/api/search.html#search) index to answer free-text queries. Choose _New Search Index_ from the menu next to \"Design Documents\":\n\n![create database](/img/atom2.png)\n\nThen we can define an index by creating a JavaScript function that is executed for every document in the database, calling `index` for every value that is to be searchable:\n\n![create database](/img/atom3.png)\n\n```js\nfunction (doc) {\n  index(\"title\", doc.title, { store: true});\n  index(\"tags\", doc.tags.join(\" \"));\n  index(\"description\", doc.description, {store: false});\n  index(\"date\", doc.date, {store: true});\n  index(\"image\", doc.image, {store: true, index: false});\n  index(\"link\", doc.link, {store: true, index:false});\n}\n```\n\nThe index function takes three parameters:\n\n1. The name of field to be stored in the index e.g. `\"title\"`.\n2. The value to be indexed e.g. `doc.title`.\n3. An options object. When `store` is set to true, a copy of the value is stored unaltered in the index for retrieval at query-time. When `index` is set to `false` the value is not indexed for search, but _is_ reproduced in the search results. \n\n## CORS and effect\n\nIf we want to be able to query our Cloudant database directly from a web page, we need to make two further tweaks to the Cloudant configuration.\n\nFirstly we must enable CORS (Cross-Origin Resource Sharing) in the Cloudant dashboard:\n\n![create database](/img/atom4.png)\n\nEnabling CORS instructs Cloudant to output the HTTP headers that will allow an in-page web request (sometimes called an AJAX request) to proceed without an error. By default, the rules-of-the-road for the web wouldn't allow a web page to fetch JSON from a different domain name, and CORS is the work-around.\n\nSecondly, we need to make the database readable. You can either make the database _world readable_ (grant `_reader` access to everyone) or create an API Key that grants `_reader` access to our database of blog post meta data. Both options are accessible from the \"Permissions\" panel in the Cloudant dashboard:\n\n![create database](/img/atom5.png)\n\nNow our database is created and set up, we need a script to poll the blog's Atom feed, convert it to JSON and write it to the Cloudant database.\n\n## Atom feed poller\n\nWe can write a simple Node.js script to fetch the Atom feed using a handful of npm modules:\n\n- [@cloudant/cloudant](https://www.npmjs.com/package/@cloudant/cloudant) - to interface with the Cloudant database.\n- [request](https://www.npmjs.com/package/request) to fetch the Atom XML,\n- [feedparser](https://www.npmjs.com/package/feedparser) to parse the Atom XML.\n- [striptags](https://www.npmjs.com/package/striptags) to remove HTML tags from the content.\n\nThe code itself then becomes pretty simple:\n\n```js\nconst FeedParser = require('feedparser')\nconst request = require('request')\nconst striptags = require('striptags')\nconst crypto = require('crypto')\nconst cloudant = require('@cloudant/cloudant')\n\n// create an MD5 hash of the supplied string\nconst md5 = function(string) {\n  return crypto.createHash('md5').update(string).digest('hex')\n}\n\n// poll an RSS feed, return an array of items\nconst poll = function(url) {\n  return new Promise((resolve, reject) => {\n    var req = request(url)\n    var feedparser = new FeedParser()\n    var items = []     \n     \n    req.on('error', function (error) {\n      // handle any request errors\n    });\n     \n    req.on('response', function (res) {\n      var stream = this; // `this` is `req`, which is a stream\n     \n      if (res.statusCode !== 200) {\n        this.emit('error', new Error('Bad status code'))\n      } else {\n        stream.pipe(feedparser);\n      }\n    });\n     \n    feedparser.on('error', function (error) {\n      reject(error)\n    });\n     \n    feedparser.on('readable', function () {\n      // This is where the action is!\n      var stream = this // `this` is `feedparser`, which is a stream\n      var item = null\n\n      while (item = stream.read()) {\n        items.push(item)\n      }\n    });\n\n    feedparser.on('end', function() {\n      var newitems = []\n      for (var i in items) {\n        var item = items[i]\n        var newitem = {}\n        newitem._id = md5(item.link)\n        newitem.title = item.title\n        newitem.description = striptags(item.description)\n        newitem.date = item.date\n        newitem.link = item.link\n        newitem.author = item.author\n        newitem.image = item.image.url\n        newitem.tags = item.categories\n        newitems.push(newitem)\n      }\n      resolve(newitems)\n    })\n  });\n}\n\nconst main = function(opts) {\n  return poll(opts.BLOGURL).then((items) => {\n    var db = cloudant({account: opts.ACCOUNT, password: opts.PASSWORD, plugins: ['promises']}).db.use(opts.DBNAME)\n    return db.bulk({docs: items})\n  })\n}\n\nexports.main = main\n```\n\nThe `main` function is passed an object with the following attributes:\n\n- `BLOGURL` - the URL of the blog's Atom feed.\n- `ACCOUNT` - the admin username of the Cloudant service.\n- `PASSWORD` - the admin password of the Cloudant service.\n- `DBNAME` - the name of the Cloudant database to write to.\n\nWe can deploy this code to [IBM Cloud Functions](https://console.bluemix.net/openwhisk/) using the `bx wsk` tool (substituting your Cloudant account, password and blog URL):\n\n```sh\n# create a package called cloudantblog containing the config\nbx wsk package update cloudantblog \\ \n  --param ACCOUNT \"myaccount\" \\\n  --param PASSWORD \"mypassword\" \\\n  --param DBNAME \"blog\" \\\n  --param BLOGURL \"http://myblog.com/feed.xml\" \n  \n# add an action into the package\nzip -r poll.zip index.js node_modules\nbx wsk action update cloudantblog/poll --kind nodejs:8 poll.zip\n\n# run periodically - every fifteen minutes\nbx wsk trigger create cloudantblog_trigger --feed /whisk.system/alarms/alarm --param cron \"*/15 * * * *\"\nbx wsk rule update cloudantblog_rule cloudantblog_trigger cloudantblog/poll\n```\n\nIBM Cloud Functions now has your polling code and is invoking it every 15 minutes. The script fetches your blog's Atom feed turns it into JSON ready to be inserterd into the Cloudant database and then writes all the records in a single bulk request. It manages to deduplicate the listings because it uses a hash of the document's URL as the document id - Cloudant won't accept two documents with the same `_id` so duplicates are rejected.\n\n## Performing searches\n\nOur database should contain some documents. Let's see them by querying our database's [\\_all_docs](https://console.bluemix.net/docs/services/Cloudant/api/database.html#get-documents) endpoint:\n\n```sh\nexport COUCH_URL=\"https://USER:PASS@HOST.cloudant.com/blog\"\ncurl \"$COUCH_URL/_all_docs?include_docs=true\"\n# {\"rows\":[...])\n```\n\nYou should see a handful of documents.\n\nNow we can query the search index we created earlier:\n\n```sh\ncurl \"$COUCH_URL/_design/search/_search/search?q=*:*\"\n{  \n  \"total_rows\": 10,\n  \"bookmark\": \"\",\n  \"rows\": [\n    {\n      \"id\": \"4281157a51e3f0c16fea1596fa10713e\",\n      \"order\": [\n        1,\n        0\n      ],\n      \"fields\": {\n        \"image\": \"http://localhost:4000/assets/img/kristina-tripkovic.jpg\",\n        \"date\": \"2018-04-27T08:00:00.000Z\",\n        \"link\": \"http://localhost:4000/2018/04/27/Cloudant-Fundamentals-1.html\",\n        \"title\": \"Cloudant Fundamentals 1/10\"\n      }\n    }\n    ...\n  ]\n}\n```\n\nThe the `q=*:*` matches every indexed record. The array of `rows` returned contains a `fields` object containing each item indexed with `store: true` during the indexing process.\n\nImagine we wish to answer a user query for documents matching the search phrase \"red apples\", then we can construct a Cloudant Search query to look for \"red apples\" in the description field:\n\n```\ndescription:'red apples'\n```\n\nA better search for this use-case is this:\n\n```\ntitle:'red apples'^100 OR tags:'red apples'^10 OR description:'red apples'\n```\n\nThe above query matches the `title`, `tags` or `description` fields against the query string, but attaches greater weight to title matches, than tags or description matches. This use of the `^` operator weights the search results to bring more relevant documents to the top of the results.\n\nWe can send this query to Cloudant using `curl`:\n\n```sh\ncurl \"$COUCH_URL/_design/search/_search/search?q=title:'red+apples'^100+OR+tags:'red+apples'^10+OR+description:'red+apples'\"\n```\n\n## Querying from the front end\n\nThe final piece of the puzzle is making the search request from inside a web page. Here there are myriad options:\n\n- [XMLHttpRequest](https://developer.mozilla.org/en-US/docs/Web/API/XMLHttpRequest)\n- [jQuery.ajax()](http://api.jquery.com/jquery.ajax/)\n- [SuperAgent](https://visionmedia.github.io/superagent/)\n- [Fetch](https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API)\n\nLet's use `fetch` because it's new and shiny and it couldn't be easier:\n\n```js\n// search string looking for 'red apples' in title/tags/description\nvar url = \"https://myhost.cloudant.com/blog/_design/search/_search/search?q=title:'red+apples'^100+OR+tags:'red+apples'^10+OR+description:'red+apples'\"\n\n// fetch the url\nfetch(url).then((response) => { \n  return response.json() \n}).then((json) => {\n  console.log(json)\n})\n\n// {\"total_rows\":10,\"bookmark\":\"xxx\",\"rows\":[...]}\n```\n\nAll that remains is to loop over the returned JSON's `rows` attribute to pick out and render the data. How you do that depends on your front-end stack. I like the way [Vue.js](https://vuejs.org/) manages the plumbing between your JavaScript \"model\" and your HTML \"view\". There are thousand other ways of building a dynamic page using other frameworks (React, Angular, Ember et al) or none:\n\n```js\n// build search results HTML\nvar html = ''\nfor(var i in json.rows) {\n  const doc = json.rows[i].fields\n  html += `<p><a href=\"${doc.link}\">${doc.title}</a></p>`\n}\n\n// update the web page\ndocument.getElementById('content').innerHTML = html\n```\n\n## Example\n\nThe Cloudant blog is an example of this technology in action. It is a Jekyll-powered static website whose Atom feed is being polled by an IBM Cloud Functions action and whose search facility is powered by a Cloudant database containing the post's meta data.\n",
    "url": "/2018/08/10/Adding-Search-to-a-Static-Jekyll-site.html",
    "tags": "Search Static",
    "id": "47"
  },
  {
    "title": "Time-sortable _ids",
    "description": "Making _ids unique and time-sortable",
    "content": "\n\n\nA Cloudant database document's `_id` field has to be unique. When you create a document and leave the `_id` field blank, the database will create one for you:\n\n```sh\ncurl -X POST \\ \n     -d'{\"x\":1}' \\\n     -H 'Content-type: application/json' \n     \"https://USERNAME:PASSWORD@HOST.cloudant.com/mydb\"\n{\"ok\":true,\"id\":\"97728a06c27d1e11378cd43635c98c1e\",\"rev\":\"1-0785e9eb543380151003dc452c3a001a\"}\n```\n\nCloudant's generated `_id` fields are 32 characters long and made entirely of numerals and lowercase letters. They are unique, or at least have a negligible probability of clashing, by virtue of being a long pseudo-random string of characters. \n\nAlternatively, you may supply your own `_id` field which is useful when your app knows something unique about your domain's data:\n\n```sh\ncurl -X POST \\\n     -d'{\"_id\":\"user:glynn\",\"x\":1}' \\\n     -H 'Content-type: application/json' \\\n     \"https://USERNAME:PASSWORD@HOST.cloudant.com/mydb\"\n{\"ok\":true,\"id\":\"user:glynn\",\"rev\":\"1-0785e9eb543380151003dc452c3a001a\"}\n```\n\nIn this case I'm using the `_id` to store both the *type* of the document (\"user\") and something unique about each user I'm storing (\"glynn\") in the same portmanteau `_id`.\n\n## Making a sortable _id field\n\n\n![sortable]({{< param \"image\" >}})\n> Photo by [Jeff Frenette on Unsplash](https://unsplash.com/photos/Y_AWfh0kGT4)\n\nIn some applications it would be useful for the `_id` field to sort into date/time order. The `_id` is used to create the database's *primary index* which is used to fetch documents by their id (`GET /db/id`) and when selecting ranges of documents (`GET /db/_all_docs`). If a database's `_id` fields sorted into time order, I could extract data by time without having to create a secondary index e.g. I could fetch the 100 most recently added documents to a database by simply querying the primary index:\n\n```sh\ncurl \"https://USERNAME:PASSWORD@HOST.cloudant.com/mydb/_all_docs?descending=true&limit=100\"\n```\n\nAll I need is an `_id` scheme that can generate ids that are both unique in the database and yet sort into date/time order. \n\nOne solution is published in the [kuuid](https://www.npmjs.com/package/kuuid) Node.js library I wrote for just this purpose. Simply use `kuuid.id()` to generate your `_id` values:\n\n```js\n// import the library\nconst kuuid = require('kuuid')\n\n// build up a new document using kuuid as the document's _id\nlet doc = {\n  _id: kuuid.id(),\n  name: 'Glynn',\n  location: 'UK',\n  verified: true\n}\n\n// insert into the database\ndb.insert(doc)\n```\n\nwhich creates a document that looks like this:\n\n```js\n{\n   _id: '0001AKY50w6w4833bGxL26pDWU4UFhTX',\n  name: 'Glynn',\n  location: 'UK',\n  verified: true \n}\n```\n\n## How do sortable ids work?\n\nA `kuuid`-generated id consists of 32 characters made up of numbers and upper case & lower case letters. It is split into two sections:\n\n![kuuid](/img/kuuid.png)\n\n1. the first eight characters contain the date/time, stored as the number of seconds since the 1st of January 1970.\n2. the remaining twenty four characters are 128 bits of random data.\n\nBoth pieces of information are encoded in \"base 62\", allowing more information to be packed into the same number of characters by using a case-sensitive character set.\n\nTwo ids generated in the same second will have the same first eight characters, but the chances of the remaining 24 characters clashing are vanishingly remote. \n\nThe documents will be sorted in the database's primary index in *rough* date order, that is with a precision of one second.\n\nBy judicious use of the [GET /db/_all_docs](https://console.bluemix.net/docs/services/Cloudant/api/database.html#get-documents) and use of the `startkey`/`endkey`/`descending` parameters, the database's primary index can be queried to provide ranges of documents by type in approximate time order. The `kuuid` library provides a `prefix` function that calculates the 8-digit for string that correseponds to a user-supplied date or timestamp:\n\n```js\n// get data between 1st & 15th Feb 2018\nconst kuuid = require('kuuid')\nconst startDate = '2018-02-01T00:00:00.000Z' // 1st Feb 2018\nconst endDate = '2018-02-15T00:00:00.000Z' // 15th Feb 2018\nconst k1 = kuuid.prefix(startDate)\nconst k2 = kuuid.prefix(endDate)\ndb.list({ startkey: k1, endkey: k2 }).then(console.log)\n\n// get data from 12:30 on 30th Nov 2017 to now, newest first\nconst date = '2017-11-30T12:30:00.000Z' \nconst k3 = kuuid.prefix(date)\ndb.list({ endkey: k3, descending: true }).then(console.log)\n```\n\nThis form of querying is a little convoluted, but if your ids are going to be 32-character random strings, it seems useful to make them loosely time-ordered just to be able to quickly establish the documents that were added recently, if nothing else.\n\n## Combining a kuuid and a document type\n\nOptionally, we could still keep the convention of storing the document *type* in the `_id` field too:\n\n```js\nlet doc = {\n  _id: 'user:' + kuuid.id(),\n  name: 'Glynn',\n  location: 'UK',\n  verified: true\n}\n```\n\nOur `_id` field is now sorted by document type AND time! \n\n",
    "url": "/2018/08/24/Time-sortable-document-ids.html",
    "tags": "Time Indexing",
    "id": "48"
  },
  {
    "title": "Generating sample data",
    "description": "Creating realistic JSON data in bulk",
    "content": "\n\n\nApplication development using Cloudant as the database, for me, starts with data design. Having carefully considered how your application's data should be modelled in JSON we may turn to the querying and indexing required:\n\n- How do my queries perform with 10k, 1m or 10m documents?\n- How long does it take for a new batch of data to be indexed?\n- Is it better to use a Cloudant MapReduce or Cloudant Search index to solve a particular problem?\n\nOftentimes, app development starts with a blank database. It's helpful at this point to put the theory to the test with a meaningful amount of data - to a/b test two indexes, benchmark queries and measure indexing and throughput performance.\n\nTo do this we need a source of data. As our application isn't live yet, we don't have any real data.\n\nThis is where the [datamaker](https://www.npmjs.com/package/datamaker) tool comes in.\n\n![pic](/img/kristian-strand-791607-unsplash.jpg)\n\n## What is datamaker?\n\n*datamaker* is a command-line tool that can generate random data. Not just random numbers, but company names, addresses, emails, dates etc.\n\nIt's a free, open-source tool published on npm (Node.js & npm are required). To install it, simply run :\n\n```sh\n$ npm install -g datamaker\n```\n\nGive it a spin by piping in a template string. Placeholders for random data are signified by named tags encased in double curly braces:\n\n```sh\n{% raw %}\n$ echo 'My name is {{name}}.' | datamaker\nMy name is Loreta Brenner.\n{% endraw %}\n```\n\nIf you need more data, the `--iterations`/`-i` flag is used to specify the number of data points:\n\n```sh\n{% raw %}\n$ echo '{{date}},{{company}}' | datamaker -i 10\n1975-03-21,Refoment Holdings LLC\n1989-12-13,Unmold Stores \n1977-02-10,Psammite Mutual S.A\n1983-04-15,Spinsterdom GmbH\n2018-06-22,Recite Stores B.V\n2012-02-24,Willing SIA\n1989-09-22,Pong \n1996-04-28,Sabotage Industries LLC\n2004-03-24,Toxidermic Mutual Corp\n2014-01-04,Betaine Corp \n{% endraw %}\n```\n\nWe can use *datamaker* to form CSV or XML data, but for a Cloudant database we need JSON. The best way to do this is to create a template containing one of your documents, with placeholder tags marking where the data should go:\n\n```js\n{% raw %}\n{\n  \"_id\": \"{{uuid}}\",\n  \"name\": \"{{firstname}} {{surname}}\",\n  \"dob\": \"{{date 2014-01-01}}\",\n  \"address\": {\n    \"street\": \"{{street}}\",\n    \"town\": \"{{town}}\",\n    \"postode\": \"{{postcode}}\"\n  },\n  \"telephone\": \"{{tel}}\",\n  \"pets\": [\"{{cat}}\",\"{{dog}}\"],\n  \"score\": {{float 1 10 1}},\n  \"email\": \"{{email}}\",\n  \"url\": \"{{website}}\",\n  \"description\": \"{{words 20}}\",\n  \"verified\": {{boolean 0.75}},\n  \"salary\": {{float 10000 70000 0}}\n}\n{% endraw %}\n```\n\nNotice how some of the *datamaker* tags can take parameters: `{{float 1 10 1}}` means \"generate a floating point number between 1 and 10, with 1 decimal place.\n\nWe can then pass the path of the file to *datamaker* with the `--template`/`-t` option and specify \"json\" with the `--format`/`-f` flag:\n\n```sh\n$ datamaker -t ./template.json -f json -i 10\n{\"_id\":\"8ZDJ7JJPK4LFN2SH\",\"name\":\"Selena High\",\"dob\":\"2015-07-02\",\"address\":{\"street\":\"2568 Holbrook\",\"town\":\"Masham\",\"postode\":\"HR10 1YL\"},\"telephone\":\"+213-5637-126-628\",\"pets\":[\"Romeo\",\"Bailey\"],\"score\":1.1,\"email\":\"elissa-bonds@hotmail.com\",\"url\":\"http://montana.com\",\"description\":\"offer purposes ends closure cherry applying heather incidents mar alien precipitation universities apartment cycling containing graham remedy lance tackle cotton\",\"verified\":true,\"salary\":13628}\n{\"_id\":\"AQ4LAEZYGPUCKAG9\",\"name\":\"Madalyn Bernal\",\"dob\":\"2015-11-30\",\"address\":{\"street\":\"3656 Palace Street\",\"town\":\"Winsford\",\"postode\":\"SP5 8FR\"},\"telephone\":\"+255-4662-982-251\",\"pets\":[\"Lilly\",\"Apollo\"],\"score\":5.9,\"email\":\"aleshia_marshall@washer.com\",\"url\":\"https://volunteer.com\",\"description\":\"generated our languages relates enlargement questionnaire kitty passes parish coin progressive safe either primarily de remedy barbie dvd defining table\",\"verified\":false,\"salary\":26392}\n{\"_id\":\"ZS1S800XUJAPX53J\",\"name\":\"Demetra Alba\",\"dob\":\"2014-10-16\",\"address\":{\"street\":\"9270 Wilpshire Street\",\"town\":\"Lancaster\",\"postode\":\"WC4 0BP\"},\"telephone\":\"+216-6024-252-842\",\"pets\":[\"Midnight\",\"Shadow\"],\"score\":8.4,\"email\":\"marline-alarcon@hotmail.com\",\"url\":\"http://www.tap.com\",\"description\":\"mg charging sharon blake deutsch popularity bang addition canal dt cycle prayer bowl eleven karaoke reuters urban iraqi cholesterol soviet\",\"verified\":false,\"salary\":47107}\n{\"_id\":\"GNMTP3ODG3SR31PX\",\"name\":\"Sal Riggs\",\"dob\":\"2014-09-29\",\"address\":{\"street\":\"9029 O Road\",\"town\":\"Framlingham\",\"postode\":\"TD9 4KF\"},\"telephone\":\"+66-0335-951-687\",\"pets\":[\"Angel\",\"Harley\"],\"score\":5.7,\"email\":\"lovemessenger@yahoo.com\",\"url\":\"http://www.brochure.com\",\"description\":\"require cameron possibilities evaluating slovenia us nightlife guarantee distributed norfolk middle ob sponsored newman terrain bolivia examines arms quantitative advance\",\"verified\":true,\"salary\":21190}\n{\"_id\":\"87XJDY18USNMRG86\",\"name\":\"Norberto Pacheco\",\"dob\":\"2018-07-03\",\"address\":{\"street\":\"1809 Winser Avenue\",\"town\":\"Bridgnorth\",\"postode\":\"AL59 1LV\"},\"telephone\":\"+34-8379-225-249\",\"pets\":[\"sox\",\"Buddy\"],\"score\":1.2,\"email\":\"leida-carey-elder@legacy.com\",\"url\":\"http://reason.skanland.no\",\"description\":\"gambling ensuring sporting worldwide losing norfolk east are exception princess boxing costumes macedonia maintenance phone mens clinics disagree arrival text\",\"verified\":true,\"salary\":15901}\n{\"_id\":\"Q0LN7F6DXGZQIHAH\",\"name\":\"Arlie Bosley\",\"dob\":\"2015-02-02\",\"address\":{\"street\":\"0889 Arcon Lane\",\"town\":\"Airth\",\"postode\":\"LE51 3UC\"},\"telephone\":\"+266-9546-824-552\",\"pets\":[\"Ziggy\",\"Bentley\"],\"score\":8.8,\"email\":\"skye.pitts@produces.com\",\"url\":\"http://www.applications.com\",\"description\":\"sport completely pain reno weighted junction humanitarian bent algeria papers council newspaper wa electronic nano visiting chinese largely showed generic\",\"verified\":true,\"salary\":30753}\n{\"_id\":\"2EQ9YKFKK024BQ94\",\"name\":\"Cherryl Cooney-Holland\",\"dob\":\"2018-04-29\",\"address\":{\"street\":\"8591 Warham Lane\",\"town\":\"Whitstable\",\"postode\":\"TF28 6LN\"},\"telephone\":\"+973-8154-367-109\",\"pets\":[\"Maggie\",\"Apollo\"],\"score\":8.5,\"email\":\"beatriz-schulze@yahoo.com\",\"url\":\"https://electrical.com\",\"description\":\"people surname engineering rid reduction charging salvation telescope kb for subjects comments honest memory civilization patricia employ election was honors\",\"verified\":true,\"salary\":39242}\n{\"_id\":\"482AB79QSS80URSL\",\"name\":\"Manuel Loftis\",\"dob\":\"2016-12-07\",\"address\":{\"street\":\"4548 Blucher Avenue\",\"town\":\"Hunstanton\",\"postode\":\"EN77 1TR\"},\"telephone\":\"+61-1705-364-098\",\"pets\":[\"Rusty\",\"Ginger\"],\"score\":7.7,\"email\":\"ladawn_battle@hotmail.com\",\"url\":\"http://www.aircraft.mil.ge\",\"description\":\"arrive labour pamela realistic platforms typing thou wc spa scan animal passion patients wayne contamination fiber roses mixing wanting aids\",\"verified\":false,\"salary\":15796}\n{\"_id\":\"RJA2DPS954J22AQ9\",\"name\":\"Lovella Richmond\",\"dob\":\"2018-01-12\",\"address\":{\"street\":\"6173 Byrth\",\"town\":\"Skipton\",\"postode\":\"WA5 7FR\"},\"telephone\":\"+33-7083-380-202\",\"pets\":[\"Daisy\",\"Gus\"],\"score\":5.5,\"email\":\"jayne.roche@wet.com\",\"url\":\"http://www.source.com\",\"description\":\"livestock geography italy commitment reseller strengthen gp meals announces href wage supervision guarantees problems lip tuner did site site then\",\"verified\":true,\"salary\":69739}\n{\"_id\":\"HNI4ISQDR17SEBD2\",\"name\":\"Keesha Tong\",\"dob\":\"2014-06-16\",\"address\":{\"street\":\"0567 Penny Road\",\"town\":\"North Berwick\",\"postode\":\"LA63 0MM\"},\"telephone\":\"+675-7584-818-118\",\"pets\":[\"Muffin\",\"Cody\"],\"score\":4.4,\"email\":\"isobel_fowler@gmail.com\",\"url\":\"https://www.retro.lib.ct.us\",\"description\":\"nuclear pennsylvania cooperation builder penny identical palestine detected le frequently collect customer providers string ticket col receivers spring suited chip\",\"verified\":false,\"salary\":44255}\n```\n\nThe *datamaker* project has tens of supported tags - see the [project's documentation](https://github.com/glynnbird/datamaker#tag-reference) for details. Airport codes, URLs, email addresses, prices, currencies etc.\n\n## Importing data into a Cloudant database\n\nThe tool to import JSON data into Cloudant already exists: it's [couchimport](https://www.npmjs.com/package/couchimport) which supports the `jsonl` format (one JSON document per line) out of the box. Simply pipe the output of *datamaker* into *couchimport*:\n\n```sh\n$  datamaker -t ./template.json -f json -i 10000 | couchimport --db mydatabase --type jsonl\n```\n\nThe output of the *datamaker* is written to Cloudant in a series of bulk HTTP API calls. Simple as that!\n\n## References\n\n- [datamaker on npm](https://www.npmjs.com/package/datamaker)\n- [source code](https://github.com/glynnbird/datamaker)",
    "url": "/2018/09/14/Generating-sample-JSON-data.html",
    "tags": "Data JSON",
    "id": "49"
  },
  {
    "title": "Search Analyzers",
    "description": "Processing data prior to indexing",
    "content": "\n\n\nCloudant Search is the free-text search technology built in to the Cloudant database that is powered by [Apache Lucene](http://lucene.apache.org/). Lucene-based indexes are used for:\n\n- finding documents that best match a supplied string.\n- constructing fielded queries in Lucene's query language  e.g. `state:florida AND (status:provisional OR status:published)`.\n- counting facets, that is counts of repeating values within the result set.\n- or all of the above.\n\nWhen creating a Cloudant Search index, thought must be given as to which fields from your documents *need* to indexed and *how* they are to be indexed. \n\nOne aspect of the indexing process is the choice of *analyzer*. An analyzer is code that may:\n\n- lowercase the string - making the search case-insensitive.\n- tokenise the string - breaking a sentence into individual words.\n- stem the words - removing language-specific word endings e.g. *farmer* becomes *farm*\n- remove stop words - ignoring words like *a*, *is* *if* can make the index smaller and more efficient.\n\nAt indexing-time source data is processed using the analyzer logic prior to sorting and storage in the index. At query-time the search terms are processed using the same analyzer code before interrogating the index.\n\n![jigsaw](/img/hans-peter-gauster-252751-unsplash.jpg)\n> Photo by [Hans-Peter Gauster on Unsplash](https://unsplash.com/photos/3y1zF4hIPCg)\n\n## Testing the analyzer\n\nThere is a [Cloudant Search API call](https://console.bluemix.net/docs/services/Cloudant/api/search.html#testing-analyzer-tokenization) that will apply one of the built-in Lucene analyzers to a supplied string to allow you to see the effect of each analyzer.\n\nTo look at each anaylzer in turn, I'm going to pass the same string to each analyzer to measure the effect:\n\n> \"My name is Chris Wright-Smith. I live at 21a Front Street, Durham, UK - my email is chris7767@aol.com.\"\n\n### Standard analyzer\n\n> {\"tokens\":[\"my\", \"name\", \"chris\", \"wright\", \"smith\", \"i\", \"live\", \"21a\", \"front\", \"street\", \"durham\", \"uk\", \"my\", \"email\", \"chris7767\", \"aol.com\"]}\n\n- punctation removed\n- words split on spaces and punctuation\n- stop words removed (no 'is', 'at')\n- all lowercase\n- note how \"aol.com\" remains intact\n\n### Keyword analyzer\n\n> {\"tokens\":[\"My name is Chris Wright-Smith. I live at 21a Front Street, Durham, UK - my email is chris7767@aol.com.\"]}\n\n- string remains intact\n\n### Simple analyzer\n\n> {\"tokens\":[\"my\", \"name\", \"is\", \"chris\", \"wright\", \"smith\", \"i\", \"live\", \"at\", \"a\", \"front\", \"street\", \"durham\", \"uk\", \"my\", \"email\", \"is\", \"chris\", \"aol\",\"com\"]}\n\n- punctuation removed\n- words split on spaces and punctuation\n- no stop words (notice \"is\", \"at\")\n- all lowercase\n- note how \"chris7767\" became \"chris\" and \"21a\" becomes \"a\"\n\n\n### Whitespace analyzer\n\n> {\"tokens\":[\"My\", \"name\", \"is\", \"Chris\", \"Wright-Smith.\", \"I\", \"live\", \"at\", \"21a\", \"Front\", \"Street,\", \"Durham,\", \"UK\", \"-\" , \"my\" ,\"email\", \"is\", \"chris7767@aol.com.\"]}\n\n- some punctuation removed\n- words split on spaces\n- no stop words (notice \"is\", \"at\")\n- case sensitive\n- note how email remains intact\n \n### Classic analyzer\n\n> {\"tokens\":[\"my\", \"name\", \"chris\", \"wright\", \"smith\", \"i\", \"live\", \"21a\", \"front\", \"street\", \"durham\", \"uk\", \"my\", \"email\", \"chris7767@aol.com\"]}\n\n- punctation removed\n- words split on spaces and punctuation\n- stop words removed (no 'is', 'at')\n- all lowercase\n- email remains intact\n\n### English analyzer\n\n> {\"tokens\":[\"my\", \"name\",\"chri\", \"wright\", \"smith\", \"i\", \"live\", \"21a\", \"front\", \"street\", \"durham\", \"uk\", \"my\", \"email\", \"chris7767\",\"aol.com\"]}\n\n- punctation removed\n- words split on spaces and punctuation\n- words stemmed (notice \"chris\" becomes \"chri\")\n- stop words removed (no 'is', 'at')\n- all lowercase\n- email remains intact\n\nThe language-specific analyzers make the most changes to the source data:\n\n```\nThe quick brown fox jumped over the lazy dog.\n{\"tokens\":[\"quick\",\"brown\",\"fox\",\"jump\",\"over\",\"lazi\",\"dog\"]}\n\nFour score and seven years ago our fathers brought forth, on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal.\n{\"tokens\":[\"four\",\"score\",\"seven\",\"year\",\"ago\",\"our\",\"father\",\"brought\",\"forth\",\"contin\",\"new\",\"nation\",\"conceiv\",\"liberti\",\"dedic\",\"proposit\",\"all\",\"men\",\"creat\",\"equal\"]}\n```\n\n## Which analyzer should I pick?\n\nIt depends on your data. If you have structured data (email addresses, zip codes, names etc in separate fields), then it's worth picking an analyzer that retains the data you need to keep *intact* for your search needs. \n\nOnly index the fields that you need. Keeping the index small helps to improve performance.\n\nLet's deal with common data sources and look at best analyzer choices.\n\n### Names\n\nIt's likely that name fields should use an analyzer that doesn't stem words. The **Whitespace** analyzer retains the words' case (meaning the search terms would have to be a full, case-senstive match) and leaves double-barrelled names intact. If you want to split up double-barrelled names, then the **Standard** analyzer would do the job.\n\n### Email addresses\n\nThere is a built-in **Email** analyzer for just this purpose which lowercases everything and then behaves like the **Keyword** analyzer. \n\n### Unique id\n\nOrder numbers, payment references and UUIDs such as \"A1324S\", \"PayPal0000445\" and \"ABC-1412-BBG\" should be retained without any pre-processing, so the **Keyword** analyzer is preferred.\n\n### Country codes\n\nCountry codes such as \"UK\" should also use the **Keyword** analyzer to prevent the removal of stopwords that match the country codes e.g. \"IN\" for India. Note that the Keyword Analzer is case-sensitive.\n\n### Text\n\nA block of free-form text is best processed with a language-specific analyzer such as the **English** analyzer or in a more general case, the **Standard** analyzer.\n\n\n## Store: true or include_docs=true?\n\nWhen returning data from a search there are two options\n\n- at index-time, choose the `{store: true}` option to indicate that that the field you dealing with needs to be stored inside the index. A field can be \"stored\" even if it isn't used for indexing itself e.g. you may want to \"store\" a telephone number, even if your search algorithm doesn't allow search by phone number. \n- or, pass `?include_docs=true` at query-time to indicate to Cloudant that you want the entire bodies of each matching document to be returned.\n\nThe former option means having a larger index but is the fastest way of retrieving data. The latter option keeps the index small but adds extra query-time work for Cloudant as it has to fetch document bodies after the search result set is calculated. This can be slower to execute and add a further burden to a Cloudant cluster.\n\nIf possible, choose the former option: \n\n- only index the fields that are to be searchable.\n- only store the fields you need retrieving at query-time.\n\n## Entity extraction\n\nProviding a good search experience depends on the alignment of your users' search needs with structure in the data. Throwing lots of unstructured data at an indexing engine gets you only so far; if you can add further structure to unstructured data, then the search experience will benefit as fewer \"false positives\" will be returned. Let's take an example:\n\n> \"Edinson Cavani scored two superb goals as Uruguay beat Portugal to set up a World Cup quarter-final meeting with France. Defeat for the European champions finished Cristiano Ronaldo's hopes of success in Russia just hours after Lionel Messi and Argentina were knocked out, beaten 4-3 by Les Bleus.\"\n> \n> Source: BBC News https://www.bbc.co.uk/sport/football/44439361\n\nFrom this snippet, I would manually extract the following \"entities\":\n\n- Edinson Cavani - a footballer\n- Uruguay - a country\n- Portugal - another country\n- World Cup - a football competition\n- Cristiano Ronaldo - a footballer\n- Russia - a country\n- Lionel Messi - a footballer \n- Argentina - a country\n- Les Bleus - a nickname of the French national football team\n\nEntity extraction is the process of locating known entities (given a database of such entities) and storing the entities in the search engine instead of or as well as the source text. The [Watson Natural Language and Understanding API](https://www.ibm.com/watson/services/natural-language-understanding/) can be fed raw text and will return entities it knows about (you can provide your own enitity model for your domain-specific application):\n\n![screenshot](/img/analyzers-screenshot.png)\n\nAs well as entities, the API can also place the article in a hierarchy of categories. In this case, Watson suggested:\n\n- / travel / tourist destinations / france\n- / sports / soccer\n- / sports / football\n\nPre-processing your raw data, by calling the Watson API for each document and storing a list of entities/concepts/categories in your Cloudant document, provides automatic meta data about your free-text information and can provide an easier means to search and navigate your app.\n\n\n\n\n\n\n\n\n",
    "url": "/2018/10/19/Search-Analyzers.html",
    "tags": "search",
    "id": "50"
  },
  {
    "title": "Stale, update and stable",
    "description": "Triggering indexing and permitting stale queries",
    "content": "\n\n\n**tl;dr** If you are using `stale=ok` in queries to Cloudant or CouchDB 2.x, you\nmost likely want to be using `update=false` instead. If you are using\n`stale=update_after`, use `update=lazy` instead.\n\nThis question has come up a few times, so here's a reference to what the\nsituation is with these parameters to query requests in [Cloudant][c] and\n[CouchDB][couch] 2.x.\n\nCouchDB originally used `stale=ok` on the query string to specify that you were\nokay with receiving out-of-date results. By default, CouchDB lazily updates\nindexes upon querying them rather than when JSON data is changed or added. If up\nto date results are not strictly required, using `stale=ok` provides a latency\nimprovement for queries as the request does not have to wait for indexes to be\nupdated before returning results. This is particularly useful for databases with\na high write rate.\n\nAs an aside, Cloudant automatically enqueues indexes for update when primary\ndata changes, so this problem isn't so acute. However, in the face of high\nupdate rate bursts, it's still possible for indexing to fall behind so a delay\nmay occur.\n\nWhen using a single node, as in CouchDB 1.x, this parameter behaved as you'd\nexpect. However, when clustering was added to CouchDB, a second meaning was\nadded to `stale=ok`: also use the same set of shard replicas to retrieve the\nresults.\n\nRecall that [Cloudant and CouchDB 2.x stores three copies of each shard][1] and\n[by default will use the shard replica that starts returning results fastest for\na query request][2]. This latter fact helps even out load across the cluster.\nHeavily loaded nodes will likely return slower and so won't be picked to respond\nto a given query. When using `stale=ok`, the database will instead always use\nthe same shard replicas for every request to that index. The use of the same\nreplica to answer queries has two effects:\n\n1. Using `stale=ok` could drive load unevenly across the nodes in your database\n   cluster because certain shard replicas would always be used for the queries\n   to the index that specify `stale=ok`. This means a set of nodes could receive\n   outside numbers of requests.\n2. If one of the replicas was hosted on a heavily loaded node in the cluster,\n   this would slow down all queries to that index using `stale=ok`. This is\n   compounded by the tendency of `stale=ok` to drive imbalanced load.\n\nThe end result is that using `stale=ok` can, counter-intuitively, cause queries\nto become slower. Worse, they may become unavailable during cluster split-brain\nscenarios because of the forced use of a certain set of replicas. Given that\nmostly people use `stale=ok` to improve performance, this wasn't a great state\nto be in.\n\nAs `stale=ok`'s existing behaviour needed to be maintained for backwards\ncompatibility, the fix for this problem was to introduce two new query string\nparameters were introduced which set each of the two `stale=ok` behaviours\nindependently:\n\n1. `update=true/false/lazy`: controls whether the index should be up to date\n   before the query is executed.\n    1. `true`: the index will be updated first.\n    2. `false`: the index will not be updated.\n    3. `lazy`: the index will not be updated before the query, but enqueued for\n       update after the query is completed.\n2. `stable=true/false`: controls the use of the certain shard replicas.\n\nThe main use of `stable=true` is that queries are more likely to appear to \"go\nforward in time\" because each [shard replica may update its indexes in different\norders][2]. However, this isn't guaranteed, so the availability and performance\ntrade offs are likely not worth it.\n\nThe end result is that virtually all applications using `stale=ok` should move\nto instead use `update=false`.\n\n[1]: /2015/10/19/Read-Write-Behaviour-in-a-cluster.html\n[2]: /2016/01/31/Understanding-Cloudant-Indexing.html\n[c]: https://www.ibm.com/cloud/cloudant\n[couch]: https://couchdb.apache.org/",
    "url": "/2018/11/06/What-is-stale-update-and-stable.html",
    "tags": "indexing querying",
    "id": "51"
  },
  {
    "title": "The Data Warehouse",
    "description": "Exporting Cloudant data to a warehouse",
    "content": "\n\n\n> Addendum: Since publication of this blog, couchwarehouse has added support for MySQL and PostgreSQL as well as SQLite. This post is still valid, but bear in mind you now have a choice of target database types.\n\nOne of Cloudant's best use-cases is as a rock-solid, always-on operational datastore. It is built for fault-tolerance, storing multiple copies of your data on separate servers so that a Cloudant cluster can withstand the loss of multiple nodes without loss of service.\n\nLet's take the example of an online shop that uses Cloudant to store its transactions. At the birth of the business its sales database is empty, but as time progresses and as the business becomes more successful, the database grows to a healthy size - perhaps hundreds of thousands or millions or documents. The management of the company will be asking questions of the database:\n\n- How many sales did we make this week?\n- What are the top ten products sold?\n- What are the peak times for selling a particular type of product?\n- How successful was our \"Halloween\" marketing campaign?\n  \nAlthough Cloudant has built-in [MapReduce](https://console.bluemix.net/docs/services/Cloudant/api/creating_views.html#views-mapreduce-) which can provide simple aggregations of data against pre-defined keys (e.g. sales by year/month/day), it will eventually fall short of a business analyst's expectations when faced with complex, ever changing queries or questions that relate the sales data to other data sets (such as marketing click-through data). \n\n> If you need to ask ad-hoc questions of your data without affecting the performance of your operational dataset, then you need **a Data Warehouse**.\n\nCloudant is your high-uptime operational data store, and a Data Warehouse is a query engine, which organises its data in a way that optimises for querying rather than uptime or data resilience. If you need to ensure that critical data is stored in multiple locations with a high availability (and a handful of fixed queries) you need Cloudant. If you need to run an ever-changing set of complex queries you *also* need a Data Warehouse.\n\n![warehouse](/img/samuel-zeller-118195-unsplash.jpg)\n> [Photo by Samuel Zeller on Unsplash](https://unsplash.com/photos/JuFcQxgCXwA)\n\nThe most common scenario is you need *both*, with Cloudant data being fed to a data warehouse periodically. In this post we'll look at how we would write some code to copy and transform Cloudant data before writing it to relational database, allowing us to query using Structured Query Language (SQL).\n\n## What do the Cloudant documents look like?\n\nIn our example, we are storing *one document per completed sale*:\n\n```js\n{\n  _id: \"001fgS954GN35e4NJPyK1W9aiE44m2xD\",\n  customerId: \"001edS7k4gJxqY1aXpni3gHuOy0WusLe\",\n  customerEmail: \"bob@aol.com\",\n  saleDate: \"2018-09-15\",\n  saleTime: \"10:56:22\",\n  paymentRef: \"PayPal584477238823\",\n  currency: \"GBP\",\n  basket: [\n    {\n      productId: \"A6624\",\n      productName: \"Fender Road Worn 60's Jazzmaster\",\n      productVariant: \"3-tone burst\"\n     },\n    {\n      productId: \"B8852\",\n      productName: \"Fender '68 Custom Twin Reverb\",\n      productVariant: null,\n    }\n  ],\n  total: 2390,\n  status: \"paid\",\n  dispatched: false,\n  dispatchAddress: {\n    street: \"19 Front Street\",\n    town: \"Middletown\",\n    zip: \"W1A 1AA\"\n  },\n  dispatchCourierRef: \"\"\n}\n```\n\nThings to note:\n\n- This document contains everything we need to know about a sale. There may be further supplemental information about the user/product/payment in other databases, but fetching *this* document gives us enough information to render an \"order summary\" web page or email. This is good practice in a NoSQL database - in a database without joins, we don't want to have to make several round-trips to the database to piece together all of the data we need.\n- A document is created when the payment is confirmed.\n- The document maybe updated later to indicate that the order has been dispatched and to back-fill the `dispatchCourierRef`.\n- The document contains an array of products in the `basket` field which store one or more line items purchased.\n\n## How can I generate some sample data?\n\nCreate a `template.json` file containing the outline of the document to create with placeholder tags where the data will be placed:\n\n```\n{% raw %}\n{\n  \"_id\": \"{{uuid}}\",\n  \"customerId\": \"{{uuid}}\",\n  \"customerEmail\": \"{{email}}\",\n  \"saleDate\": \"{{date 1999-01-01}}\",\n  \"saleTime\": \"{{time}}\",\n  \"paymentRef\": \"PayPal{{digits 16}}\",\n  \"currency\": \"{{currency}}\",\n  \"basket\": [\n    {\n      \"productId\": \"A{{digits 3}}\",\n      \"productName\": \"{{words 3}}\",\n      \"productVariant\": \"{{words 6}}\"\n    },\n    {\n      \"productId\": \"B{{digits 3}}\",\n      \"productName\": \"{{words 3}}\",\n      \"productVariant\": \"{{words 6}}\"\n    }\n  ],\n  \"total\": {{price 100 2600}},\n  \"status\": \"paid\",\n  \"dispatched\": {{boolean 95}},\n  \"dispatchAddress\": {\n    \"street\": \"{{street}}\",\n    \"town\": \"{{town}}\",\n    \"zip\": \"{{postcode}}\"\n  },\n  \"dispatchCourierRef\": \"\"\n}\n{% endraw %}\n```\n\nThen using the [datamaker](https://www.npmjs.com/package/datamaker) command-line tool, create thousands of sample documents and pipe them into the [couchimport](https://www.npmjs.com/package/couchimport) tool to write the documents to Cloudant:\n\n```sh\n$ datamaker -t ./template.json -f json -i 10000 | couchimport --db mydatabase --type jsonl\n```\n\nWe can use a command of the above form to generate thousands, or millions of documents. Simply change the `-i` parameter to the number of documents you need.\n\n## How can I get Cloudant data into a Data Warehouse?\n\nA quick way of getting a queryable view of a Cloudant or CouchDB database is to use the [couchwarehouse](https://www.npmjs.com/package/couchwarehouse) command-line tool. Once installed, creating a warehouse is simple from the command-line:\n\n```sh\n$ couchwarehouse --url https://U:P@host.cloudant.com --db mystore\n```\n\nThe utility will attempt to discover the schema of your data, create a local [SQLite](https://www.sqlite.org/index.html) database with a database table that matches your documents' schema and begin populating the table from the Cloudant/CouchDB changes feed.\n\n![schematic](/img/couchwarehouseschematic.png)\n\nIn another terminal, you can then run `sqlite3` and begin querying your data with SQL:\n\n```\n$ sqlite3 couchwarehouse.sqlite\n\nsqlite> SELECT customerEmail,dispatchAddress_town,status,total FROM mystore LIMIT 10;\ncustomerEmail                                  dispatchAddress_town  status      total     \n",
    "url": "/2018/11/16/Cloudant-and-the-data-warehouse.html",
    "tags": "SQL Warehouse",
    "id": "52"
  },
  {
    "title": "Caching with Nginx",
    "description": "Caching Cloudant reads for faster performance",
    "content": "\n\n\nCloudant and its open-source sibling Apache CouchDB were were born on the web. Their HTTP/HTTPS API is not a bolt-on afterthought - it is *the* way of interacting with the database built in from the ground up. Let's take the use-case of Cloudant being used as a back-end database in a traditional client/server web app:\n\n![schematic](/img/nginx_schematic_1.png)\n\nWeb users interact with a web page, sending HTTP requests to one of a number of application servers. The application, needing data to render the page will make an HTTP request to Cloudant to get fetch the data and then respond back in kind to the client.\n\nIf the same request is being made to Cloudant over and over again in a short time frame, then Cloudant simply answers each request.  Under production loads and to avoid overworking the database, developers may choose to cache data *in their app* rather than make a round-trip to the database. This is suitable for:\n\n- data that doesn't change very often e.g. a database of US zip codes\n- slices of data that are accessed frequently but where it doesn't matter when the user sees a slightly stale version of the query. This is very application-dependent but let's imagine your e-commerce site is to have a list of three special offers on the front page. As the front page is accessed frequently, it makes little sense to query the database for every page render.\n\n![cache]({{< param \"image\" >}})\n\n> [Photo by Denise Johnson on Unsplash](https://unsplash.com/photos/CQpN2IdkSdA)\n\nThere are many ways to implement a cache. In this article I'll show how a Nginx proxy can be created to cache HTTP requests to take some of the load off your Cloudant service and to get data to your app quickly.\n\n## What is Nginx?\n\n[Nginx](https://www.nginx.com/) is an open-source web server. At its simplest it can serve out a tree of static files over HTTP. It can also be configured as a \"reverse proxy\", that is it can sit between a client and server and transparently route traffic between them, caching some of the content to allow a future repeat request to be serviced from the local cached data.\n\n![reverse proxy schematic](/img/nginx_schematic_2.png)\n\n\nIn our application we'll be configuring Nginx as a reverse proxy and placing it between our application servers and Cloudant. Instead of our application connecting directly to Cloudant, it will instead connect to Nginx which will either return some cached content or make the Cloudant request and return that.\n\nNginx can be installed in two places:\n\n-  on the same machine as your application code (your app will connect to port on \"localhost\".\n-  or, on a separate machine your network and shared between multiple instances of your application server. \n\nThe former approach is simpler, but the second allows multiple application servers so share the same cache pool.\n\n## Installing Nginx\n\nFollow the [installation instructions for your platform](https://docs.nginx.com/nginx/admin-guide/installing-nginx/installing-nginx-plus/) - on my Mac I used [brew](https://brew.sh/):\n\n```sh\nbrew install nginx\n```\n\n## Configuring Nginx\n\nThe configuration for Nginx belongs in a file called `nginx.conf`. We're going to leave the installed configuration *as is* and create a new one with the following content:\n\n```\nerror_log /usr/local/var/log/nginxerror.log;\npid /usr/local/var/run/nginx.pid;\n\nevents {\n}\n\nhttp {\n  \n  # define custom log format with cache HIT/MISS\n  log_format main '$remote_addr - $upstream_cache_status  [$time_local]  $status '\n    '\"$request\" $body_bytes_sent \"$http_referer\" '\n    '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n  access_log /usr/local/var/log/nginx/access.log  main;\n\n  # define path where cached data is stored\n  proxy_cache_path /tmp/cache keys_zone=cloudant:10m;\n\n  server {\n\n    # listen on port 8080 (HTTP)\n    listen       8080;\n\n    # this server's root directory maps to Cloudant's root\n    location / {\n\n      # define Cloudant root\n      proxy_pass https://4268d9ec-250f-4d59-bcbd-fb47a14ef856-bluemix.cloudant.com;\n      \n      # pass through headers e.g. authentication \n      proxy_pass_request_headers on;\n      \n      # cache data in the \"cloudant\" pool\n      proxy_cache cloudant;\n\n      # this is how the cache key is calculated\n      proxy_cache_key $host$uri$is_args$args$request_body;\n\n      # cached data from 200 requests is valid for 10 minutes\n      proxy_cache_valid 200 10m;\n\n      # even POST data is cached\n      proxy_cache_methods GET HEAD POST;\n \n      # add a header to show whether the data is a from cache or not\n      add_header X-Cache-Status $upstream_cache_status;\n    }\n  }\n}\n```\n\nN.B Change the hostname to the hostname of **your Cloudant service** in the `proxy_pass` line of the configuration file.\n\nWe can then run `nginx` with the command:\n\n```sh\n$ nginx -c $PWD/nginx.conf \n```\n\nand stop it with\n\n```sh\n$ nginx -s stop\n```\n\nTo monitor `nginx`'s logs, simply tail the log file:\n\n```sh\n$ tail -f /usr/local/var/log/nginx/access.log\n```\n\n## Testing the nginx proxy with curl\n\nI like to setup an environment variable containing the URL of my Cloudant service to save typing. In this case, the URL needs to be of the form:\n\n```sh\n$ export COUCH_URL=\"http://USERNAME:PASSWORD@localhost:8080\"\n```\n\nNotice that:\n\n- We are using `http` not `https`. Nginx is serving out HTTP only - it will use HTTPS to communicate with Cloudant from there but uses HTTP to service its clients.\n- We need to include our Cloudant username & password in the URL. Nginx will pass on the authentication headers we supply.\n- We use `localhost` on port 8080 as our hostname when we want to communicate with Cloudant via the proxy.\n\nNow we can test the connection by visiting the top of the Cloudant API service:\n\n```sh\n$ curl $COUCH_URL/\n{\"couchdb\":\"Welcome\",\"version\":\"2.1.1\",\"vendor\":{\"name\":\"IBM Cloudant\",\"version\":\"7410\",\"variant\":\"paas\"},\"features\":[\"geo\",\"scheduler\",\"iam\"]}\n```\n\nIt we repeat the request with the `-i` command-line switch, we can see whether the data is coming from Cloudant or via the cache by looking at the `X-Cache-Status` header:\n\n```sh\n$ curl -i $COUCH_URL\nHTTP/1.1 200 OK\nServer: nginx/1.15.6\nDate: Fri, 16 Nov 2018 10:07:44 GMT\nContent-Type: application/json\nContent-Length: 144\nConnection: keep-alive\nCache-Control: must-revalidate\nX-Couch-Request-ID: 6204aa106f\nX-Frame-Options: DENY\nStrict-Transport-Security: max-age=31536000\nX-Content-Type-Options: nosniff\nX-Cloudant-Request-Class: unlimited\nX-Cloudant-Backend: bm-cc-us-south-11\nVia: 1.0 lb1.bm-cc-us-south-11 (Glum/1.66.0)\nX-Cache-Status: HIT\n\n{\"couchdb\":\"Welcome\",\"version\":\"2.1.1\",\"vendor\":{\"name\":\"IBM Cloudant\",\"version\":\"7410\",\"variant\":\"paas\"},\"features\":[\"geo\",\"scheduler\",\"iam\"]}\n```\n\nIn the `nginx` logs, you should see \"HIT\" or \"MISS\" against each entry:\n\n```\n127.0.0.1 - MISS  [16/Nov/2018:10:06:41 +0000]  200 \"GET / HTTP/1.1\" 144 \"-\" \"curl/7.54.0\" \"-\"\n127.0.0.1 - HIT  [16/Nov/2018:10:07:44 +0000]  200 \"GET / HTTP/1.1\" 144 \"-\" \"curl/7.54.0\" \"-\"\n```\n\nThe first fetch was a \"MISS\", the second a \"HIT\".\n\nTry fetching some data and repeating the request to get the cached version. We can use the `time` command to get an idea of how much the cache is speeding things up e.g.\n\n```sh\n$ time curl -s $COUCH_URL/cities/_all_docs?limit=500 > /dev/null\nreal\t0m0.849s\nuser\t0m0.007s\nsys\t0m0.007s\n$ time curl -s $COUCH_URL/cities/_all_docs?limit=500 > /dev/null\nreal\t0m0.020s\nuser\t0m0.007s\nsys\t0m0.006s\n```\n\nThe first request took 850ms, the second (cached) request took 20ms.\n\n## Putting cache to work in your app\n\nUsing the Nginx-powered cache in your own app is as simple as feeding a different URL to the Cloudant library:\n\n```js\n// setup Express\nconst express = require('express')\nconst app = express()\n\n// setup Cloudant library\nconst Cloudant = require('@cloudant/cloudant')\nconst cloudantReadOnly = Cloudant({\n  url : `http://${process.env.USERNAME}:${process.env.password}@localhost:8080`, \n  plugins: 'promises'\n})\nconst cloudantWriteOnly = Cloudant({\n  password: process.env.password,\n  account: process.env.USERNAME,\n  plugins: 'promises'\n})\n\n// home page\napp.get('/', async (req, res) => {\n\n  // do search\n  const db = cloudantReadOnly.db.use('mydb')\n  const doc = await db.find({selector: {\"name\": \"London\"}})\n  res.send(doc)\n})\n\napp.listen(3000)\n```\n\nThe above code makes two Cloudant objects: one to handle read-only requests via the Nginx proxy, the other for writes that connects directly to Cloudant. The root path of this app performs a Cloudant query via the proxy, outputting the result.\n\nRunning this app has the same performance profile as the `curl` tests: cached data is retrieved much faster than running a query on a Cloudant cluster on the other side of the world. \n\nIt's worth bearing in mind that by caching repeated queries, Cloudant is handling fewer queries for you which will be reflected in provisioned throughput you need to buy in your Cloudant Lite and Standard plans provision. The \"query\" requests (`POST /db/_find`, `GET /db/_all_docs`, `GET /db/_search`, `GET /db/_design/...`) are the request types most-likely to eat up your Cloudant capacity. Caching queries helps to prevent maxing-out your Cloudant service and delivers better performance for your users.\n\n## When to use caching\n\nEmploying caching is a trade-off between speed of returning the results against the freshness of data returned. If you know your data isn't changing frequently, then a generous cache window (say an hour or a day) may be used. If it's important that fresh data is surfaced to your users quickly, then a shorter window (say 5 or 10 minutes) may be better. \n\nCaching works well when handling \"peaky\" traffic: let's say a particular page on your site becomes popular because of the success of a marketing campaign. It's better in this case to cache the pertinent content and deliver the results quickly, rather than wasting your Cloudant database resources producing the same results over and over again.\n\n> Caching can help take the load from your expensive primary data store by bring cheaper and faster resources to bear instead. Oh and cached data is returned faster.\n\nThe `nginx` configuration caches all `GET` & `HEAD` requests by default. I added `POST` to the `proxy_cache_methods` configuration to catch [Cloudant Query](https://console.bluemix.net/docs/services/Cloudant/api/cloudant_query.html) API calls which use the `POST /db/_find` method. This may have unintended consequences if you route writes through this proxy e.g. `POST /db/_bulk_docs` or `POST /db`. I would recommend only sending *read* requests through the proxy and any API calls that modify data should be sent directly to Cloudant.\n\n![reverse proxy schematic](/img/nginx_schematic_3.png)\n",
    "url": "/2018/11/23/Caching-Cloudant-with-Nginx.html",
    "tags": "Nginx Cache",
    "id": "53"
  },
  {
    "title": "Fuzzy search using soundex",
    "description": "Using the soundex algorithm to find words that sound alike.",
    "content": "\n\n\nIf you want to find documents in your database that contain a word that sounds like some other word even though it does not have the same spelling  (a homophone), you can use the soundex algorithm.\n\nSoundex is an algorithm for indexing names by how they are pronounced in English.\n\nIts purpose is to encode words that sound alike with the same representation so that they can be matched despite minor differences in spelling. The algorithm is described [here](https://en.wikipedia.org/wiki/Soundex).\n \nHere is a Javascript function that implements the soundex algorithm:\n\n```js\nvar soundex = function (str) {\n    // Fold the input string to lower case and split it into its constituent letters.  \n    letters = str.toLowerCase().split('');\n\n    // Take off the first letter.\n    first_letter = letters.shift();\n\n    // Here are the soundex codes for each letter. \n    // Letters that sound similar have the same soundex code. \n    // Vowels and 'h', 'w' and 'y' are ignored.     \n    soundex_letter_codes = {\n     a: '', e: '', h: '', i: '', o: '', u: '', w: '', y:'',\n     b: 1, f: 1, p: 1, v: 1,\n     c: 2, g: 2, j: 2, k: 2, q: 2, s: 2, x: 2, z: 2,\n     d: 3, t: 3,\n     l: 4,\n     m: 5, n: 5,\n     r: 6\n    };\n \n    \n    // Convert the string to its soundex value.\n    // The soundex value of the string consists of the first letter followed by \n    // the soundex code of subsequent letters.\n    // If two or more letters with the same soundex code are adjacent \n    // only retain one of them.\n\n    soundex_value = \n     // Take the first letter of the word ...\n     first_letter + \n     // ... and append the soundex code of each subsequent letter ...\n     letters.map(function (letter) { return soundex_letter_codes[letter] })\n     // ... but if two or more letters with the same soundex code are adjacent,\n     // only retain the first.\n            .filter(function (letter_soundex_value, i, a) {\n         return ((i === 0) ? \n                 letter_soundex_value !== soundex_letter_codes[first_letter] : \n                 letter_soundex_value !== a[i - 1]\n                );\n     }).join('');\n\n    // Make the soundex value four characters long \n    // by truncating it or padding it with zeros. \n    // Fold it to upper case.\n    soundex_value = (soundex_value + '000').slice(0, 4).toUpperCase(); \n\n\n    // Return the soundex value.\n    return soundex_value;\n};\n```\n\nYou can call the `soundex` function in the Map function of a view. For example:\n\n```js\nfunction (doc) {\n  emit(soundex(doc.name), null);\n} \n```\n\nIn the example the soundex representation of the `name` field is indexed.\n\nYou can of course index the soundex representation of whatever field you choose simply by passing the field as the parameter of the `soundex` function.\n\n`\"Smith\"` has the soundex value `S530`. So here is an example of using the view to find documents that have a name that sounds like `\"Smith\"`.\n\n```sh\nacurl \"https://$ACCOUNTNAME.cloudant.com/$DATABASE/_design/$DDOC/_view/find_name_by_soundex?key=\\\"S530\\\"&include_docs=true\"\n```\n\nThat request returns not only documents with the name `\"Smith\"` but also documents with similar names such as `\"Smythe\"` or `\"Schmidt\"`. \n\n",
    "url": "/2018/12/12/soundex-view.html",
    "tags": "soundex views",
    "id": "54"
  },
  {
    "title": "Count Distinct",
    "description": "Counting distinct values using MapReduce indexes",
    "content": "\n\n\n\nIn 2017 I blogged about creating [custom indexes outside of Cloudant](https://medium.com/ibm-watson-data-lab/custom-indexers-for-cloudant-6b7e65186db1) for problems that didn't fit Cloudant's indexing engine. One of those was the *count distinct problem*. \n\nImagine you are recording web events as people (and bots) interact with your website - events that look like this in JSON documents in a Cloudant database:\n\n```js\n{\n  \"_id\": \"96f898f0f6ff4a9baac4503992f31b01\",\n  \"_rev\": \"1-ff7b85665c4c297838963c80ecf481a3\",\n  \"path\": \"/blog/post-1.html\",\n  \"date\": \"2018-12-04\",\n  \"time\": \"17:15:59\",\n  \"mobile\": true,\n  \"browser\": \"Chrome\",\n  \"ip\": \"85.25.222.52\"\n}\n```\n\nWe want to report on the number of distinct IP addresses that have visited our site but as there are around four billion possible IP addresses, a *count distinct* operation can use vast amounts of memory. At the time, I proposed using [Redis's HyperLogLog](http://antirez.com/news/75) index instead - this achieves an *approximate* count using only a fixed handful of kilobytes of memory. \n\n![cache]({{< param \"image\" >}})\n\n> Photo by [Sylvanus Urban on Unsplash](https://unsplash.com/photos/XVYz_QeiEBw)\n\nAs of CouchDB 2.2, a HyperLogLog-based *reducer* is available to allow approximate count distinct operations to be performed in MapReduce indexes. Here's how it's done.\n\n## The Map function\n\nWe are going to create a MapReduce index whose *key* is the thing we want to count (the ip address). A JavaScript function is defined as the \"map\" part of the MapReduce to emit the required key:\n\n```js\nfunction(doc) {\n  emit(doc.ip)\n}\n```\n\nThis creates an index ordered by ip address - one item per document:\n\n```\n1.142.99.0\n77.152.2.245\n85.25.222.52\n```\n\n## The reducer\n\nThe new `_approx_count_distinct` reducer will reduce this index to approximate count of the distinct keys in the index:\n\n```\n{\"rows\":[\n{\"key\":null,\"value\":50133}\n]}\n```\n\nSo across the data set there are approximately 50133 distinct ip addresses (with a relative error of 2%).\n\nAt query time, you may also examine ranges of keys. Let's say you wanted to examine traffic from your own local 192.168.* network\n\n```\n#?startkey=\"192.168.0.0\"&endkey=\"192.169.0.0\"\n{\"rows\":[\n{\"key\":null,\"value\":99}\n]}\n```\n\n## Using the _approx_count_distinct in the dashboard\n\nThe MapReduce definition can be created in the Cloudant dashboard:\n\n![screenshot](/img/count-distinct.png)\n\nAdd your map function and choose the `_approx_count_distinct` reducer or enter `_approx_count_distinct` in the CUSTOM text box.\n\n## Conclusion\n\nThe `_approx_count_distinct` reducer has one job: to provide an estimate of distinct counts (with an accuracy of 2%) much more efficiently than getting a precise answer. It is used in the same way as the other built-in reducers `_sum`, `_count` and `_stats` but remember that it acts on the index's *key*, not the *value*.\n\n",
    "url": "/2018/12/14/Count-Distinct-with-Cloudant.html",
    "tags": "MapReduce Count",
    "id": "55"
  },
  {
    "title": "Natural Language Classification",
    "description": "Using AI to classify reviews as positive or negative.",
    "content": "\n\n\nIn this post we'll combine the Cloudant database with [IBM's Watson Natural Language Classifier (NLC) service](https://www.ibm.com/watson/services/natural-language-classifier/) to automatically determine whether reviews left on a website are positive or negative.\n\nImagine we are capturing product reviews on an e-commerce website and storing them as JSON in a Cloudant database:\n\n```js\n{\n  \"_id\": \"product885252:review1005252\",\n  \"date\": \"2019-01-26T15:22:41.000Z\",\n  \"product_id\": \"885252\",\n  \"product_name\": \"Gourmexia Smoothie Blender B662X\",\n  \"reviewer\": \"susan1982\",\n  \"review\": \"Very well made device. Works straight out of the box - really happy!\"\n}\n```\n\nAfter while, our database will hold plenty of reviews that our users can look through to help their buying decision. In order to be able to display the reviews on the website in two columns - positive and negative - we are going to need to classify whether a review is positive or negative and store that \"summary\" in the review JSON e.g.\n\n```js\n{\n  \"_id\": \"product885252:review1005252\",\n  \"date\": \"2019-01-26T15:22:41.000Z\",\n  \"product_id\": \"885252\",\n  \"product_name\": \"Gourmexia Smoothie Blender B662X\",\n  \"reviewer\": \"susan1982\",\n  \"review\": \"Very well made device. Works straight out of the box - really happy!\",\n  \"summary\": \"positive\"\n}\n```\n\nWe will need to either capture this new \"summary\" data from the user when they add to the review, or we could infer whether the review is positive or negative by using Artificial Intelligence. By training a Natural Language Processing (NLP) model with some reference reviews, it should be able to classify reviews it hasn't seen. \n\nNot having any experience of NLP, it's best to reach for a cloud service such as the [Watson Natural Language Classifier (NLC) service](https://www.ibm.com/watson/services/natural-language-classifier/).\n\n![banner]({{< param \"image\" >}})\n> Image by [Patrick Tomasso on Unsplash](https://unsplash.com/photos/Oaqk7qqNh_c)\n\n## Sign up\n\nSimply add a new NLC service in your [IBM Cloud](https://www.ibm.com/cloud/) dashboard by searching for \"natural language\":\n\n![signup](/img/nlc1.png)\n\nChoose a service name and pick the geography in which the service will run:\n\n![signup](/img/nlc2.png)\n\nMake a note of the `URL` and the `API Key` that is delivered with your provisioned service - we'll need them later.\n\nNext up, we need to train our model.\n\n## Training data\n\nTo train the service we need to feed it a CSV file containing examples of what a positive and negative review looks like. The CSV file contains two columns: the first with the review text and the second with the classification (positive/negative:\n\n```csv\nterrible product. Rubbish,negative\nThe best thing I've ever bought! Recommended!, positive\nI was really pleased with this device. Would buy again,positive\nTotal waste of time. Returned for a refund,negative\n```\n\nLuckily we have a body of existing review text we can extract from our Cloudant database. I used the [couchimport](https://www.npmjs.com/package/couchimport) command-line tool that allows us to export Cloudant data as a CSV file:\n\n```sh\ncouchexport --db reviews --delimiter \",\" > reviews.csv\n```\n\nWe can then load this data into a spreadsheet, trim off the columns so that we only have the review text and a column for positive/negative. \n\n![signup](/img/nlc3.png)\n\nThe NLC best-practice is to have fewer than sixty words of text in your training data - it would be good to remove lenghty expositions on product features from the review and leave in the parts where the user expresses an opinion on the product or service.\n\nThen comes the laborious process of populating the second column. This is a one-off process to build the data set that will train Watson to be able to classify future reviews automatically. Once complete, export the two column spreadsheet as a CSV file and we're ready to do the training. The NLC documentation recommends having 5-10 records of each classification in the training set.\n\n## Training\n\nThe Watson NLC service is all powered by an HTTP API, so we can do all the work from the command-line using *curl*. To save some typing, let's put our NLC service URL and API Key in environment variables:\n\n```sh\nexport URL=\"https://gateway.watsonplatform.net/natural-language-classifier/api\"\nexport APIKEY=\"9955882a8785grneYRRXpg5M3wBq1XwY7798gstcUm\n```\n\nWe're now ready to train our classifier using our `reviews.csv` file:\n\n```sh\ncurl -i --user \"apikey:$APIKEY\" \\\n-F training_data=@reviews.csv \\\n-F training_metadata=\"{\\\"language\\\":\\\"en\\\",\\\"name\\\":\\\"ReviewClassifier\\\"}\" \\\n\"$URL/v1/classifiers\"\n```\n\nThe above command gives the classifier a name and specifies the language that the review data is written in and uploads the `reviews.csv` file. (If you collect reviews in different languages, you'll need a classifier per language).\n\nIn response, the API returns you some JSON:\n\n```js\n{\n  \"classifier_id\" : \"9cccc4x485-nlc-1438\",\n  \"name\" : \"ReviewClassifier\",\n  \"language\" : \"en\",\n  \"created\" : \"2018-12-20T11:05:53.740Z\",\n  \"url\" : \"https://gateway.watsonplatform.net/natural-language-classifier/api/v1/classifiers/9cccc4x485-nlc-1438\",\n  \"status_description\" : \"The classifier instance is in its training phase, not yet ready to accept classify requests\",\n  \"status\" : \"Training\"\n}\n```\n\nThis indicates the `classifier_id` of the model we are training and states that training is now underway. Make a note of this, as we'll need it to query the model later. Let's store it in an environment variable so that we don't have to keep typing it:\n\n```sh\nexport CLASSIFIERID=\"9cccc4x485-nlc-1438\"\n```\n\nWe can check if the model training is complete by calling another API passing in the `classifier_id` from the first request:\n\n```sh\ncurl --user \"apikey:$APIKEY\" \\\n  \"$URL/v1/classifiers/$CLASSIFIERID\"\n```\n\nAfter a few minutes, the response to this API call will indicate that training is complete and we're ready to proceed to the next step.\n\n## Classifying new data\n\nNow the model is trained, we can pass it new data that it has never seen before and it will determine whether it is \"positive\" or \"negative\":\n\n```sh\ncurl -G --user \"apikey:$APIKEY\" \\\n\"$API/v1/classifiers/$CLASSIFIERID/classify\" \\\n--data-urlencode \"text=The item was terrible and did not work\"\n{\n  \"classifier_id\" : \"9cccc4x485-nlc-1438\",\n  \"url\" : \"https://gateway.watsonplatform.net/natural-language-classifier/api/v1/classifiers/9cccc4x485-nlc-1438\",\n  \"text\" : \"The item was terrible and did not work\",\n  \"top_class\" : \"negative\",\n  \"classes\" : [ {\n    \"class_name\" : \"negative\",\n    \"confidence\" : 0.9608260421942736\n  }, {\n    \"class_name\" : \"positive\",\n    \"confidence\" : 0.03917395780572638\n  } ]\n}\n```\n\nThe resulting data will show you the\n\n- `top_class` - which of the classifications is the best fit.\n- `classes` - a break down of the confidences that the model attributed to each class.\n\nFor our use-case we can simply take the `top_class` value and add it to our review JSON.\n\n## Plumbing it in\n\nAll that's left to do is to call the \"classify\" API endpoint upon the submission of a new product preview from the web front end and surface the \"top_class\" value in our stored JSON. The call can be made from anywhere, but you'll get the smallest latencies if you provision your Watson model as geographically close as possible to your app servers.\n\nThe HTTP is pretty easy to access from any programming language. Here's a Node.js function that classifies the passed-in string:\n\n```js\nconst request = require('request')\n\nconst classify = async (str) => {\n  const URL = process.env.URL\n  const APIKEY = process.env.APIKEY\n  const CLASSIFIERID = process.env.CLASSIFIERID\n  return new Promise((resolve, reject) => {\n    const r = {\n      method: 'get',\n      url: URL + '/v1/classifiers/' + CLASSIFIERID + '/classify',\n      qs: {\n        text: str\n      },\n      headers: {\n        Authorization: 'Basic ' + Buffer.from('apikey:' + APIKEY).toString('base64')\n      },\n      json: true\n    }\n    request(r, (e, r, b) => {\n      if (e) return reject(e)\n      resolve(b)\n    })\n  })\n}\n```\n\nwhich can be called as follows to extract the \"top_class\" value:\n\n```js\nconst c = await classify('this product is terrible')\nconsole.log(c.top_class)\n```\n\n## Useful Links\n\n- [Get Started with AI in 15 minutes](https://medium.com/ibm-watson/get-started-with-ai-in-15-minutes-28039853e6f3)\n- [IBM's Watson Natural Language Classifier (NLC) service](https://www.ibm.com/watson/services/natural-language-classifier/)\n- [IBM Cloudant](https://www.ibm.com/uk-en/cloud/cloudant)",
    "url": "/2019/01/18/Natural-Language-Classification.html",
    "tags": "CSV Watson",
    "id": "56"
  },
  {
    "title": "Partitioned Databases - Data Design",
    "description": "Designing your data for a partitioned database, including indexing and querying",
    "content": "\n\n\n> This is the second part of a series of posts on Partitioned Databases in Cloudant. [Part One][1], [Part Three][3] and [Part Four][4] may also be of interest.\n\nModelling data with a JSON document store is very different from modelling data in a relational database system. Generations of computer scientists have been taught how to [normalize data](https://en.wikipedia.org/wiki/Database_normalization) into tables, that is organising data into their own collections so that information is not repeated and relationships between collections are modelled with foreign keys.\n\nOrganising data in a \"NoSQL\" database like Cloudant makes \"joins\" prohibitively expensive and as a result the data design may involve the repetition of data. As with any data modelling exercise, there should be careful consideration of how the data is to be consumed - it is these \"access patterns\" that, together with database's best practise documentation, will influence how data is represented in the JSON documents.\n\nLet's imagine we are building an e-commerce store with the Cloudant service. To do so we are going to use three Cloudant databases to store the following objects:\n\n- `users` - a *user* is a registered user of the site with their contact details, delivery addresses and payment methods.\n- `products` - a *product* is a sellable product on our e-commerce store.\n- `orders` - an *order* represents the sale of one or more products to a user, together with how they paid and information from the courier to indicate dispatch and delivery.\n\nWe are going to design our data with the following goals:\n\n- Favouring small documents that tend to be added or removed (not updated over and over).\n- Leveraging Cloudant's *partitioned databases* feature where possible - two-part `_id` fields group documents that logically belong together on the same database shard.\n- Although there are relationships within the databases (i.e. an order represents a *user* buying one or more *products*), data is allowed to be duplicated where it makes sense to prevent the application having to make multiple round trips to the database to populate a web page.\n\n![cake](/img/toa-heftiba-239004-unsplash.jpg)\n> Photo by [Toa Heftiba on Unsplash](https://unsplash.com/photos/5-MNAjL81Iw)\n\n## The products database\n\nThe *products* database contains one document per sellable product. Products are categorised into a taxonomy of categories e.g. \"Home > Kitchen > Small Appliances\", a hierarchy which is surfaced in the website as a clickable menu of category names.\n\nEach product has a simple list of attributes, some of which are searchable (e.g. keywords) others (e.g. image) are used to render the front-end web page or mobile app.\n\nThe document's _id contains:\n\n- The product's place in the taxonomy joined by \"#\" characters e.g. Home#Kitchen#Small Appliances. This forms the partition key, so products in the same category are stored together.\n- A \":\" to indicate the divide between the partition key and the document key.\n- The document's *productid* following the colon. \n\nA sample document looks like this:\n\n```js\n{\n  \"_id\": \"Home#Kitchen#Small Appliances:1000042\",\n  \"type\": \"product\",\n  \"taxonomy\": [\"Home\",\"Kitchen\",\"Small Appliances\"],\n  \"keywords\": [\"Salter\",\"Scales\",\"Weight\",\"Digital\",\"Kitchen\"],\n  \"productid\": \"1000042\",\n  \"brand\": \"Salter\",\n  \"name\": \"Digital Kitchen Scales\",\n  \"description\": \"Slim Colourful Design Electronic Cooking Appliance for Home / Kitchen, Weigh up to 5kg + Aquatronic for Liquids ml + fl. oz. 15Yr Guarantee - Green\",\n  \"colours\": [\"red\",\"green\",\"black\",\"blue\"],\n  \"price\": 14.99,\n  \"delivery\"; 0,\n  \"image\": \"assets/img/0gmsnghhew.jpg\"\n}\n```\n\n## Users database\n\nThe *users* database stores details about the registered users of our store. We store:\n\n- 1 core document per user.\n- 1 additional document for each user's delivery addresses.\n\nThe document's `_id` fields are organised so that for a known `userid` (i.e. if a user is logged in and we know their `userid`), we can fetch everything we need to know about that user in one Cloudant Query because the data resides in the same partition. If we need to store other data about the user in the future e.g. payment methods, user preferences etc, further documents can be added following the same pattern.\n\nHere's some example documents:\n\n```js\n{\n  \"_id\": \"user19952622:auth\",\n  \"type\": \"user\",\n  \"userid\": \"user19952622\",\n  \"name\": \"Bob Smith\",\n  \"email\": \"bob.smith@aol.com\",\n  \"password\": \"1f6b5d0e151388786d3820cded9408e2\",\n  \"salt\": \"43614d9b1dec23da34a5b6f4eb71fb59\",\n  \"active\": true,\n  \"email_verified\": true\n}\n{\n  \"_id\": \"user19952622:delivery1\",\n  \"type\": \"userdelivery\",\n  \"userid\": \"user19952622\",\n  \"name\": \"home\",\n  \"address\": \"19 Front Street, Darlington, DL5 1TY\",\n  \"default\": true\n}\n{\n  \"_id\": \"user19952622:delivery2\",\n  \"type\": \"userdelivery\",\n  \"userid\": \"user19952622\",\n  \"name\": \"work\",\n  \"address\": \"22 Central Tower, Newcastle, NE1 4JD\",\n  \"default\": false\n}\n```\n\n## Orders database\n\nAn order consists of a number of documents written to the database when a shopping basket of items is paid for:\n\n- 1 core \"order\" document containing meta data.\n- a number of \"orderlineitem\" - one per item in the basket.\n- 1 \"payment\" document containing details of how the payment was made.\n- 1 dispatch document indicating which courier is deliverying the items.\n- 1 delivery document to indicate the arrival of the items.\n\nAs all of the documents reside in the same partition and can be fetched with a single query directed to the order's partition. In most cases, data is only written to Cloudant - the same document is not updated over and over as the order progresses.\n\n```js\n{\n  \"_id\": \"order555:order\",\n  \"type\": \"order\",\n  \"user\": \"Bob Smith\",\n  \"orderid\": \"order555\",\n  \"userid\": \"user19952622\",\n  \"basket\": [\"Salter - Digital Kitchen Scales\", \"Kenwood - Stand Mixer\"],\n  \"total\": \"214.98\",\n  \"deliveryAddress\": \"19 Front Street, Darlington, DL5 1TY\",\n  \"date\": \"2019-01-28T10:44:22.000Z\"\n}\n{\n  \"_id\": \"order555:item1\",\n  \"type\": \"orderlineitem\",\n  \"orderid\": \"order555\",\n  \"userid\": \"user19952622\",\n  \"productid\": \"1000042\",\n  \"name\": \"Salter - Digital Kitchen Scales\",\n  \"quantity\": 1,\n  \"unitPrice\": 14.99,\n  \"delivery\": 0\n}\n{\n  \"_id\": \"order555:item2\",\n  \"type\": \"orderlineitem\",\n  \"orderid\": \"order555\",\n  \"userid\": \"user19952622\",\n  \"productid\": \"88752\",\n  \"name\": \"Kenwood - Stand Mixer\",\n  \"quantity\": 1,\n  \"unitPrice\": 199.99,\n  \"delivery\": 0\n}\n{\n  \"_id\": \"order555:payment\",\n  \"type\": \"orderpayment\",\n  \"orderid\": \"order555\",\n  \"userid\": \"user19952622\",\n  \"paid\": \"true\",\n  \"provider\": \"PayPal\",\n  \"provider_ref\": \"PayPal161619885998772\",\n  \"date\": \"2019-01-28T10:45:27.000Z\",\n  \"total\": \"214.98\"\n}\n{\n  \"_id\": \"order555:dispatch\",\n  \"type\": \"orderdispatch\",\n  \"orderid\": \"order555\",\n  \"userid\": \"user19952622\",\n  \"dispatched\": \"true\",\n  \"date\": \"2019-01-28T16:02:00.000Z\",\n  \"courier\": \"UPS\",\n  \"courierid\": \"15125425151261289\"\n}\n{\n  \"_id\": \"order555:delivery\",\n  \"type\": \"orderdelivery\",\n  \"orderid\": \"order555\",\n  \"userid\": \"user19952622\",\n  \"delivered\": \"true\",\n  \"courier\": \"UPS\",\n  \"courierid\": \"15125425151261289\"\n}\n```\n\n## Querying the partitions\n\nThe database already indexes each document's `_id` field and with *partitioned databases*, documents belonging to the same partition reside on the same shard, making querying data in a single partition very efficient. We've made use of partitions to keep:\n\n- products belonging to the same category in a partition per category.\n- orders and the supplemental order data stored in a partition per order,\n- users and other user meta data is stored in a partition per user.\n\nHere's how we can use Cloudant Query to fetch documents from these partitions.\n\n### Fetch products belonging a category\n\nTo fetch the first one hundred products from the `Home#Kitchen#Small Appliances` category, we can simply send a blank *selector* to the partition's `_find` endpoint:\n\n```\nPOST /products/_partition/Home%23Kitchen%23Small%20Appliances/_find\n{\n  \"selector\": {},\n  \"limit\": 100\n}\n```\n\nAlternatively, the `_all_docs` endpoint can be used to fetch all the data from a partition\n\n```\nGET /products/_partition/Home%23Kitchen%23Small%20Appliances/_all_docs\n```\n\n### Searching within a known category\n\nIf we know the product category (perhaps the user has navigated to the \"Home#Kitchen#Small Appliances\" page), we can search for products within that partition by first defining a partitioned index and then querying it. A query aimed at a single partition and serviced by a pre-defined index constitutes best practice for a Cloudant database.\n\nWe can define a partial Cloudant Search index with a JavaScript function \n\n```js\nfunction(doc) {\n  if (doc.type == 'product') {\n    var words = doc.taxonomy.join(' ') + \n                doc.keywords.join(' ') + \n                doc.brand + ' ' +\n                doc.name + ' ' +\n                doc.description + ' ' +\n                doc.colours.join(' ');\n    index('default', words, { store: false, index: true });\n  }\n}\n```\n\nQueries can be directed to a single partition with:\n\n```sh\ncurl $URL/products/_partition/Home%23Kitchen%23Small%20Appliances/_design/mydesigndoc/_search/mysearchindex?q=salter+scales+red\n```\n\nNote that an index defined on a partitioned database is itself partitioned by default, although this behaviour can be overridden by supplying a `partitioned: false` flag at query-time to create a *global index*.\n\n### Fetch order data\n\nSimilarly, all of an order's details can be fetched by pulling all of the data from the order's partition:\n\n```\nGET /orders/_partition/order555/_all_docs\n```\n\nIf we only need the line items from the order we can be more specific:\n\n```\nPOST /products/_partition/order555/_find\n{\n  \"selector\": {\n    \"type\": \"orderlineitem\"\n  },\n  \"limit\": 100\n}\n```\n\nIf we only need the top-level meta data we can be even more selective: in fact, we don't even need to perform a query - we can simply fetch the document by its `_id`:\n\n```\nGET /orders/order555:order\n```\n\n### Fetch user data\n\nThe same technique can be used for the users database:\n\n```\nGET /users/_partition/user19952622/_all_docs\n```\n\nor we could fetch a user's default postal address with this query:\n\n```\nPOST /users/_partition/user19952622/_find\n{\n  \"selector\": {\n    \"type\": \"userdelivery\",\n    \"default\": \"true\"\n  },\n  \"fields\": [\"address\"]\n}\n```\n\n## Querying the whole data set - Indexing\n\nIn addition to the default primary index, we can define secondary indicies to instruct the database to create additional data structures, ordering the documents by different attributes. The secondary indicies can service additional access patterns we need for our application. Here's some examples:\n\n- Product Search - a user needs to be able to perform a free-text search for products across all categories or within a single category.\n- Orders by Customer - in the customer's dashboard they need to be able to view their order history.\n- Orders by Time - for reporting purposes, the business needs to be able to extract all of the orders made between two dates.\n- Sales Report - a total of paid-for orders needs to generated by year, year/month or year/month/day.\n- Authentication - for authentication we need to pull back the \"auth\" document for a user of a known email address.\n\nLet's dive into the detail of how we would achieve each of these use-cases using different features of Cloudant.\n\n### Indexing - Product Search\n\nIn order to allow the user to search products with a string of words e.g. \"Salter Scales Red\" or \"Digital Aquatronic\", we need to index each document's searchable words and employ a \"free text\" search engine. Cloudant has a free text search engine built in in the form of its [Cloudant Search](https://console.bluemix.net/docs/services/Cloudant/api/search.html#search) API. An index is defined with a JavaScript index definition function inside a design document. The function calls `index` to instruct the database to index selected data items. Here's an example:\n\n```js\nfunction(doc) {\n  if (doc.type == 'product') {\n    var words = doc.taxonomy.join(' ') + \n                doc.keywords.join(' ') + \n                doc.brand + ' ' +\n                doc.name + ' ' +\n                doc.description + ' ' +\n                doc.colours.join(' ');\n    index('default', words, { store: false, index: true });\n  }\n}\n```\n\nIn this case we are concatenating all the strings we want to be searchable and indexing them as the `default` text index, which provides a general-purpose search facility. We can query the index with a simple HTTP API call:\n\n```sh\ncurl $URL/_design/mydesigndoc/_search/mysearchindex?q=salter+scales+red\n```\n\nWith a little extra work we could:\n\n- Index strings separately which would allow searches to restricted to certain fields e.g. searching only product names. See [here](https://console.bluemix.net/docs/services/Cloudant/api/search.html#index-functions).\n- Indicate that we wish the database to count repeated strings in the result set or to group the price into multiple price brackets using *faceting*. See [here](https://console.bluemix.net/docs/services/Cloudant/api/search.html#faceting).\n- Query the data by weighting some fields more than others e.g. matches to the product name are worth more than matches to the product description. See [here](https://console.bluemix.net/docs/services/Cloudant/api/search.html#query-syntax).\n\nNote that in order to create an index that spans all the partitions, we need to supply `partitioned: false` in the design document that defines the index. See [documentation](https://console.bluemix.net/docs/services/Cloudant/guides/database_partitioning.html#creating-a-global-view-index)\n\n### Indexing - Orders by Customer\n\nIn our website's dashboard, we need to display a single user's order history. To service this access pattern, the *orders* database needs a *global* secondary index on the `userid` and `date` fields.\n\nTo create an index that only works on documents where `type=\"order\"` and is indexed by `userid` and `date` we POST the following JSON to the `/orders/_index` endpoint:\n\n```js\n{\n   \"index\": {\n     \"partial_filter_selector\": {\n       \"type\": \"order\"\n     },\n     \"fields\": [ \"userid\", \"date\" ]\n   },\n   \"ddoc\": \"orders-by-customer-index\",\n   \"type\": \"json\",\n   \"partitioned\": false\n}\n```\n\n- The `partial_filter_selector` is the query that is performed at index-time to determine whether a document belongs in the the index. Only the top-level order summary documents get past this filter.\n- The `fields` array is a list of documents fields to be indexed.\n- The `ddoc` is our reference for this index. It determines which design document the index definition will be stored in and allows us to select the use of this index at query-time.\n- The `type` specifies whether we want a \"json\" index powered by Cloudant's MapReduce engine or a \"text\" index powered by Cloudant Search.\n\nWe can then query the index by posting JSON to the `/orders/_find` endpoint:\n\n```js\n{\n   \"selector\": {\n      \"type\": \"order\",\n      \"userid\": \"user19952622\"\n   },\n   \"use_index\": \"orders-by-customer-index\"\n}\n```\n\n### Indexing - Orders by Time\n\nIn order to get all orders ordered by time, we need to create another *global* Cloudant Query index, again only including the core order documents and ordering by the `date` field:\n\n```js\n{\n   \"index\": {\n     \"partial_filter_selector\": {\n       \"type\": \"order\"\n     },\n     \"fields\": [\"date\" ]\n   },\n   \"ddoc\": \"orders-by-date\",\n   \"type\": \"json\",\n   \"partitioned\": false\n}\n```\n\nWe can then query the index by posting JSON to the `/orders/_find` endpoint, in this case to find orders occuring after in January 2019:\n\n```js\n{\n   \"selector\": {\n      \"type\": \"order\",\n      \"date\": {\n         \"$gte\": \"2019-01-01\",\n         \"$lt\": \"2019-01-02\"\n      }\n   },\n   \"use_index\": \"orders-by-date\"\n}\n```\n\n### Indexing - Sales Report\n\nPerforming aggregations for reporting purposes requires the use of Cloudant's MapReduce feature. A JavaScript function, embedded in a design document, is called by Cloudant for every document in the database. The function emits the keys/value pairs that define the ordering and grouping of the data and which fields are to be aggregated. Here's an example function that emits orders by year/month/day:\n\n```js\nfunction(doc) {\n  if (doc.type == \"order\") {\n    // turn the date string into a Date object\n    var date = new Date(doc.date);\n    // extract the date parts\n    var year = date.getFullYear();\n    var month = date.getMonth() + 1; // months go 0-11\n    var day = date.getDate();\n    // emit the key and value\n    emit([year,month,day], doc.total);\n  }\n}\n```\n\nBy choosing the `_sum` reducer, we instruct Cloudant to totalise the emitted value (`doc.total`). The MapReduce query engine allows data to be grouped by year/month/day, year/month, year or to group all the data to produce a grand total. In addition, the data can be filtered between `startkey` and `endkey` key values and the reducer can be switched off at query-time to just extract data between two dates.\n\nNote that in order to create an index that spans all the partitions, we need to supply `partitioned: false` in the design document that defines the index. See [documentation](https://console.bluemix.net/docs/services/Cloudant/guides/database_partitioning.html#creating-a-global-view-index)\n\n### Indexing - Authentication\n\nWhen a user is logging in, we can't do partitioned query on our `users` database because we don't yet know the user's `userid`. We have to create a secondary index on the user's email address and select a user record by the email field. We can reduce the index size by using a `partial_filter_selector` to only include the main user records that are active and have been verified:\n\n```js\n{\n   \"index\": {\n     \"partial_filter_selector\": {\n       \"type\": \"user\",\n       \"active\": true,\n       \"email_verified\": true\n     },\n     \"fields\": [\"email\" ]\n   },\n   \"ddoc\": \"users-by-email\",\n   \"type\": \"json\",\n   \"partitioned\": false\n}\n```\n\nTo find a document matching a supplied email address we can use the following query, only asking for the fields we need:\n\n```js\n{\n   \"selector\": {\n      \"type\": \"user\",\n      \"active\": true,\n      \"email_verified\": true,\n      \"email\": \"joe@aol.com\"\n   },\n   \"use_index\": \"users-by-email\",\n   fields: [\"userid\", \"password\", \"salt\"]\n}\n```\n\n## Summary\n\nThe Cloudant partial databases feature provides a step change in query performance for data designs that use the `_id` field to store a partition key and a document key. Careful data design can ensure that data that belongs together, and that you application expects to query in isolation, is stored in its own partition. Queries directed at a single partition only use fraction of the computing resource of a \"whole database\" query, resulting in faster performance and lower per-query pricing.\n\nOther use-cases and access patterns can be serviced by secondary indexes that span *all* the partitions.\n\n## Further reading\n\n- [Partitioned databases - Introduction][1]\n- [Partitioned databases - Data Design][2]\n- [Partitioned databases - Data Migration][3]\n- [Partitioned databases - Partition sizing][4]\n- [Partitioned databases - Cloudant Documentation][5]\n\n[1]: {{< ref \"2019-03-05-Partition-Databases-Introduction.md\" >}}\n[2]: {{< ref \"2019-03-05-Partition-Databases-Data-Design.md\" >}}\n[3]: {{ ref \"2019-03-05-Partition-Databases-Data-Migration.md\" >}}\n[4]: {{ ref \"2019-03-05-Partition-Databases-Sizing.md\" >}}\n[5]: https://cloud.ibm.com/docs/Cloudant/guides/database_partitioning.html#partitioned-databases\n",
    "url": "/2019/03/05/Partition-Databases-Data-Design.html",
    "tags": "Partitioned Indexing Query",
    "id": "57"
  },
  {
    "title": "Partitioned Databases - Data Migration",
    "description": "Copying data from a standard database to a partitioned database.",
    "content": "\n\n\n> This is the third part of a series of posts on Partitioned Databases in Cloudant. [Part One][1], [Part Two][2] and [Part Four][4] may also be of interest.\n\nCloudant's new *Partitioned Databases* feature allows a Cloudant database to be organised into partitions (blocks of data guaranteed to reside on the same database shard) by specifying a two part `_id` field consisting of the parition and document id e.g.\n\n```js\n{\n  \"_id\": \"US:52412\",\n  \"name\": \"Boston\",\n  \"state\": \"Massachusetts\",\n  \"country\": \"US\"\n  ...\n}\n```\n\n- \"US\" identifies the partition - all documents starting with \"US\" will be stored in the same physical database shard\n- \"52412\" is a unique identifier for the document. It must be unique within the partition.\n- the partition and document identifiers are separated by a \":\" character.\n\nA partitioned database allows queries limited to a single parition - such queries can be performed much more efficiently than whole-database (global) queries. \n\n![pie]({{< param \"image\" >}})\n> Photo by [Timothy Muza on Unsplash](https://unsplash.com/photos/Jw4rKiZFiSM)\n\n## Migrating to a partitioned database\n\nMigrating existing data over to a partitioned database will require creating a new database with the `partitioned=true` flag:\n\n```sh\ncurl -X PUT \"$URL/cities2?partitioned=true\"\n```\n\nThe new database will need to be populated with a copy of the original data, but with the new `partitionid:documentid` format.\n\ni.e we need to transform documents of this form:\n\n```js\n{\"_id\":\"52412\",\"country\":\"US\",name:\"Boston\"}\n```\n\ninto this form:\n\n```js\n{\"_id\":\"US:52412\",\"country\":\"US\",name:\"Boston\"}\n```\n\nto ensure that each city is placed in a per-country partition.\n\nCloudant's [Replication API](https://console.bluemix.net/docs/services/Cloudant/api/replication.html#replication) allows data to be copied or synced from a source database to a target database. [Filters](https://console.bluemix.net/docs/services/Cloudant/api/advanced_replication.html#filtered-replication) can be used to decide whether a document should be replicated or not but replication **doesn't** allow you to *transform* the data as it is replicated.\n\nThere is a neat trick that allows data to be moved from one database to another, while modifying the `_id` field (or any other field for that matter) in the process. To do this we are going to use two command-line tools:\n\n1. [couchbackup](https://www.npmjs.com/package/@cloudant/couchbackup) - allows CouchDB/Cloudant data be backed-up and stored as text files on your machine. It also comes with a tool to restore that data back to the database, or to a new empty database.\n2. [jq](https://stedolan.github.io/jq/) is a JSON processor used to format and modify JSON data structures.\n\nOur process is this:\n\n- export the source data using *couchbackup*.\n- transform the backed-up data using *jq*.\n- restore the transformed data to a new database using *couchrestore*.\n\nThe three actions can be achieved in a single command:\n\n```sh\ncouchbackup --db cities | jq -c '. | map(._id = .country + \":\" + ._id)' | couchrestore --db cities2\n```\n\nLet's break that down:\n\n`couchbackup --db cities` simply initiates a backup of the \"cities\" database. The data is output in batches of several hundred documents with one batch per line e.g.\n\n```js\n[{\"_id\":\"52412\",\"country\":\"US\",name:\"Boston\"},{\"_id\":\"781\",\"country\":\"UK\",name:\"Oxford\"}]\n[{\"_id\":\"152\",\"country\":\"IN\",name:\"Malaut\"},{\"_id\":\"782\",\"country\":\"PK\",name:\"Nārang\"}]\n```\n\nThe jq line `jq -c '. | map(._id = .country + \":\" + ._id)'` means:\n\n- `-c` - compact output (one array per line).\n- `.` - process the top level JSON object, in this case our array of cities.\n- ` | map()` - iterate over every item in the array\n- `._id = .country + \":\" + ._id` - sets the `_id` field to be the document's `country` attribute followed by a colon followed by the existing `_id`.\n\nThe result is a transformed array of countries:\n\n```\n[{\"_id\":\"US:52412\",\"country\":\"US\",name:\"Boston\"},{\"_id\":\"UK:781\",\"country\":\"UK\",name:\"Oxford\"}]\n[{\"_id\":\"IN:152\",\"country\":\"IN\",name:\"Malaut\"},{\"_id\":\"PK:782\",\"country\":\"PK\",name:\"Nārang\"}]\n```\n\nPiping this data to `couchrestore` populates the new database.\n\n## Other considerations\n\n- the choice of a partition key is very important. Consult [the documentation](https://console.bluemix.net/docs/services/Cloudant/guides/database_partitioning.html#what-makes-a-good-partition-key-) to pick a partition key that has many values, no hot spots and repeats throughout the data set.\n- Design documents that contain index definitions will need careful thought through. An index definition in a partitioned database is itself partitioned by default. Audit your indexes and try to make your most common access patterns are serviced by partitioned queries with an index, as these are the cheapest and most performant.\n- Global queries can still be used on a partitioned database but ensure they are backed by a matching `partitioned: false` index. \n\n## Further information\n\n- [Partitioned databases - Introduction][1]\n- [Partitioned databases - Data Design][2]\n- [Partitioned databases - Data Migration][3]\n- [Partitioned databases - Partition sizing][4]\n- [Partitioned databases - Cloudant Documentation][5]\n- [jq documentation](https://stedolan.github.io/jq/manual/)\n- [couchbackup](https://www.npmjs.com/package/@cloudant/couchbackup)\n\n[1]: {{< ref \"2019-03-05-Partition-Databases-Introduction.md\" >}}\n[2]: {{< ref \"2019-03-05-Partition-Databases-Data-Design.md\" >}}\n[3]: {{ ref \"2019-03-05-Partition-Databases-Data-Migration.md\" >}}\n[4]: {{ ref \"2019-03-05-Partition-Databases-Sizing.md\" >}}\n[5]: https://cloud.ibm.com/docs/Cloudant/guides/database_partitioning.html#partitioned-databases\n",
    "url": "/2019/03/05/Partition-Databases-Data-Migration.html",
    "tags": "Partitioned Migration",
    "id": "58"
  },
  {
    "title": "Partitioned Databases - Introduction",
    "description": "Organise data into partitions for a speed boost and cost savings.",
    "content": "\n\n\n> This is the first part of a series of posts on Partitioned Databases in Cloudant. [Part Two][2], [Part Three][3] and [Part 4][4] may also be of interest.\n\nCloudant has a new feature called *Partitioned Databases* which makes querying faster to execute while being cheaper per query request. In this article, we'll find out what Partitioned Databases are, how to set them up and how they work. Other posts provide a deep dive into [data modelling][2] and [data migration][3].\n\n![pie]({{< param \"image\" >}})\n> Photo by [Toa Heftiba on Unsplash](https://unsplash.com/photos/jjZsHhEtees)\n\n## Creating a partitioned database\n\nNormally a database is created with the `PUT /db` API endpoint:\n\n```sh\n$ curl -X PUT \"$URL/mydb\"\n{\"ok\": true}\n```\n\nTo specify that you want a *partitioned database*, simply add `?partitioned=true` to the URL:\n\n```sh\n$ curl -X PUT \"$URL/mypartitioneddb?partitioned=true\"\n{\"ok\": true}\n```\n\nThe `partitioned` flag can only be supplied when creating the database - an existing database cannot be transformed into a partitioned database after the fact.\n\n## The two-part _id field\n\nPartitioned database documents still have a unique `_id` field, but with a twist - the `_id` field is in two parts:\n\n1. The partition key\n2. The document key\n\nseparated by a \":\" character:\n\n![partition key](/img/partition1.png)\n\nThe whole `_id` must be unique in the database, but there can be many documents with the same partition key. Documents sharing a partition key are stored together on the same database *shard* making it inexpensive for the database to find and query them collectively.\n\nA database created with `?partitioned=true` will insist on a two-part `_id` field for each of its documents. If you attempt to create a document in a partitioned database with an invalid `_id`, your request will receive a `400 - Bad Request` response.\n\n## How Cloudant stores data in a partitioned database\n\nEach Cloudant service is hosted on multi-server distributed system. When you create a database, whether it be partitioned or not, the documents in the database are spread around the database cluster with the database being split into mulitple *shards*. In a *partitioned database* the partition key defines the location of the data in the cluster, with all documents that share the same partition key residing on the same shard (the same document ids in a non-partitioned database would be scattered around the cluster).\n\nAt query-time, we can direct queries to a single *partition*. As Cloudant knows on which shard each partition resides in the cluster, it need only query *that shard*. This saves computation expense, generates fewer network requests and the benefits are passed on to the caller in terms of performance and per-query cost.\n\n![partition query](/img/partition2.png)\n\nThe above diagram shows in simplistic terms that a full-database query hits every shard in the database, but a query directed to a single partition is only directed to one.\n\nIn reality, there is more complexity at play: multiple *copies* of each shard are distributed around the cluster, but for the purposes if this post it's a detail that needn't concern us. If you ignore the concept of shards and that there are multiple copies of each, you can imagine your database split into a number of logical partitions, determined by the value of the partition key you supply in the `_id` field.\n\n## What makes a good partition key?\n\nA good partition key should have:\n\n- Many values - lots of small partitions are better than a few large ones.\n- No hot spots - avoid designing a system that makes one partition handle a high proportion of the workload. If the work is evenly distributed around the partitions, the database will perform more smoothly.\n- Repeating - If each partition key is unique, there will be one document per partition. To get the best out of partitioned databases, there should be multiple documents per partition - documents that logically belong together.\n\nLet's look at some use-cases and some good and bad choices for a partition key.\n\n| Use-case                   | Description                 | Partition Key | Effectiveness                                                                                                     |\n|",
    "url": "/2019/03/05/Partition-Databases-Introduction.html",
    "tags": "Partitioned",
    "id": "59"
  },
  {
    "title": "Partitioned Databases - Sizing",
    "description": "Calculating the size of each partition in a partitioned database.",
    "content": "\n\n\n> This is the fourth part of a series of posts on Partitioned Databases in Cloudant. [Part One][1], [Part Two][2] and [Part Three][3] may also be of interest.\n\nChoosing a partition key for a partitioned Cloudant databases is about selecting an attribute that has:\n\n- Many values - lots of small partitions are better than a few large ones.\n- No hot spots - avoid designing a system that makes one partition handle a high proportion of the workload. If the work is evenly distributed around the partitions, the database will perform more smoothly.\n- Repeating - If each partition key is unique, there will be one document per partition. To get the best out of partitioned databases, there should be multiple documents per partition - documents that logically belong together.\n\nWe don't want a single partition to get too big (bigger than a handful of GB) or to have to handle a disproportionate amount of the database's workload. \n\n![cake]({{< param \"image\" >}})\n> Photo by [Annie Spratt on Unsplash](https://unsplash.com/photos/oudLkxglHuM)\n\nAlthough there are no API calls that can deliver a league table of the largest partitions in your database, it's simple enough to generate a [MapReduce view](https://console.bluemix.net/docs/services/Cloudant/api/creating_views.html#views-mapreduce-) to estimate the size of each partition. Here's how.\n\n## Creating a MapReduce view to estimate partition size\n\nMapReduce views are defined as JavaScript functions that are executed for each document in the database. We want to extract the *partition key* from each document's `_id` field and use that as the view's key.\n\n```js\nfunction (doc) {\n  // extract the document _id's partition key\n  var id = doc._id\n  var partition_key = id.slice(0, id.indexOf(':'))\n  \n  // calculate size of document\n  var docsize = JSON.stringify(doc).length\n  \n  // create view where the key is the partition_key and the value is the document size\n  emit(partition_key, docsize);\n}\n```\n\nThe view can be defined by pasting the JavaScript into the Cloudant dashboard (Design Documents `+` New View):\n\n![create view](/img/partitionsize1.png)\n\nIf we choose the `_sum` reducer, we get the sum of the document sizes by partition (not including conflicts or attachments) - if we choose the `_count` reducer we get the number of documents per partition.\n\n## Querying the view\n\nThe MapReduce view can be queried with `group_level=1` to generate totals/counts by the view's key (the partition key):\n\n```sh\ncurl '$URL/$DB/_design/partitions/_view/partitions?group_level=1'\n```\n\n![query view](/img/partitionsize2.png)\n\nTo sort the data set we can pipe this data into [jq](https://stedolan.github.io/jq/manual/#sort,sort_by(path_expression)) and sort the array by `value` (to get the biggest or fullest partitions last in the list):\n\n```sh\ncurl '$URL/$DB/_design/partitions/_view/partitions?group_level=1'| jq '.rows | sort_by(.value)'\n[\n  {\n    \"key\": \"user649\",\n    \"value\": 18370\n  },\n  {\n    \"key\": \"user278\",\n    \"value\": 18977\n  },\n  {\n    \"key\": \"user245\",\n    \"value\": 19048\n  },\n  ...\n  {\n    \"key\": \"user489\",\n    \"value\": 45121\n  },\n  {\n    \"key\": \"user755\",\n    \"value\": 46365\n  },\n  {\n    \"key\": \"user970\",\n    \"value\": 46513\n  }\n]\n```\n\n## Top ten partition count\n\nTo produce a \"top ten\" list of partitions by document count, we're going to need a custom \"reduce\" function.\n\n### Map\n\nOur map function  simply emits a value of \"1\" for each document against a key of the document's *partition key*:\n\n```js\nfunction (doc) {\n  var partition = doc._id.slice(0, doc._id.indexOf(':'))\n  emit(partition, 1);\n}\n```\n\n### Reduce\n\nThe custom reducer (written by [Adam Kocoloski](https://twitter.com/kocolosk)) pushes responsibility for the secondary aggregation of the list of partition document counts to the database by employing a second \"reduction\" phase. This is neat in terms of the form of the returned data but custom-reducers are difficult to write, debug and maintain and are not to be recommended for general use in Cloudant as they lead to poor database performance.\n\n```js\nfunction (keys, values, rereduce) {\n  var topTenPlusBoundaryKeys = function(partitions) {\n    // preserve boundary keys because we may not have the correct count for them yet\n    // not that it matters, but the array is reversed so these labels are correct\n    var first = partitions.pop();\n    var last = partitions.shift();\n\n    // sort the remaining entries by value\n    partitions.sort(function(p1, p2) { return p2.count - p1.count; });\n\n    // return the top ten partitions, plus the boundary partitions, all sorted\n    var topTen = partitions.slice(0, 10);\n    if(first) { topTen.push(first) };\n    if(last) { topTen.push(last) };\n    topTen.sort(function(p1, p2) { return p2.count - p1.count; });\n    return topTen;\n  };\n\n  if (rereduce) {\n    // account for boundary keys by summing over each partition\n    var totals = values.reduce(function(acc, currentVals) {\n      currentVals.forEach(function(elem) {\n        if(acc[elem.partition]) {\n          acc[elem.partition] += elem.count;\n        } else {\n          acc[elem.partition] = elem.count;\n        }\n      });\n      return acc;\n    }, {});\n    \n    // convert back into an Array with expected structure\n    var reduced = [];\n    for (var elem in totals) {\n      reduced.push({partition: elem, count: totals[elem]})\n    };\n    // sort in reverse order just to stay consistent with rereduce=false\n    // again, all that's required is to find the boundary keys\n    reduced.sort(function(p1, p2) {\n      if(p2.partition < p1.partition) {\n        return -1;\n      }\n      if(p1.partition < p2.partition) {\n        return 1;\n      }\n      return 0;\n    });\n\n    return topTenPlusBoundaryKeys(reduced);\n  }\n  else {\n    // compute the number of index entries per partition\n    var reduced = keys.reduce(function(output, currentKey, index) {\n      if(currentKey[0] == output[0].partition) {\n        output[0].count += values[index]\n      }\n      else {\n        output.unshift({partition: currentKey[0], count: values[index]})\n      }\n      return output;\n    }, [{partition: keys[0][0], count: 0}]);\n    \n    return topTenPlusBoundaryKeys(reduced);\n  }\n}\n```\n\n### Querying the top-ten view\n\nQuery the view without any parameters produces a list of the partitions with the most documents.\n\n```sh\ncurl '$URL/$DB/_design/partitions/_view/topten'\n{\"rows\":[\n{\"key\":null,\"value\":[\n{\"partition\":\"user970\",\"count\":73},\n{\"partition\":\"user755\",\"count\":73},\n{\"partition\":\"user489\",\"count\":71},\n{\"partition\":\"user396\",\"count\":71},\n{\"partition\":\"user12\",\"count\":71},\n{\"partition\":\"user816\",\"count\":70},\n{\"partition\":\"user113\",\"count\":70},\n{\"partition\":\"user9\",\"count\":69},\n{\"partition\":\"user815\",\"count\":69},\n{\"partition\":\"user662\",\"count\":69},\n{\"partition\":\"user1\",\"count\":50},\n{\"partition\":\"user999\",\"count\":44}]}\n]}\n```\n\n## Further information\n\n- [Partitioned databases - Introduction][1]\n- [Partitioned databases - Data Design][2]\n- [Partitioned databases - Data Migration][3]\n- [Partitioned databases - Partition sizing][4]\n- [Partitioned databases - Cloudant Documentation][5]\n\n[1]: {{< ref \"2019-03-05-Partition-Databases-Introduction.md\" >}}\n[2]: {{< ref \"2019-03-05-Partition-Databases-Data-Design.md\" >}}\n[3]: {{ ref \"2019-03-05-Partition-Databases-Data-Migration.md\" >}}\n[4]: {{ ref \"2019-03-05-Partition-Databases-Sizing.md\" >}}\n[5]: https://cloud.ibm.com/docs/Cloudant/guides/database_partitioning.html#partitioned-databases\n",
    "url": "/2019/03/05/Partition-Databases-Sizing.html",
    "tags": "Partitioned Sizing",
    "id": "60"
  },
  {
    "title": "Building a CRM System",
    "description": "Creating a customer relations system with a partitioned Cloudant database.",
    "content": "\n\n\nA Customer Relationship Management (CRM) system is simply a means of recording your business's relations with your customers. It may consist of:\n\n- a searchable database of your customers, with the customer name, description and contact details.\n- a time-ordered list of notes detailing the interactions you have had with the customer.\n- a list of contact details of the people  you have relationships with at that company.\n\nIt may also store other \"per customer\" objects e.g. sales made, appointments set or  anything else that you need to keep track of your relationship with the customer. CRM systems tend to be multi-user to allow several people in your organisation to manage the relationship with your customers.\n\n![crm1]({{< param \"image\" >}})\n> Photo by [Erol Ahmed on Unsplash](https://unsplash.com/photos/mfEeaOfacTQ)\n\nLots of people choose off-the-shelf CRM solutions such as [Salesforce](https://www.salesforce.com/uk/) or [Zoho](https://www.zoho.com/crm/) but in this article we're going to explore how you might build your own CRM system using a few cloud components:\n\n- a static website storing the CRM front-end using HTML, CSS and client-side JavaScript.\n- an HTTP API powered by a functions-as-a-service, serverless platform.\n- a NoSQL database to store the data as JSON documents.\n\nSpecifically, we're going to employ [IBM Cloud Functions](https://console.bluemix.net/openwhisk/) with its API Gateway integration and [IBM Cloudant](https://www.ibm.com/uk-en/cloud/cloudant) as the data store.\n\n![crm1](/img/crm1.png)\n\nAssuming we've set up an [IBM Cloud](https://www.ibm.com/cloud/) account and provisioned a [Cloudant service](https://www.ibm.com/cloud/cloudant) within it, the next thing we need to do is to create a Cloudant database with the `partitioned=true` flag (our Cloudant service's URL, containing the service credentials, is hidden in an environment variable `COUCH_URL`) using the command-line tool `curl`:\n\n```sh\ncurl -X PUT \"$COUCH_URL/crm?partitioned=true\"\n```\n\nThe database is called `crm` and is created with a simple `PUT` HTTP call.\n\n## Data Design\n\nBefore we starting writing code, it's worth thinking about how our data will look in the database, how data will be retrieved and to consider how the application will scale if and when the database grows to millions of documents. \n\nTo get the best out of Cloudant we're going to:\n\n- leverage Cloudant's new [Partitioned Databases](https://cloud.ibm.com/docs/services/Cloudant/guides?topic=cloudant-database-partitioning#partitioned-databases) feature that stores related data together in the same partition, where each document shares the same *partition key*.\n- generate indexes so that each query we need to do is backed by a suitable index.\n\nAs a user will typically be dealing with one customer at a time, we are going to store all of a company's data in a _per company_ partition. We will generate a *partition key* per customer, which will form the prefix of each document's key field (its `_id`). As well as a core document storing the company meta data, we can have any number of additional documents  stored alongside it in the same partition:\n\n- customer notes.\n- customer contact person.\n- customer links.\n\nEach document type is distinguished by the `type` field in the document and additional types can be added at a later date as our product is developed.\n\n### One document per customer\n\nThere will be a one document per company we are dealing with that stores the main company meta data (name, address, description etc). Its `_id` will be `<partition key>:0` so that when when querying a partition and sorting by `_id`, it appears first:\n\n```js\n{\n  \"_id\": \"3z86qZ0S:0\",\n  \"type\": \"company\",\n  \"name\": \"Andertons\",\n  \"address\": {\n    \"state\": \"Surrey\",\n    \"street\": \"58-59 Woodbridge Road\",\n    \"town\": \"Guildford\",\n    \"zip\": \"GU1 4RF\"\n  },\n  \"description\": \"Musical instrument supply.\",\n  \"ts\": \"2019-03-14T13:17:48.690Z\"\n}\n```\n\nIn this case the partition key (`3z86qZ0S`) is randomly generated although it could be any string that uniquely identifies a business e.g. an incrementing number (`42`), a telephone number (`01483456777`), a domain name (`andertons.co.uk`) or something else (`andertonsgu14rf`).\n\n### One document per customer note\n\nAn additional document will be added for each note we store against the customer:\n\n```js\n{\n  \"_id\": \"3z86qZ0S:zzyIvXhl2oWGGH37lp1V3riq2q3k7LkU\",\n  \"type\": \"note\",\n  \"title\": \"Notes from 14th March meeting\",\n  \"description\": \"Spoke to Lee about stocking...\",\n  \"ts\": \"2019-03-14T15:29:14.438Z\"\n}\n```\n\nEach note document shares a partition key with company it relates to (`3z86qZ0S`) and adds its own document key (`zzyIvX....`) to keep the `_id` unique. The `type` field identifies which object type is being stored. The `description` field is in [Markdown](https://daringfireball.net/projects/markdown/) format so that it can contain structure (bullet points, sections, hyperlinks etc) which can be rendered correctly on the HTML front end. \n\n### One document per contact person\n\n```js\n{\n  \"_id\": \"3z86qZ0S:zzyIvXop2eYZeD1B1Sl13tBZfD2nC3qN\",\n  \"type\": \"contact\",\n  \"name\": \"Lee Anderton\",\n  \"email\": \"lee@andertons.co.uk\",\n  \"ts\": \"2019-03-14T15:21:56.599Z\"\n}\n```\n\nEach contact document shares a partition key with company it relates to (`3z86qZ0S`) and adds its own document key (`zzyIvX....`) to keep the `_id` unique. The `type` field identifies which object type is being stored.\n\n### One document per customer link\n\n```js\n{\n  \"_id\": \"3z86qZ0S:zzyIvXpa15FhND3IJwL61gD5l40vK7Yn\",\n  \"type\": \"link\",\n  \"title\": \"YouTube channel\",\n  \"url\": \"https://www.youtube.com/channel/UCSNxIry_FPFcQDFRbi3VOAw\",\n  \"ts\": \"2019-03-14T15:21:09.312Z\"\n}\n```\n\nEach link document shares a partition key with company it relates to (`3z86qZ0S`) and adds its own document key (`zzyIvX....`) to keep the `_id` unique. The `type` field identifies which object type is being stored.\n\n## Thinking about sort order\n\nWhen looking at a customer in our CRM system we want a chronological list of events - we'd like all documents from the partition in \"newest first order\". Although we can sort data at query time, it's sometimes helpful to make the `_id` field sort in the order you need in most cases. I use the [kuuid](https://www.npmjs.com/package/kuuid) module to generate 32-character document keys that are unique and sort by time. The `kuuid.idr()` function makes ids that sort in *reverse* or \"newest first\" order.\n\nWe can see from the Cloudant dashboard the sort order in action. The documents sort into \"newest first\" order without any effort, with the exception of the core `type:company` document which is fixed to the top of the sorted list by virtue of its `<partition key>:0` _id.\n\n![sort order](/img/crm2.png)\n\nWhen a new note/contact/link object is added it will slip into *second place* in this list, behind the `type: company` document. This is exactly what we want to be able to render a company page in our CRM system - we can get the company meta data from the `<partition key>:0` document and the next most recent objects, all in one API call.\n\n## Indexing\n\nAs well as fetching data from a partition in newest first order, this application needs two indexes to service other access patterns:\n\n1. A global index that allows the user to search for companies by company name.\n2. A partitioned index that allows a single partition's documents to be fetched by \"type\".\n\n### Index - for searching by company name\n\nTo search across the the entire database, we need a _global index_ - we need to supply `partitioned:false` in the index definition. We can also keep the index small with a `partial_filter_selector`, an index-time filter that decides which documents make it into the index - we only want `type: company` documents. \n\n```sh\n# create a global index on \"name\" (for docs of type==compamy)\n\n# content type\nCT=\"Content-type: application/json\"\n\n# index definition in JSON:\n# - fields - index \"name\" as a string\n# - partial_filter_selector - only allow docs of type==company into the index\n# - type - \"text\" means use a Lucene-based free-text index\n# - ddoc - store all global indexes in a Design Document called \"global\"\n# - name - call this index \"byName\"\n# - partitioned - \"false\" means \"make this a global index\" (as opposed to a partitioned one)\nI='{\"index\":{\"fields\":[{\"name\":\"name\",\"type\":\"string\"}],\"partial_filter_selector\":{\"type\":\"company\"}},\"type\":\"text\",\"ddoc\":\"global\",\"name\":\"byName\",\"partitioned\":false}'\n\n# send index definition to Cloudant\ncurl -X POST -H \"$CT\" -d\"$I\" \"$COUCH_URL/crm/_index\"\n```\n\n### Index - for searching for document types within a partition\n\nWhen fetching documents of a single type relating to a single company, we can use a _partitioned_ index (`partitioned:true` is supplied here for clarity, but it is the default for a partitioned database). Directing a query to a single partition uses far fewer database resources making the query faster and cheaper to execute than a global query.\n\n```sh\n# create a partitioned index on \"type\"\n\n# index definition in JSON:\n# - fields - index \"type\" attribute\n# - type - \"json\" means use a MapReduce-based index\n# - ddoc - store all partitioned indexes in a Design Document called \"partitioned\"\n# - name - call this index \"byType\"\n# - partitioned - \"true\" means \"make this a partitioned index\" (as opposed to a global one)\nI='{\"index\":{\"fields\":[\"type\"]},\"type\":\"json\",\"ddoc\":\"partitioned\",\"name\":\"byType\",\"partitioned\":true}'\n\n# send index definition to Cloudant\ncurl -X POST -H \"$CT\" -d\"$I\" \"$COUCH_URL/crm/_index\"\n```\n\n## Building an API\n\nI like to create a separate IBM Cloud Function for each API method. My core API calls are:\n\n- `POST /crm/addcompany` - add a new company (in a new partition).\n- `POST /crm/addcontact` - add a contact to a company.\n- `POST /crm/addlink` - add a link to a company.\n- `POST /crm/addnote` - add a note to a company.\n- `GET /crm/fetch` - fetch the company and its recent history.\n- `GET /crm/fetchfilter` - fetch documents of a specified type for a company.\n- `GET /crm/search` - search for a company by name.\n\nEach of these API calls has [its own directory](https://github.com/glynnbird/crm/tree/master/api) in the source code, as each API call may have its own dependencies. Each API call has its own `./deploy.sh` script which uploads the Cloud Function and creates the API Gateway configuration around it. An overall [deploy.sh](https://github.com/glynnbird/crm/blob/master/api/deploy.sh) script sets up the Cloud Functions package and deploys all of the code in sequence.\n\nOnce deployed, the API can be secured to require an API key or an OAuth login from Google/Facebook/GitHub.\n\nLet's look at one API call in detail: `GET /crm/fetch`\n\n### GET /crm/fetch\n\nThis API call fetches a single business's core meta data document and that company's recent history in one API call. It's able to do so because:\n\n- each business's data is in a separate partition, so if you know the partition key, a query can be directed to *that partition* making for a faster and cheaper database operation.\n- the business's documents are ordered so that the *first* document contains the business details and the rest of the documents are in \"newest first\" time order.\n\nWe can simply call the `_all_docs` API call for the selected partition to fetch the data we need, using `limit` parameter to define how many documents to return.\n\nThis is the simplified source code ([original is here](https://github.com/glynnbird/crm/blob/master/api/fetch/fetch.js)):\n\n```js\nconst Cloudant = require('@cloudant/cloudant')\n\n// main\nasync function main(args) {\n\n  // connect to Cloudant\n  const cloudant = Cloudant({url: args.COUCH_URL})\n  \n  // custom request to fetch all the documents from\n  // a known partition partition (or the first 10)\n  const r = { \n    method: 'get',\n    path: encodeURIComponent('crm') + '/_partition/' + encodeURIComponent(args.partition) + '/_all_docs',\n    qs: {\n      limit: 10,\n      include_docs: true\n    }\n  }\n\n  // make the API call\n  const info = await cloudant.request(r)\n  return {\n    body: info,\n    statusCode: 200,\n    headers: { 'Content-Type': 'application/json' }\n  }\n}\n\nexports.main = main\n```\n\n## Accessing the API from a web app\n\nThe API Gateway supplied with IBM Cloud Function is CORS-enabled, so that any web app can make HTTP calls directly to the API Gateway URLs to access the API methods. I chose the [Fetch API](https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API) which makes client-side HTTP requests a breeze:\n\n```js\nconst CRMAPIsearch = async (term) => {\n  const data = await fetch(APIURL + '/search?query=' + encodeURIComponent(term))\n  const obj = await data.json()\n  return obj\n}\n```\n\nA [Vue.js](https://vuejs.org/)-based front-end allows the user to search for business and add notes/links/contacts by completing and submitting forms:\n\n![screenshot](/img/crm3.png)\n\nThe web app will win no prizes for user interface design but is works functionally!\n\nTo do:\n\n- make each objected deletable and editable.\n- add other objects - sale, appointment, complaint etc.\n- extend the global search facility to make the company address and description searchable.\n- provide a local, partitioned search facility to allow a single company's notes, contacts, links etc to be searched.\n- add authentication so that the API is only accessible to authorised users.\n\n## Try it yourself\n\nYou can try this yourself by [following the instructions in the project's README](https://github.com/glynnbird/crm/). \n\n## Further information\n\n- [Partitioned databases - Introduction][1]\n- [Partitioned databases - Data Design][2]\n- [Partitioned databases - Data Migration][3]\n- [Partitioned databases - Partition sizing][4]\n- [Partitioned databases - Cloudant Documentation][5]\n\n[1]: {{< ref \"2019-03-05-Partition-Databases-Introduction.md\" >}}\n[2]: {{< ref \"2019-03-05-Partition-Databases-Data-Design.md\" >}}\n[3]: {{ ref \"2019-03-05-Partition-Databases-Data-Migration.md\" >}}\n[4]: {{ ref \"2019-03-05-Partition-Databases-Sizing.md\" >}}\n[5]: https://cloud.ibm.com/docs/Cloudant/guides/database_partitioning.html#partitioned-databases\n",
    "url": "/2019/03/29/Building-a-CRM-System.html",
    "tags": "Partitioned Serverless",
    "id": "61"
  },
  {
    "title": "Time-series Data Storage",
    "description": "Storing and querying time-series data in Cloudant.",
    "content": "\n\n\nTime-series data is simply the recording of data points in time order such as:\n\n- Internet of Things (IoT) installations where sensors report readings periodically.\n- Monitoring user-interface interactions from a website to get a sense of which part of the page your users are interested in.\n- Storing the logs from a distributed computer system for diagnostics.\n- Recording the sales of products from a store.\n\nIn this post we'll examine options for storing time-series data in Cloudant.\n\n![clock]({{< param \"image\" >}})\n> Photo by [Sonja Langford on Unsplash](https://unsplash.com/photos/eIkbSc3SDtI)\n\n## The ever-growing data set\n\nIt's tempting to create a single, ever-growing Cloudant database to store your time-series data but it's worth taking time to think about how much data you'll be storing and to estimate the data size growth.\n\nLet's say we're storing IoT data in documents like this:\n\n```js\n{\n  \"_id\": \"e30bff4b286dd88c1cc178b7dbacaded\",\n  \"_rev\": \"1-8a10be69dc1e81531fee2c9ca2688508\",\n  \"type\": \"reading\",\n  \"device\": \"GE5521965B\",\n  \"reading\": 25.3,\n  \"units\": \"celcius\",\n  \"timestamp\": \"2019-03-29T09:28:31.229Z\"\n}\n```\n\nAt approximately 200 bytes per reading, each sensor would generate 6GB of raw data per year. Bear in mind that Cloudant stores data in triplicate and additional storage will be required for any secondary indexes you define. With only fifty or so sensors, your application could easily generate a terabyte of data per year.\n\nIn a lot of applications the most recent data, say the last year's, is of the most interest and older data can be archived to slower and cheaper storage methods such as [IBM Cloud Object Storage](https://www.ibm.com/uk-en/cloud/object-storage). Keeping your time-series store scalable and cost effective can become striking a balance between fast, primary data in a database and slower, cheaper data access in archive storage.\n\nPutting all of our data in a single, ever-growing Cloudant database is impractical because:\n\n- large monolithic Cloudant databases will become progressively less efficient unless _resharded_ to keep the shard sizes optimal.\n- mass deletion of individual Cloudant documents is a non-starter for archival of old data. A [Cloudant deletion](https://console.bluemix.net/docs/services/Cloudant/api/document.html#delete) leaves a _tombstone_ document behind - a document recording the last `_id`/`_rev` pair to ensure that the document remains deleted after replication.\n\nA common work-around is to adopt the \"timeboxed database\" pattern, as outlined in the next section.\n\n## The timeboxed database pattern\n\nThis pattern is pretty straightforward.\n\n1. Create a database per time-period (e.g month).\n2. Always write new data into _this month's_ database.\n3. Ahead of time, create a new empty database ready for next month's data. Don't forget to create any [Design Documents](https://console.bluemix.net/docs/services/Cloudant/api/design_documents.html#design-documents) needed to service queries in the new database.\n\n![ts1](/img/timeseries1.png)\n\nThis month's data can be queried from the _current_ database. Historical data can be queried by directing requests to the older databases that cover the time-range you are interested in.\n\n![ts2](/img/timeseries2.png)\n\nTo remove unwanted data, simply delete older monthly databases that are no longer needed. Deleting a whole database recovers all the data occupied by the database (JSON documents and associated indexes) cleanly and quickly.\n\n![ts3](/img/timeseries3.png)\n\nThis _write only_ approach combined with the deletion of older data by removing databases plays to Cloudant strengths.\n\n## Storing time/date in a time-series database\n\nThe choice of time and date format in your JSON document is important because JSON has no native date/time data type. This is explained in more detail [here]({{< ref \"/2018-05-22-Date-formats.md\" >}}) but the gist is:\n\n- use ISO-8601 format (\"2019-03-29T10:36:03.510Z\") to store time-sortable, human & machine readable time stamps.\n- use \"milliseconds since 1970\" format for easy date/time arithmetic.\n- use component day/month/year/hour/minute/second/millisecond/timezone pieces if you need to query by the pieces e.g. find events that occur when \"hour == 18\".\n- you can use more than one of the above.\n\n## Time-sortable ids\n\nIf we're using 32 characters to store a \"random\" `_id` field, then it would be handy if it sorted in chronological order in a time-series database. This technique is outlined [here]({{< ref \"/2018-08-24-Time-sortable-document-ids.md\" >}}).\n\nIn brief, the front of the `_id` is a time-sortable string and rest is random data. The `_id` field then sorts in approximate date/time order (to a precision of one second).\n\n![An example time-sortable _id value](/img/kuuid.png)\n\n## Using timeboxed databases that are also partitioned\n\nDepending on your use-case, it may also be useful to make your timeboxed databases _partitioned_ too, that is generated with the `?partitioned=true` flag to enable Cloudant's new [Partitioned Databases](/2019/03/05/Partition-Databases-Introduction.html) feature.\n\nThe choice of [partition key](https://console.bluemix.net/docs/services/Cloudant/guides/database_partitioning.html#what-makes-a-good-partition-key-) is beyond the scope of this post, but in a IoT application a _device id_ may be a good choice, as long as there are many devices generating data at a similar rate. \n\nHere's an example document that uses the full gamut of Cloudant time-series tricks: monthly databases, time-ordered ids and partitioned databases:\n\n```js\n{\n  \"_id\": \"GE5521965B:001h9p4Z0XeJzT0372TQ0hk2q83CLdqg\",\n  \"type\": \"reading\",\n  \"device\": \"GE5521965B\",\n  \"reading\": 25.3,\n  \"units\": \"celcius\",\n  \"timestamp\": \"2019-03-29T10:49:30.980Z\"\n}\n```\n\n- the `_id` field has a partition key of the reading's device id and a document key which is time-sortable, so the documents in a partition sort in date/time order.\n- the `timestamp` field is stored in ISO-8601 which is time-sortable, readable and parsable by MapReduce functions.\n- documents are are only ever written. Cloudant loves write-only design patterns.\n\nQueries for a single timebox *and* a single device (e.g. \"get me the latest 50 readings for device X\") can be directed at a single partition and will use only a fraction of the resources of a global query.\n\n```sh\n// Fetch the 50 latest readings for device GE5521965B\n// As the data is partitioned by device id, we can direct the query to a single partition.\n// The data within the partition is in date/time order so the last 50 records are the\n// ones we are interested in. descending=true reverses the order.\ncurl \"$URL/logs_2019_03/_partition/GE5521965B/_all_docs?limit=50&descending=true&include_docs=true\"\n```\n\n## Aggregating time-series data with MapReduce\n\nA very common use-case for a time-series database is to produce aggregations of the recorded data grouped by time. [Cloudant's MapReduce](https://console.bluemix.net/docs/services/Cloudant/api/creating_views.html#views-mapreduce-) allows the generation of materialized views of data by grouped, complex keys.\n\nCloudant supports the following reducers:\n\n- `_count` - row count, optionally grouped by keys.\n- `_sum` - the sum of numeric values, optionally grouped by keys.\n- `_stats` - totals and counts required for mean, variance and standard deviation calculation, optionally grouped by keys.\n- `_approx_count_distinct` - an approximate count of distinct keys.\n\nBefore we can reduce any data we need to generate the keys and values in the MapReduce index using a JavaScript function: \n\n```js\nfunction(doc) {\n\n  // convert timestamp to JavaScript Date object\n  var d = new Date(doc.timestamp)\n  \n  // calculate time components\n  var year = d.getFullYear()\n  var month = d.getMonth() + 1\n  var day = d.getDate()\n  var hour = d.getHours()\n  var minute = d.getMinutes()\n  \n  emit([year,month,day,hour,minute], doc.reading);\n}\n```\n\nThis JavaScript function is encoded into a Design Document and saved in the database:\n\n```js\n{\n  \"_id\": \"_design/aggregate\",\n  \"views\": {\n    \"byYMDHM\": {\n      \"reduce\": \"_stats\",\n      \"map\": \"function(doc) {\\n\\n  // convert timestamp to JavaScript Date object\\n  var d = new Date(doc.timestamp)\\n  \\n  // calculate time components\\n  var year = d.getFullYear()\\n  var month = d.getMonth() + 1\\n  var day = d.getDate()\\n  var hour = d.getHours()\\n  var minute = d.getMinutes()\\n  \\n  emit([year,month,day,hour,minute], doc.reading);\\n}\"\n    }\n  },\n  \"language\": \"javascript\",\n  \"options\": {\n    \"partitioned\": false\n  }\n}\n```\n\n- the `_id` of the Design Document always starts with \"_design/\" and this identifier is used to query the view later.\n- the `views` object contains one entry per MapReduce view. The name (\"byYMDHM\") is used to query the view later.\n- the `map` function is the JavaScript function as a JSON-encoded string.\n- the `reduce` string is the name of the built-in reducer.\n- the `language` is \"javascript\" by default.\n- the `options.partitioned` flag being `false` ensures that a _global_ view is calculated in this _partitioned_ database. If omitted, the view would only be queryable on a _per partition_ basis.\n\nOnce the Design Document is saved in the database, the view builds asynchronously - for a small database it will appear that the view is ready \"immediately\", but for larger datasets it may take some time before the view is ready to query.\n\nThe view is queried using the design document name and view name used earlier:\n\n```sh\ncurl \"$URL/logs_2019_03/_design/aggregate/_view/byYMDHM?group_level=3\"\n```\n\n- `group_level` - is used to define how many elements of an array-based key are grouped together e.g. when emitting `[year,month,day,hour,minute]` as the key, a `group_level=3` means \"group by year, month & day\".\n- `starkey`/`endkey` - can be used to limited the return data to keys between the supplied keys.\n- `reduce` (true/false) - can be used to \"switch off\" the reducer step at query-time\n- See [Using Views](https://cloud.ibm.com/docs/services/Cloudant/api?topic=cloudant-using-views#using-views) in the Cloudant documentation for more details\n\nNote that when using monthly timeboxed databases, you may not need to emit the `year` and `month` values as all documents would emit the same first two keys, although it does help when combining aggregated results from multiple databases.\n\n## Out-of-the box time-series solutions\n\nIf you don't want to build your own time-series solution, then the [Watson Internet Of Things](https://www.ibm.com/uk-en/internet-of-things) platform can run the whole thing as-a-service including device registration, data storage, analytics, monitoring and archival.\n\n## Further reading\n\n- [Cloudant documentation](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-overview#overview)\n- [Partitioned Databases Introduction]({{< ref \"/2019-03-05-Partition-Databases-Introduction.md\" >}})\n- [Time-sortable ids]({{< ref \"/2018-08-24-Time-sortable-document-ids.md\" >}})\n- [Date formats in Cloudant]({{< ref \"/2018-05-22-Date-formats.md\" >}})\n",
    "url": "/2019/04/08/Time-series-data-storage.html",
    "tags": "Time-series Partitioned",
    "id": "62"
  },
  {
    "title": "Analysing Backups with SQL Query",
    "description": "Exploring your Cloudant backups using IBM SQL Query",
    "content": "\n\n\nIn [this post]({{< ref \"/2019-04-08-Time-series-data-storage.md\" >}}) we discussed storing time-series data in time-boxed Cloudant databases to allow recent data to be stored in Cloudant and older data to be archived and deleted from Cloudant. In this post we'll examine how to query data that has been archived and backed up to Object Storage using the IBM SQL Query service.\n\nObject storage is much cheaper per gigabyte than a database, is endlessly extensible and makes a great choice for storing backups and archived data. \n\n![mountains]({{< param \"image\" >}})\n> Photo by [Federico Bottos on Unsplash](https://unsplash.com/photos/Vtii5AjWNIQ)\n\n## Pre-requisites\n\n1. An [IBM Cloud](https://www.ibm.com/cloud/) account.\n2. An [IBM Cloudant](https://www.ibm.com/cloud/cloudant) database service provisioned within your IBM Cloud account. Make a note of the URL of your Cloudant service - we'll need that later. Make sure you have a database created in your Cloudant service containing \n3. An [IBM Cloud Object Storage](https://www.ibm.com/uk-en/cloud/object-storage) service provisioned within your IBM Cloud account. Create a _bucket_ within this service making a note of the region and bucket name you have chosen.\n4. An [IBM Cloud SQL Query](https://www.ibm.com/uk-en/cloud/sql-query) service provisioned within your IBM Cloud account.\n\n![schematic](/img/sqlquery1.png)\n\n## Getting the Cloudant data onto Object Storage\n\nWe'll be using the official [couchbackup](https://www.npmjs.com/package/@cloudant/couchbackup) tool to extract data from Cloudant. It is a Node.js application and is therefore installed using the `npm` command-line utility:\n\n```sh\nnpm install -g @cloudant/couchbackup\n```\n\nWe can then store our Cloudant URL as an environment variable:\n\n```sh\nexport COUCH_URL=\"<your Cloudant URL goes here>\"\n```\n\nAssuming we're backing up a database called `mydb` we would invoke `couchbackup` like so:\n\n```sh\n# make a shallow backup of the database\ncouchbackup --db mydb --mode shallow > mydb.txt\n```\n\nThis creates a file called `mydb.txt`. Each line of that file is an array of JSON documents - one per document in the database:\n\n```js\n[{\"_id\":\"1\"...},{\"_id\":\"2\"...},{\"_id\":\"1\"...},....]\n[{\"_id\":\"101\"...},{\"_id\":\"102\"...},{\"_id\":\"103\"...},....]\n[{\"_id\":\"201\"...},{\"_id\":\"202\"...},{\"_id\":\"203\"...},....]\n...\n```\n\nOur first job is to remove the Design Documents from this file. The IBM SQL Query service is looking for flat JSON documents - those with top-level key values and not embedded objects. We can use the [jq](https://stedolan.github.io/jq/manual/) tool to filter out documents whose `_id` field starts with `_design`:\n\n```sh\n# remove design documents from the backup file\ncat mydb.txt | jq -c 'map(select(._id | startswith(\"_design\") | not))' > mydb2.txt\n```\n\nThe above `jq` command is fed the file containing multiple arrays of objects. For each array, it iterates through each document and excludes those that are design documents. The output is fed to a second text file: `mydb2.txt`.\n\nThe final step is to extract each line of the file (each array of documents) to its own file. This is easily achieved with the `split` command-line tool:\n\n```sh\n# split each line of mydb2.txt into its own file\nsplit -l 1 mydb2.txt \n# remove the original files\nrm mydb.txt mydb2.txt\n```\n\nWe should now have a number of files (`xaa`, `xab`, `xac` etc) which can be bulk uploaded into a Cloud Object Storage bucket, in this case into a bucket called `mydb`.\n\n## Explore the data\n\nNow we have our JSON files in Cloud Object Storage, accessing them from IBM Cloud SQL Query is q breeze. We can access the data as if it were a SQL database:\n\n```sql\n-- explore the data\nSELECT *\nFROM cos://eu-gb/mydb/ STORED AS JSON\n```\n\n- `SELECT *` - choose all columns.\n- `FROM cos://eu-gb/mydb/` - from Cloud Object storage, in the `eu-gb` geography from the `mydb` bucket.\n- `STORED AS JSON` - defines the file format of the stored data.\n\nThe output data is previewed in a table in the web page which is helpful for exploring the data; looking at the data's column headings and data types.\n\n![sql query](/img/sqlquery2.png)\n\n## Converting to Parquet\n\nBefore we do any serious data exploration, it's useful to do one last data conversion: to convert the JSON data into \"Parquet\" format. Parquet is a compressed, column-oriented format that comes from the Hadoop project. Data in this format is ideal for ad-hoc querying as it has a smaller data size and faster query performance.\n\nConverting data to Parquet is a one-off operation that can be performed by executing a single Cloud SQL Query statement:\n\n```sql\n-- convert all data in 'mydb' bucket into Parquet format\n-- to be stored in the 'mydbparquet' bucket\nSELECT *\nFROM cos://eu-gb/mydb/ STORED AS JSON\nINTO cos://eu-gb/mydbparquet/ STORED AS PARQUET\n```\n\nIn this case, I'm keeping my raw JSON data and the derived Parquet files in different Cloud Object storage buckets for neatness.\n\nThe result of this operation isn't a table of data in the UI, it simply writes its results to the destination bucket. Exploring the bucket in the Cloud Object Storage UI reveals the resultant objects:\n\n![sql query](/img/sqlquery3.png)\n\nNotice that my 3.2MB of data is now only occupying 782KB of space as a Parquet file. \n\nWe can now direct queries towards the Parquet version of our data:\n\n```sql\n-- get top 10 populated countries below the equator\nSELECT country, SUM(population)\nFROM cos://eu-gb/mydbparquet/jobid=59463bdb-cd55-4df3-af7f-fcdf75fcccc8 STORED AS PARQUET\nWHERE latitude < 0\nGROUP BY 1\nORDER BY 2 DESC\nLIMIT 10\n```\n\n![sql query](/img/sqlquery4.png)\n\n## Doing more with Cloud SQL Query\n\nOnce you've got the hang of exporting Cloudant data to Object storage, converting it to Parquet and analyzing it with Cloud SQL Query, the whole operation can be scripted to run automatically. Your timeboxed Cloudant data can be archived periodically and added to your *data lake* for analysis.\n\nCloud SQL Query can do much more than outlined in this article: check out its [SQL Reference](https://cloud.ibm.com/docs/services/sql-query?topic=sql-query-sql-reference#sql-reference), [Geospatial toolkit](https://cloud.ibm.com/docs/services/sql-query?topic=sql-query-geo-functions#geo-functions) and [Timeseries](https://cloud.ibm.com/docs/services/sql-query?topic=sql-query-timeseries-functions#timeseries-functions) functions.\n\n## Further readings\n\n- [Cloud SQL Query documentation](https://cloud.ibm.com/docs/services/sql-query?topic=sql-query-overview#overview)\n- [Cloud Object Storage documentation](https://cloud.ibm.com/docs/services/cloud-object-storage?topic=cloud-object-storage-about-ibm-cloud-object-storage#about-ibm-cloud-object-storage)\n- [Cloudant documentation](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-overview#overview)\n",
    "url": "/2019/04/23/Analysing-Cloudant-Backups-with-SQL-Query.html",
    "tags": "COS SQL",
    "id": "63"
  },
  {
    "title": "Optimal Cloudant Indexing",
    "description": "Getting away with fewer indexes",
    "content": "\n\n\nTraditionally, this is taught at Cloudant data modelling class:\n\n- Design JSON representations of the objects that exist in your application - products, users, orders etc.\n- If necessary, create multiple document \"types\" in the same database - use a field in the document to differentiate one from the other e.g. `\"type\": \"product\"`, `\"type\": \"user\"` etc\n- Build indexes on the fields which you are going to query against e.g. if I am going to search products by category in product name order, I'm going to need an index on `category` & `name`. If I'm going to query my users by their email address, I'm going to need an index on `email`.\n- Build app.\n- Profit.\n\nThis is a perfectly valid approach: different document types *can* co-exist in the same database in Cloudant and we can define a handful of easily identifiable indexes per document type to service the access patterns our application needs. \n\nThe downside is that we may have many indexes, let's say three indexes per document type or nine in total - one for each use-case. The more indexes your database has, the more computation, IO and disk space will be consumed creating and updating them.\n\nTo make an index apply to only one of our document types in a scenario where many document types co-exist in the same database, we will have to setup a [partial_filter_selector](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-query#creating-a-partial-index) in the index definition - Cloudant churns through each document in the collection for each index definition and applies the _partial filter selector_ to see if it qualifies to be recorded in each index. It works, but it's an extra level of complication and to build an index on *products*, Cloudant would also have to churn through (and ignore) all of the *users* and *orders* in the same database.\n\n![]({{< param \"image\" >}})\n> Photo by [Edgar Chaparro on Unsplash](https://unsplash.com/photos/AAHxr7ZvCLs?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n\nIs there a way to manage with fewer indexes and without a partial filter selector? There is - read on.\n\n## General purpose indexes\n\nIn our data design process we're going to include a handful of additional fields in our `product` document that are explicitly _indexed fields_:\n\n```js\n{\n  \"_id\": \"998877\",\n  \"type\": \"product\",\n  \"mpn\": \"25888529952\",\n  \"category\": \"Celebration cakes\",\n  \"name\": \"Chocolate Cake 400g (Serves 6)\",\n  \"keywords\": [\"chocolate\",\"cake\",\"sophies\", \"birthday\", \"occasion\", \"walnut\"],\n  \"brand\": \"Sophies's Snacks\",\n  \"cost\": 3.50,\n  \"vat_rate\": 0.20,\n  \"vat\": 0.70,\n  \"total\": 4.20,\n  \"description\": \"Chocolate Cake layered with chocolate buttercream and topped with chocolate buttercream and walnuts\",\n  \"fat\": 21.9,\n  \"saturates\": 7.6,\n  \"energy_kj\": 176,\n  \"energy_kcal\": 422,\n  \"i1\": \"\",\n  \"i2\": \"\",\n  \"i3\": \"\"\n}\n```\n\nThe top-most fields are our normal data fields. The last three `i1`, `i2` and `i3` are fields designated to store data that is to be indexed, in the order that data is to be retrieved. \n\nWe want to service the following three use-cases on our `\"type\": \"product\"` documents with our indexes:\n\n1. Retrieve products by category name, ordered by order total, ascending or descending.\n2. Fetch a product by its manufacturer's part number\n3. Search for products by keywords\n\nSo when we store a document we also populate `i1`, `i2` and `i3` like so:\n\n```js\n  .\n  .\n  .\n  \"i1\": \"category#celebration cakes#4.20\",\n  \"i2\": \"25888529952\",\n  \"i3\": \"chocolate cake sophies celebration walnut\"\n}\n```\n\n- `i1` stores several strings, delimited by `#` - the word `catgegory`, the category name and the product price.\n- `i2` stores the manufacturer's part number as a string.\n- `i3` stores the keywords we wish to be searchable for this product.\n\nIf we query against one of these three `i*` fields, we can achieve performant searching and retrieve in the order we intended. We'll deal with querying later - first let's look at our `user` document:\n\n```js\n{\n  \"_id\": \"22815e7bb9\",\n  \"type\": \"user\",\n  \"name\": \"Bob Harry\",\n  \"email\": \"bob.harry66632@aol.com\",\n  \"date_joined\": \"2018-05-25\",\n  \"verified\": true,\n  \"password_hash\": \"edf944a17a1e0b5b5b9109cdb3486ee6\",\n  \"salt\": \"a2f67b329fbba8e859d7ea2dd4aa8dce\",\n  \"active\": true,\n  \"i1\": \"\",\n  \"i2\": \"\",\n  \"i3\": \"\"\n}\n```\n\nOur use-cases this time are:\n\n1. Fetch a user by their email address.\n2. Get users in the order that they signed up.\n3. Search for users by their name.\n\nSo we populate `i1`, `i2` and `i3` like so for users:\n\n```js\n  .\n  .\n  .\n  \"i1\": \"user#bob.harry66632@aol.com\",\n  \"i2\": \"2018-05-25\",\n  \"i3\": \"bob harry\"\n}\n```\n\n- `i1` stores two strings, delimited by `#` - the word `user`, and the user's email address.\n- `i2` stores the user's sign-up date.\n- `i3` stores the keywords we wish to be searchable for this user.\n\n\nFor our `order` document we are storing:\n\n```js\n{\n  \"_id\": \"8866743\",\n  \"type\": \"order\",\n  \"user_id\": \"22815e7bb9\",\n  \"date\": \"2019-05-03\",\n  \"products\": [\n    { \n      \"product_id\": \"998877\", \n      \"name\": \"Chocolate Cake 400g (Serves 6)\",\n      \"cost\": 3.50,\n      \"vat\": 0.70,\n      \"total\": 4.20\n    }\n  ],\n  \"total_vat\": 0.70,\n  \"total\": 4.20,\n  \"paid\": true,\n  \"i1\": \"\",\n  \"i2\": \"\",\n  \"i3\": \"\"\n}\n```\n\nOur use-cases for orders are:\n\n1. Fetch all orders for a known `user_id` ordered by `date`\n2. All orders, ordered by `date`\n3. n/a\n\nSo we populate `i1`, `i2` and `i3` like so:\n\n```js\n  .\n  .\n  .\n  \"i1\": \"order#22815e7bb9#2019-05-03\",\n  \"i2\": \"2018-05-25\",\n  \"i3\": \"\"\n}\n```\n\n- `i1` stores three strings, delimited by `#` - the word \"order\", the order's `user_id` and the order `date`.\n- `i2` stores the order `date`.\n- `i3` is a blank string because we don't need a third index for orders.\n\n\n## Indexing\n\nThis the part where we can make some savings.\n\nInstead of creating many indexes for each document type's needs, we simply need create an index on each of our `i*` fields:\n\n```\nPOST /mydb/_index HTTP/1.1\n\nContent-Type: application/json\n{\n  \"index\": {\n    \"fields\": [\"i1\"]\n  },\n  \"name\" : \"i1\",\n  \"type\" : \"json\"\n}\n```\n\n```\nPOST /mydb/_index HTTP/1.1\n\nContent-Type: application/json\n{\n  \"index\": {\n    \"fields\": [\"i2\"]\n  },\n  \"name\" : \"i2\",\n  \"type\" : \"json\"\n}\n```\n\n```\nPOST /mydb/_index HTTP/1.1\n\nContent-Type: application/json\n{\n  \"index\": {\n    \"fields\": [\n      { \"name\": \"type\", \"type\": \"string\"},\n      { \"name\": \"i3\", \"type\": \"string\"}\n    ]\n  },\n  \"name\" : \"i3\",\n  \"type\" : \"text\"\n}\n```\n\nThe `i1` & `i2` indexes are `type=json` indexes which can be used for selection and range query while preserving the order of the data in the indexed fields. The `i3` index is a `type=text` index because it's used for free-text searching.\n\nIn your application you may need different combinations of \"json\"/\"text\" indexes and you may need more than three `i` fields, or perhaps fewer.\n\n## Querying\n\nQuerying against our `i1`/`i2`/`i3` indexes is little more opaque compared with composing JSON selector queries against named fields, but it does have a certain purity. We have to know which `i` field to use to answer a use-case and the form of the data we stored in that field. I like to keep a table that relates the index to the object type, and explains the form of data stored:\n\n|         | i1                               | i2            | i3         |\n|",
    "url": "/2019/05/10/Optimal-Cloudant-Indexing.html",
    "tags": "Indexing Query",
    "id": "64"
  },
  {
    "title": "Scheduled Cloudant Backups",
    "description": "Backing up your data periodically using Kubernetes.",
    "content": "\n\n\nThe [Couchbackup](https://www.npmjs.com/package/@cloudant/couchbackup) project provides a simple command-line tool to backup a Cloudant database to a file. From there it can uploaded to [IBM Cloud Object Storage](https://www.ibm.com/cloud/object-storage) for archival. \n\n```sh\n# backup a single database to a file\ncouchbackup --db mydatabase > mydatabase.txt\n\n# copy the file to Object Storage\naws --endpoint-url=$ENDPOINT \\\n   s3 cp mydatabase.txt s3://mycloudantbackups/\n```\n\nAs well a command-line tool,  *couchbackup* can also be used as a Node.js library so that you can script your own workflows to suit your application. The couchbackup source code has [example scripts](https://github.com/cloudant/couchbackup/tree/master/examples) which you can use as a basis of your own.\n\nIn this post we'll create a Kubernetes service that runs periodically to backup a Cloudant database. This can be scheduled to run at hourly, daily, weekly or monthly intervals, for example.\n\n![container]({{< param \"image\" >}})\n> Photo by [Boba Jovanovic on Unsplash](https://unsplash.com/photos/FtRkRespN24)\n\n## Pre-requisites\n\n\n1. An [IBM Cloud](https://www.ibm.com/cloud) account.\n2. An [IBM Cloudant](https://www.ibm.com/uk-en/cloud/cloudant) service with a database containing documents you wish to backup.\n3. An [IBM Cloud Object Storage](https://www.ibm.com/cloud/object-storage) service with set of [HMAC credentials](https://cloud.ibm.com/docs/services/cloud-object-storage/hmac?topic=cloud-object-storage-service-credentials).\n4. An [IBM Kubernetes Service](https://www.ibm.com/uk-en/cloud/container-service).\n\n## Running the backup script on your machine\n\nUsing the [s3-backup-stream.js](https://github.com/cloudant/couchbackup/blob/master/examples/s3-backup-stream.js) as a basis for our script, we want our code to:\n\n- Backup a single database to object storage without storing local disk first.\n- Receive all of its connection configuration (Cloudant URL & Object Storage bucket, URL and authentication keys) as environment variables.\n\nThe source code of our script is [here](https://github.com/glynnbird/scheduledcloudantbackup.git) and can be executed on your machine as follows:\n\n```sh\n# Clone the repository.\ngit clone https://github.com/glynnbird/scheduledcloudantbackup.git\n\n# Install this project's dependencies.\ncd scheduledcloudantbackup\nnpm install\n\n# Create environment variables containing credentials.\n# Replace <placeholders> with your data. \n# Swap \"export\" for \"set\" on a Windows machine.\nexport COUCH_URL=\"https://<username>:<password>@<host>/<db>\"\nexport COS_ENDPOINT_URL=\"https://s3.<region>.cloud-object-storage.appdomain.cloud/\"\nexport COS_BUCKET=\"<bucket>\"\nexport AWS_ACCESS_KEY_ID=\"<access key id>\"\nexport AWS_SECRET_ACCESS_KEY=\"<secret access key>\"\n\n# Run the backup.\nnpm run start\n```\n\n## Running as a Docker container\n\nThis script can also be run as a Docker container. If you have [Docker](https://www.docker.com/) installed on your machine, run the following commands:\n\n```sh\n# Build the image.\ndocker build -t scheduledcloudantbackup .\n\n# Spin up a container based on this image passing in \n# environment variables with connection details.\ndocker run \\\n  --env COUCH_URL=\"$COUCH_URL\" \\\n  --env COS_ENDPOINT_URL=\"$COS_ENDPOINT_URL\" \\\n  --env COS_BUCKET=\"$COS_BUCKET\" \\\n  --env AWS_ACCESS_KEY_ID=\"$AWS_ACCESS_KEY_ID\" \\\n  --env AWS_SECRET_ACCESS_KEY=\"$AWS_SECRET_ACCESS_KEY\" \\\n  scheduledcloudantbackup\n```\n\nThe values beginning with `$` are replaced with the environment variables we created in the previous step.\n\n## Running automatically in Kubernetes\n\nTo run the backup unattended every hour, we need to schedule this process to run periodically. IBM's Kubernetes service has a \"cron job\" function that is designed for just this purpose. First we need to register our Docker image with the IBM Kubernetes service:\n\n1. Sign up for a [IBM Kubernetes](https://www.ibm.com/uk-en/cloud/container-service) service.\n2. Follow the [instructions](https://cloud.ibm.com/docs/containers?topic=containers-cs_cli_install) on how to install the `ibmcloud` command-line tools.\n3. Authenticate. e.g. `ibmcloud login`\n4. Set the Kubernetes target. e.g. `ibmcloud ks region-set eu-gb`. I chose the `eu-gb` region - [others are available](https://cloud.ibm.com/docs/containers?topic=containers-regions-and-zones). \n5. Download the cluster config e.g. `ibmcloud ks cluster-config scheduledcloudantbackup`\n6. Log into the container registory service e.g. `ibmcloud cr login`\n7. Create a namespace e.g. `ibmcloud cr namespace-add scheduledbackup`\n8. Build an image e.g. `ibmcloud cr build -t uk.icr.io/scheduledbackup/backup:1 .`\n\nSo we now have an image called `uk.icr.io/scheduledbackup/backup:1` in the IBM image registry - we next need to trigger it to run periodically with a [Kubernetes cron job](https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/).\n\nA \"cron job\" is a term taken from the Unix world - it means running a task periodically, say every hour. It has a syntax specifying the interval at which your code is to be run, in our case we want `0 * * * *` (see [\"Crontab Guru\"](https://crontab.guru/#0_*_*_*_*) for an explanation of that syntax) which means \"at the top of every hour\".\n\nOur cron job definition resides in a \"cronjob.yml\" file which tells the Kubernetes cluster which image to spin up, at what interval and the environment variables it runs with. **Edit the `cronjob.yml` to configure your Cloudant service and Object Storage environment variables before running**:\n\n```sh\nkubectl create -f cronjob.yml\n```\n\nNow wait until the top of the hour for the backup process to begin. When it's complete, you should see time-stamped backup in your Object Storage bucket!\n\n## Links\n\n- [Kubernetes CronJobs](https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/)\n- [Source code of this utility](https://github.com/glynnbird/scheduledcloudantbackup)\n- [couchbackup](https://github.com/cloudant/couchbackup)\n",
    "url": "/2019/05/16/Scheduled-Cloudant-Backups.html",
    "tags": "Backup COS Kubernetes",
    "id": "65"
  },
  {
    "title": "Partitioned Databases and Node.js",
    "description": "Using Partitioned Databases with the Node.js library.",
    "content": "\n\n\nThe Cloudant database has four supported client libraries: [Node.js](https://github.com/IBM/cloudant-node-sdk), [Java](https://github.com/IBM/cloudant-java-sdk), [Go](https://github.com/IBM/cloudant-go-sdk)and  [Python](https://github.com/IBM/cloudant-python-sdk). In this post, we'll see examples on how the Node.js library can be used with the new [Partition Databases]({{< ref \"/2019-03-05-Partition-Databases-Introduction.md\" >}}) feature.\n\n\nHere's a table of all the functions we'll be using:\n\n| Operation                         | Raw API Call                                                | Node.js function call |\n|",
    "url": "/2019/05/24/Partitioned-Databases-with-Cloudant-Libraries.html",
    "tags": "Node.js Partitioned",
    "id": "66"
  },
  {
    "title": "Paging with Bookmarks",
    "description": "Using bookmarks to page through results sets.",
    "content": "\n\n\nImagine you are creating a web application showing a set of search results, whether they be books, actors or products in your store. As the user scrolls to the bottom of the search results, another page of matches is appended to the bottom. This is known as an \"infinite scroll\" design pattern and allows the user to endlessly scroll through a large data set with ease, while only fetching a smaller batches of data from the database each time.\n\nIt is this sort of access pattern that Cloudant *bookmarks* are built for. Here's how it works:\n\n- Your application performs a search on a Cloudant database e.g. \"find me the first ten cities where the country is 'US'\".\n- Cloudant provides an array of ten Cloudant documents and a *bookmark* - an opaque key that represents a pointer to the next documents in the result set.\n- When the next set of results is required, the search is repeated but in addition to the query, the bookmark from the first response is also sent to Cloudant in the request.\n- Cloudant replies with the second set of documents an another bookmark which can be used to get a third page of results.\n- Repeat! \n\n![pic]({{< param \"image\" >}})\n\nLet's see how we would do that with code.\n\n## Cloudant Query \n\nFirst let's perform a search for all the cities in the USA. We're using [Cloudant Query](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-query) so the operation is specified as a block of JSON:\n\n```js\n{\n  \"selector\": {\n    \"country\": \"US\"\n  },\n  \"limit\": 5\n}\n```\n\nand is passed to Cloudant using the [/db/_find](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-query#selector-syntax) API endpoint: \n\n```sh\ncurl -X POST \\\n      -H 'Content-type: application/json' \\\n      -d '{\"selector\":{\"country\":\"US\"},\"limit\":5}' \\\n      \"$URL/cities/_find\"\n{\n  \"docs\":[\n    {\"_id\":\"10104153\",\"_rev\":\"1-32aab6258c65c5fc5af044a153f4b994\",\"name\":\"Silver Lake\",\"latitude\":34.08668,\"longitude\":-118.27023,\"country\":\"US\",\"population\":32890,\"timezone\":\"America/Los_Angeles\"},\n    {\"_id\":\"10104154\",\"_rev\":\"1-125f589bf4e39d8e119b4b7b5b18caf6\",\"name\":\"Echo Park\",\"latitude\":34.07808,\"longitude\":-118.26066,\"country\":\"US\",\"population\":43832,\"timezone\":\"America/Los_Angeles\"},\n    {\"_id\":\"4046704\",\"_rev\":\"1-2e4b7820872f108c077dab73614067da\",\"name\":\"Fort Hunt\",\"latitude\":38.73289,\"longitude\":-77.05803,\"country\":\"US\",\"population\":16045,\"timezone\":\"America/New_York\"},\n    {\"_id\":\"4048023\",\"_rev\":\"1-744baaba02218fd84b350e8982c0b783\",\"name\":\"Bessemer\",\"latitude\":33.40178,\"longitude\":-86.95444,\"country\":\"US\",\"population\":27456,\"timezone\":\"America/Chicago\"},\n    {\"_id\":\"4048662\",\"_rev\":\"1-e95c97013ece566b37583e451c1864ee\",\"name\":\"Paducah\",\"latitude\":37.08339,\"longitude\":-88.60005,\"country\":\"US\",\"population\":25024,\"timezone\":\"America/Chicago\"}\n  ],\n  \"bookmark\": \"g1AAAAA-eJzLYWBgYMpgSmHgKy5JLCrJTq2MT8lPzkzJBYqzmxiYWJiZGYGkOWDSyBJZAPCBD58\"\n}\n```\n\nNotice as well as an array of `docs`, Cloudant also returns a `bookmark` which we save for the next request. When we need page two of the results we repeat the query, passing Cloudant the bookmark from the first response:\n\n```sh\ncurl -X POST \\\n      -H 'Content-type: application/json' \\\n      -d '{\"selector\":{\"country\":\"US\"},\"limit\":5,\"bookmark\":\"g1AAAAA-eJzLYWBgYMpgSmHgKy5JLCrJTq2MT8lPzkzJBYqzmxiYWJiZGYGkOWDSyBJZAPCBD58\"}' \\\n      \"$URL/cities/_find\"   \n{\n  \"docs\":[\n    {\"_id\":\"4049979\",\"_rev\":\"1-1fa2591477c774a07c230571568aeb66\",\"name\":\"Birmingham\",\"latitude\":33.52066,\"longitude\":-86.80249,\"country\":\"US\",\"population\":212237,\"timezone\":\"America/Chicago\"},\n    {\"_id\":\"4054378\",\"_rev\":\"1-a750085697685e7bc0e49d103d2de59d\",\"name\":\"Center Point\",\"latitude\":33.64566,\"longitude\":-86.6836,\"country\":\"US\",\"population\":16921,\"timezone\":\"America/Chicago\"},\n    {\"_id\":\"4058219\",\"_rev\":\"1-9b4eb183c9cdf57c19be660ec600330c\",\"name\":\"Daphne\",\"latitude\":30.60353,\"longitude\":-87.9036,\"country\":\"US\",\"population\":21570,\"timezone\":\"America/Chicago\"},\n    {\"_id\":\"4058553\",\"_rev\":\"1-56100f7e7742028facfcc50ab6b07a04\",\"name\":\"Decatur\",\"latitude\":34.60593,\"longitude\":-86.98334,\"country\":\"US\",\"population\":55683,\"timezone\":\"America/Chicago\"},\n    {\"_id\":\"4059102\",\"_rev\":\"1-612ae37d982dc71eeecf332c1e1c16aa\",\"name\":\"Dothan\",\"latitude\":31.22323,\"longitude\":-85.39049,\"country\":\"US\",\"population\":65496,\"timezone\":\"America/Chicago\"}\n  ],\n  \"bookmark\": \"g1AAAAA-eJzLYWBgYMpgSmHgKy5JLCrJTq2MT8lPzkzJBYqzmxiYWhoaGIGkOWDSyBJZAO9qD40\"\n}\n```\n\nThis time we get the next five cities and a new bookmark ready for the next request.\n\nIt's the same story when using one of the Cloudant libraries to do this. First make the initial request:\n\n```js\n  const q = {\n    selector: {\n      country: 'US'\n    },\n    limit: 5\n  }\n  const data = await db.find(q)\n  // { docs: [ ... ], bookmark: '...' }\n```\n\nWe feed the bookmark from the first response into the second request for the next page of results:\n\n```js\n  const q = {\n    selector: {\n      country: 'US'\n    },\n    limit: 5,\n    bookmark: 'g1AAAAA-eJzLYWBgYMpgSmHgKy5JLCrJTq2MT8lPzkzJBYqzmxiYWJiZGYGkOWDSyBJZAPCBD58'\n  }\n  const data = await db.find(q)\n  // { docs: [ ... ], bookmark: '...' }\n```\n\n## What about Cloudant Search?\n\nPagination works in the same way for [Cloudant Search](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-search) queries. Pass the `bookmark` parameter in the URL for GET requests or in the JSON body for POSTed requests. e.g.\n\n```sh\ncurl \"$URL/cities/_search/search/_search/freetext?q=country:US&bookmark=g1AAAAA-eJzLYW\"\n```\n\nSee [the documentation](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-search#query-parameters-search) for further details.\n\n## What about MapReduce views?\n\nMapReduce views do not accept a `bookmark`. Use the [skip and limit](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-using-views) to page through results.\n\n## Can I jump straight to page X of the results?\n\nNo. Bookmarks only make sense to Cloudant if they came from the previous page of results. If you need page 3 of the results, you need to fetch pages 1 & 2 first.\n\n## What happens if I supply an incorrect bookmark?\n\nCloudant will respond with an `HTTP 400 Bad Request { error: 'invalid_bookmark'}` response if you supply an invalid bookmark. Remember you don't need a bookmark for the first search in a sequence.\n\n## What happens if I change the query?\n\nYou must keep the same query (the same selector in Cloudant Query or the same \"q\" in Cloudant Search) to get the next page of results. If you change the query, you may get an empty result set in reply.\n\n ",
    "url": "/2019/05/31/Paging-with-Cloudant-Bookmarks.html",
    "tags": "Search Query",
    "id": "67"
  },
  {
    "title": "Replicating from a Query",
    "description": "Taking a subset of data offline and writing your own replicator.",
    "content": "\n\n\nCloudant and CouchDB's replication protocol allows documents to be copied from a *source* database to a *target* database with the minimum of fuss. This unlocks a wealth of use-cases:\n\n* Multiple copies of a database across geographies to allow for disaster recovery, or to be used in a high-availability configuration.\n* Data being replicated from a cloud-based primary to an on-premise backup.\n* Mobile devices taking a copy of data, taking if offline and modifying it before replicating it back to the original source database.\n\nThe first two use-cases usually require *the whole source database* to be replicated, but the third use-case is different and is an example of what I call *the roving engineer problem*.\n\n![van]({{< param \"image\" >}})\n> Photo by [Cosmin Gurau on Unsplash](https://unsplash.com/photos/i30E515-f-k)\n\n## The roving engineer problem\n\nLet's say our company installs electric vehicle charging points. We have hundreds of mobile engineers and a database of all of our company's appointments stored in Cloudant - one document per appointment:\n\n```js\n{\n  type: \"appointment\",\n  status: \"booked\",\n  address: {\n    street: \"25 Front Street\",\n    town: \"Pleasantville\",\n    state: \"CA\",\n    zip: \"152422\"\n  },\n  model: \"KwikCharge 550S\",\n  customer: \"Tom Pickering\",\n  date: \"2019-06-30\",\n  engineer: \"501\"\n}\n```\n\nWe want to replicate each engineer's appointment documents to their mobile device so that they can take them with them (sometimes into remote areas without cellular coverage). During the installation, the engineers will add more fields to the document to record the serial number of the device they installed, the time they arrived & left etc. Mobile apps built with [PouchDB](https://pouchdb.com/) allow Cloudant documents to reside inside a web browser (or embedded in a browser inside a [Apache Cordova](https://cordova.apache.org/) container). PouchDB speaks the same replication protocol as Cloudant, so moving data about is a breeze.\n\n![roving1](/img/roving1.png)\n\nAt the end of the day, or at any time when the engineer's mobile is connected, they can sync back the data to the Cloudant service.\n\nLet's look at the ways this design pattern can be achieved.\n\n## Option 1: Filtered replication\n\nA Cloudant replication between source and target databases can be [filtered](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-advanced-replication#filtered-replication), that is a JavaScript function decides whether each document makes it through to the target or is rejected. This can, in theory, be used to implement the roving engineer problem. We create a filter function:\n\n```js\nfunction(doc, req) {\n  // ensure this is an appointment that has been booked\n  // and is assigned to our engineer (passed in at replication-time)\n  if (doc.type === 'appointment' && \n      doc.status === 'booked' && \n      doc.engineer === req.query.engineer) {\n    return true\n  } else {\n    return false\n  }\n}\n```\n\nWe can then trigger a replication from PouchDB, specifying that we wish to filter the documents with our engineer's id. \n\nThis would work, but it's slow. The first replication would ask Cloudant to trawl through every document in the database to find the handful of documents that pass the filter function's test. Subsequent replication's could be faster, because they wouldn't have to start from the beginning of the source database's changes feed, but the process would still have to sift through every engineer's documents in search of those pertaining to one engineer.\n\n## Option 2: One database per user\n\nThe *one database per user* pattern is common in CouchDB/Cloudant circles because it allows a clean replication target/source that only contains one user's documents. \n\n![roving2](/img/roving2.png)\n\nThis is much better than filtering the replication from one large database but does have some drawbacks:\n\n- The server-side databases gets bigger and bigger with each day. In this use-case only the new or recent documents are of interest to the mobile client, but replication is going to bring *all* the documents down to the mobile database.\n- With application data split out into hundreds of separate databases, it is difficult to query the data as a whole. One solution is to replicate the small databases into a large central database which can then be used for reporting. \n\n## Option 3: Replicating from a query\n\nIdeally, we'd like to keep a single server-side database to make reporting easy but we also want to keep the data size small on the mobile side. Starting with a blank mobile-side database every day we would like to replicate only the data that is of interest to one user in restricted time-window. Effectively, we would like to copy data based on a query and push the matching records to the clean, mobile database. This isn't a built-in feature of Cloudant but is easily achievable with a few steps:\n\n1. Perform the query to get the list of document ids to copy. In our case, it's going to be a handful of documents. Note that the query should be [backed by a suitable index]({{< ref \"/2018-07-12-CloudantFundamentals-Indexing.md\" >}}) to keep performance snappy.\n2. Fetch the document bodies *including the replication history* matching the document ids from step 1.\n3. Write the documents to the mobile-side database, retaining the replication history\n\n![roving2](/img/roving3.png)\n\nHere's how this achieved with client-side JavaScript:\n\n```js\n \n // clean out the client-side database\n const DBNAME = 'apointments'\n var db = new PouchDB(DBNAME)\n await db.destroy()\n db = new PouchDB(DBNAME)\n \n // remote database\n var cloudantDB = new PouchDB('https://user:pass@host/dbname')\n \n // perform the query on the server-side database\n // Find documents which are appointments that have been booked\n // for our engineer id and that are in the future.\n const q = {\n   selector: {\n      type: 'appointment',\n      status: 'booked',\n      engineer: '501',\n      date: {\n        '$gte': '2019-06-13'\n      }\n   },\n   fields: ['_id']\n }\n const data = await cloudantDB.find(q)\n \n // swap _id for id\n data.docs.map((obj) => { obj.id = obj._id; delete obj._id })\n\n // fetch the document bodies with revision history\n const bulkGetData = await cloudantDB.bulkGet({docs: data.docs, revs: true})\n let docs = []\n for(var i in bulkGetData.results) {\n    docs.push(bulkGetData.results[i].docs[0].ok)\n }\n \n // write to local PouchDB\n const response = await db.bulkDocs(docs, {new_edits: false})\n```\n\nMuch of this algorithm is formatting the output of one API call so that it's suitable for the input of another but there's a few import points:\n\n- `bulkGet` fetches a list of documents at once by their `_id` while also returning the documents' revision histories, which is important if you are writing your own replicator.\n- `bulkDocs` writes documents in bulk with the `new_edits: true` flag that specifies that no new revision tokens are to be generated - each document's revision tree is grafted on the local database's existing revision tree.\n\nPouchDB replication uses `bulkGet`/`bulkDocs` in its internal implementation, the difference between our custom replicator and standard replication is that ours is triggered from a smaller subset of data - the output of the `find` query operation.\n\nIn our use-case, the data set being \"replicated\" is small (a handful of documents). If your use-case requires larger volumes of documents to be copied, then you'll need to repeat the bulkGet/bulkDocs step, in batches of, say, a hundred documents at a time, paging through repeated query result sets.\n\n## Closing the loop\n\nWhichever option we pick to copy data to our mobile device, our engineers now have their documents in PouchDB and can take them to their remote site to do their installation work. They can modify the appointment documents to add data they collect on site, creating new revisions of the documents in PouchDB as they do so. When the engineer is ready to upload their work, a simple [db.replicate.to(cloudantDB)](https://pouchdb.com/api.html#replication) call is all that is required to push those revisions to Cloudant.\n\nIf there's a possibility that an server-side appointment document could have been modified while the engineer was out and about, then it's essential to check for and repair [document conflicts]({{< ref \"/2015-01-12-Introduction-to-Conflicts-Part-One.md\" >}}). When two documents are modified in different ways by disconnected replicas of a database, Cloudant doesn't discard data on sync, it keeps the clashing versions as _conflicts_ in its revision tree. It is your application's responsibility to detect and tidy up conflicts.\n\nIf you want to avoid conflicts altogether, then the next section shows you how.\n\n## Avoiding conflicts during replication\n\nConflicts can be avoided altogether by keeping data separate:\n\n- The appointment documents, which are generated server-side when an installation job is requested contain the meta data about the job. Ensure that only the server-side process can modify these documents.\n- The engineer-generated data should be stored in new, separate documents to store the data collected on site. The documents can reference the original appointment, but the original appointment document remains untouched.\n\n```js\n{\n  type: \"installation\",\n  appointmend_id: \"31VHCIFR4SCKFTMV\",\n  engineer_id: \"501\",\n  arrived: \"2019-06-30 10:30:00\",\n  left: \"2019-06-30 11:56:00\",\n  model: \"KwikCharge 550S\",\n  serial: \"885527727725772\",\n  notes: \"Installed to the left of garage.\",\n  tested: true,\n  current: 13,\n  car: \"Nissan Leaf\"\n}\n```\n\nBy keeping the appointment data separate from the installation data, that is the data generated server-side from the data generated when on-site, we can completely avoid the possibility of conflicts and the necessity of having to detect and repair them.\n\n## Conclusion\n\nThe *roving engineering problem* can be solved a number of ways with Cloudant, but a great solution is writing your own \"Replication from a Query\" algorithm so that only the documents you need are copied to a clean mobile database. \n",
    "url": "/2019/06/21/Replicating-from-a-Query.html",
    "tags": "Replication Query",
    "id": "68"
  },
  {
    "title": "Fuzzy search using Double Metaphone",
    "description": "Using the Double Metaphone algorithm to find words that sound alike.",
    "content": "\n\n\n## Introduction\n\nIn an earlier [article]({{< ref \"/2018-12-12-soundex-view.md\" >}}) I explained how to do a fuzzy search for documents that contain words that sound like some other given word. \nThe technique I described there uses a view that implements the Soundex algorithm. The aim of the Soundex algorithm is to encode words alike that sound alike so that they can be matched despite minor differences in spelling.\nSoundex was invented before the invention of the electronic computer and is fairly simple. \n\nA more sophisticated algorithm with a similar purpose is [Double Metaphone](http://www.drdobbs.com/the-double-metaphone-search-algorithm/184401251?pgno=2).\nDouble Metaphone aims to yield more true matches and fewer false matches. It aims to work for non-English words as well as English words. For any given word it returns up to two different encodings. For example, for `Wagner` it returns `FKNR` for the German pronunciation in which *W* is pronounced as the *v* in *vodka*, and returns `AKNR` for the Anglicized pronunciation in which *W* is pronounced as the *w* in *water*. \n\nAn implementation of the Double Metaphone algorithm in JavaScript is [here](https://github.com/words/double-metaphone/blob/master/index.js).\nIt is a function called `doubleMetaphone`. It returns an array of two strings, each string being an encoding that represents approximately the pronunciaton of the input string.\nWith a few minor changes the function can be used in a Cloudant [view](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-views-mapreduce) or [search index](https://cloud.ibm.com/docs/services/Cloudant/offerings?topic=cloudant-search).\n\n## Implementing the Double Metaphone algorithm as a view\n\nTo turn the [implementation of Double Metaphone](https://github.com/words/double-metaphone/blob/master/index.js) into a Cloudant view Map function that emits values returned by the `doubleMetaphone` function, I removed the line:\n\n```js\nmodule.exports = doubleMetaphone\n```\nand appended these lines:\n\n```js \nfunction (doc) {\n  emit(doubleMetaphone(doc.name)[0], 1);   \n  emit(doubleMetaphone(doc.name)[1], 1);\n}\n```\n\nSee the complete [Cloudant view Map function](https://gist.github.com/brianewilkins/034c203fb29a7e0c7539f6a1ed248949).\n\n## Implementing the Double Metaphone algorithm as a Search index\n\nTo turn the [implementation of Double Metaphone](https://github.com/words/double-metaphone/blob/master/index.js) into a Cloudant Search index of values returned by the `doubleMetaphone` function I removed the line:\n\n```js\nmodule.exports = doubleMetaphone\n```\n\nand appended these lines:\n\n```js\nfunction (doc) {\n  index(\"name\", doc.name);\n  index(\"encoding\", doubleMetaphone(doc.name)[0]);\n  index(\"encoding\", doubleMetaphone(doc.name)[1]);   \n}\n```\n\nSee the complete [Cloudant Search index function](https://gist.github.com/brianewilkins/0638608ceb248773b6fc456d50e5a37c). The Cloudant Search index must use the [Keyword analyzer](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-search#analyzers) so that is input is not tokenized.\n\n\n\n## Trying it out\n\nNow let's see how well it does at finding names that are similar to `Smith`.\n\nIn the following steps I use the following conventions:\n\n- `$USER` stands for your Cloudant user name;\n- `$PASS` stands for password of user $USERNAME;\n- `$ACCOUNT` stands for the name of your Cloudant account;\n- `$DB` stands for the name of your database.\n\n\n### Writing some test documents\n\nWrite some documents that contain a `name`:\n\n```curl\n# Smith\ncurl -u $USER:$PASS -X POST https://$ACCOUNT.cloudant.com/$DB -H \"Content-Type: application/json\" -d '{\"name\": \"Smith\"}'\n\n# Names like Smith\ncurl -u $USER:$PASS -X POST https://$ACCOUNT.cloudant.com/$DB -H \"Content-Type: application/json\" -d '{\"name\": \"Smythe\"}'\ncurl -u $USER:$PASS -X POST https://$ACCOUNT.cloudant.com/$DB -H \"Content-Type: application/json\" -d '{\"name\": \"Smyth\"}'\ncurl -u $USER:$PASS -X POST https://$ACCOUNT.cloudant.com/$DB -H \"Content-Type: application/json\" -d '{\"name\": \"Smit\"}'\ncurl -u $USER:$PASS -X POST https://$ACCOUNT.cloudant.com/$DB -H \"Content-Type: application/json\" -d '{\"name\": \"Schmidt\"}'\ncurl -u $USER:$PASS -X POST https://$ACCOUNT.cloudant.com/$DB -H \"Content-Type: application/json\" -d '{\"name\": \"Schmitt\"}'\n\n# Names unlike Smith\ncurl -u $USER:$PASS -X POST https://$ACCOUNT.cloudant.com/$DB -H \"Content-Type: application/json\" -d '{\"name\": \"Jones\"}'\ncurl -u $USER:$PASS -X POST https://$ACCOUNT.cloudant.com/$DB -H \"Content-Type: application/json\" -d '{\"name\": \"Taylor\"}'\ncurl -u $USER:$PASS -X POST https://$ACCOUNT.cloudant.com/$DB -H \"Content-Type: application/json\" -d '{\"name\": \"Martinez\"}'\ncurl -u $USER:$PASS -X POST https://$ACCOUNT.cloudant.com/$DB -H \"Content-Type: application/json\" -d '{\"name\": \"Wang\"}'\n```\n\n### Creating the view and search index\n\nThe design document `_design/doubleMetaphone` which contains the `doubleMetaphone` view and search index is [ddoc.txt](https://gist.github.com/brianewilkins/6f145b801da0726b77d10e0bc782dfb4).\nWrite it to the database:\n\n```curl\ncurl -u $USER:$PASS -X POST https://$ACCOUNT.cloudant.com/$DB -H \"Content-Type: application/json\" -d @ddoc.txt\n```\n\n### Querying the view to find the Double Metaphone encoding of a name\n\nBy querying the view you can find the two Double Metaphone encodings of each name. The Double Metaphone encodings of the name `Smith` are `SM0` and `XMT`.\n\n\t$ curl -s -u $USER:$PASS https://$ACCOUNT.cloudant.com/$DB/_design/doubleMetaphone/_view/doubleMetaphone?include_docs=true | jq '.rows[] | select(.doc.name==\"Smith\") | .key'\n\t\"SM0\"\n\t\"XMT\"\n\n### Querying the view to find similar sounding names\n\nNow find which names in the database share a Double Metaphone encoding (`SM0` or `XMT`) with `Smith`. \n\nThe view request below returns each name that that shares a Double Metaphone encoding with `Smith`. A name appears twice in the result if both its Double Metaphone encodings match a Double Metaphone encoding of `Smith`.\n\n\t$ curl -s -u $USER:$PASS -X POST https://$ACCOUNT.cloudant.com/$DB/_design/doubleMetaphone/_view/doubleMetaphone?include_docs=true -H \"Content-Type: application/json\" -d '{\"keys\":[\"SM0\",\"XMT\"]}' | jq '.rows[] | {\"Name\": .doc.name, \"Encoding\": .key}'\n\t{\n\t  \"Name\": \"Smith\",\n\t  \"Encoding\": \"SM0\"\n\t}\n\t{\n\t  \"Name\": \"Smyth\",\n\t  \"Encoding\": \"SM0\"\n\t}\n\t{\n\t  \"Name\": \"Smythe\",\n\t  \"Encoding\": \"SM0\"\n\t}\n\t{\n\t  \"Name\": \"Schmitt\",\n\t  \"Encoding\": \"XMT\"\n\t}\n\t{\n\t  \"Name\": \"Smith\",\n\t  \"Encoding\": \"XMT\"\n\t}\n\t{\n\t  \"Name\": \"Smyth\",\n\t  \"Encoding\": \"XMT\"\n\t}\n\t{\n\t  \"Name\": \"Schmidt\",\n\t  \"Encoding\": \"XMT\"\n\t}\n\t{\n\t  \"Name\": \"Smythe\",\n\t  \"Encoding\": \"XMT\"\n\t}\n\t{\n\t  \"Name\": \"Smit\",\n\t  \"Encoding\": \"XMT\"\n\t}\n\nThe names `Smith`, `Smyth` and `Smythe` all have as Double Metaphone encodings both `SM0` and `XMT`, so they match `Smith`.\n\nBecause the names `Schmidt`, `Schmitt` and `Smit` have the Double Metaphone encoding `XMT`, they match `Smith` too.\n\n### Querying the search index to find similar sounding names\n\nThis search request returns all the names in the database that are like (share a Double Metaphone encoding with) `Smith`.\n\n\n\t$ curl -s -u $USER:$PASS \"https://$ACCOUNT.cloudant.com/$DB/_design/doubleMetaphone/_search/doubleMetaphone?q=encoding:(SM0%20OR%20XMT)&include_docs=true\" | jq .rows[].doc.name\n\t\"Smyth\"\n\t\"Smythe\"\n\t\"Smith\"\n\t\"Schmidt\"\n\t\"Schmitt\"\n\t\"Smit\"\n\n\n## Conclusion\n\nThe Double Metaphone algorithm has identified all the names in the database that are like `Smith`.\n\nIt won't always find all words that sound similar to a given word though.\nOut of twenty-eight [widely differing alternative spellings](https://gist.github.com/brianewilkins/e14c4519c85ba751ef67257fc6dec34a) of the name of the Russian composer `Tchaikovsky`, ten have a Double Metaphone encoding that matches `Tchaikovsky`.\nSo it's not perfect. As it is a somewhat subjective judgment which words are similar enough to count as a match, and spellings of the same name can vary a lot, it is hard to see how it could ever be.",
    "url": "/2019/08/08/fuzzy-search-using-the-double-metaphone-algorithm.html",
    "tags": "Double Metaphone views Search",
    "id": "69"
  },
  {
    "title": "Design Docs For Life",
    "description": "All about Cloudant Design Documents.",
    "content": "\n\n\nA Design Document is a special Cloudant document whose `_id` field begins with `_design/` e.g. `_design/search`. It stores meta data about a secondary index or indexes: the name of the index, which fields are to be indexed etc. Design documents are created in one of two ways:\n\n1. You create and update them manually as you would for any normal Cloudant document. This method is required for [MapReduce](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-views-mapreduce) views, [Cloudant Search](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-search) and [Geospatial](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-cloudant-nosql-db-geospatial) indexes.\n2. They are created for you when you instruct the database to create a [Cloudant Query](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-query) index using the `POST /db/_index` endpoint. Such design documents are not intended for you to maintain - they merely record state for the Cloudant Query service.\n\nAs a side note, a Design Document can also contain definitions for [List](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-design-documents#list-functions), [Show](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-design-documents#show-functions), [Update](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-design-documents#update-handlers) and [Filter](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-design-documents#filter-functions) functions, none of which we'll look at in this post. \n\n![design docs]({{< param \"image\" >}})\n> Photo by [Kelly Sikkema on Unsplash](https://unsplash.com/photos/o2TRWThve_I)\n\n## View building\n\nWhen a Design Document is created, whether by hand or via Cloudant Query, Cloudant asynchronously begins to build the index. Indexing requires Cloudant to trawl through each document in the database in turn to build the index - in fact, as a Cloudant database is split into many (sixteen by default) pieces called _shards_ the index builds independently on each shard copy. If the database document count is significant, it may take some time to finish the indexing process. \n\nYou can query a view before the index has finished building, but you will not get a response until indexing is complete. You can check on the progress of an indexing job by querying the [\\_active\\_tasks](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-active-tasks) endpoint.\n\n## Design Document Tips\n\n### Cloudant Query indexes can be more efficient than MapReduce\n\nThe MapReduce engine requires Cloudant to execute your JavaScript map function using a JavaScript engine where as Cloudant Query work stays within the Erlang space of Cloudant's core. As a result, Cloudant Queries are more efficient to build than their MapReduce equivalents.\n\n### Store data in a index-friendly way\n\nCloudant Query doesn't give you the opportunity to modify the data prior to indexing (as can be achieved in a JavaScript map function) so it is important to store data in the form that is suitable for querying. With only a handful of indexed fields, many of your application access patterns can be [queried optimally]({{< ref \"/2019-05-10-Optimal-Cloudant-Indexing.md\" >}}).\n\n### Partial Indexes are smaller\n\nIf you only need to index a sub-set of a database with Cloudant Query then a [partial index](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-query#creating-a-partial-index) can make the index smaller and more efficient by applying a filter _selector_ at indexing time. A similar effect is achieved with an `if` statement in a MapReduce or Search index:\n\n```js\nfunction(doc) {\n  // we only want totals of paid orders\n  if (doc.type === 'order' && doc.status === 'paid') {\n  \n    // build in index of order value by date, but only for paid-for orders\n    emit(doc.date, doc.total)\n  }\n}\n```\n\nA small index requires fewer computing resources to manage so should be faster to query.\n\n### Use meaningful names\n\nAfter using Cloudant for a while, you might find yourself staring at an index definition and wondering what it is for, who created it and whether it is still needed. Having a good naming convention is important in most software projects, and this applies to design document management too. Here are some ideas:\n\n- Keep global indexes and partitioned indexes separate e.g. `global/orderValueByDate`, `partitioned/userOrdersByTime`.\n- Ensure that the name of the index reflects the access pattern it is built to service e.g. `eventsByType`, `salesByYearMonthDay`, `booksByAuthor`.\n- Add comments into JavaScript view and search index definitions to explain non-obvious code and which part of your application uses the index.\n- You may wish to _version_ your view names to allow a smooth migration from one version of your code to another `e.g. salesReport/productCountByDatev5`.\n\n### Partitioned indexes are faster\n\nIf your data can be moulded into a [Partitioned Database]({{< ref \"/2019-03-05-Partition-Databases-Introduction.md\" >}}) model, then queries that are directed to a single partition will execute faster and be charged less per query than their global equivalents. \n\n### Specify use_index\n\nWhen querying using Cloudant Query, supply [use_index](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-query#finding-documents-by-using-an-index) when you know which index you intend Cloudant to use to answer your query.\n\n### Use the \"explain\" API\n\nLearn how to use the [explain API](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-query#explain-plans) which describes which index would be used to answer an individual query. \n\n## Automated testing for JavaScript\n\nJavaScript map functions are *just code* and are as susceptible to bugs and typos as any other source code. It's worth running your map functions through an automated test suite, to ensure that your code is emitting the correct keys and values in all circumstances. Here's an example test suite for a map function:\n\n```js\nconst map = require('./designdocs/global.js').views.byDeviceId\n\nlet emittedValues = []\n\nconst emit = function(key, value) {\n  emittedValues.push({key: key, value: value})\n}\n\nconst index = function(key, value) {\n  emittedValues.push({key: key, value: value})\n}\n\nconst reset = function() {\n  emittedValues = []\n}\n\ntest('Ensure nothing is emitted for person documents', function() {\n  const doc = {\n    _id: '123',\n    _rev: '1-2324',\n    type: 'person',\n    x: 1\n  }\n  reset()\n  map(doc)\n  expect(emittedValues.length).toEqual(0)\n}\n\ntest('Ensure order date and value is emitted for paid order documents', function() {\n  const doc = {\n    _id: '123',\n    _rev: '1-2324',\n    type: 'order',\n    date: '2018-02-42',\n    total: 56.22,\n    status: 'paid'\n  }\n  reset()\n  map(doc)\n  expect(emittedValues).toEqual(['2018-02-42', 56.22])\n}\n```\n\nThe above code uses the [Jest](https://jestjs.io/) testing framework and exercises our map function to ensure that it behaves correctly with the right stimulus. Dummy \"emit\"/\"index\" functions collect the emitted keys and values where they can be checked by your test code.\n\nNote: Automated testing can help catch syntax errors and logical edge cases, but exactly reproduce how Cloudant's JavaScript engine will execute your map functions, your test suite would have to use the same version of SpiderMonkey JavaScript engine.\n\n## Lifecycle management for Design Documents\n\nUnderstanding the lifecycle of indexes is important when it comes to your application's configuration management.\n\n1. CREATE - When a Design Document that contains index definitions is first saved, the process of building the index begins. Each copy of each shard of the database builds the index independently and in parallel. The index is not usable until all the processes are complete.\n2. UPDATE - When a Design Document that alters one or more MapReduce indexes is updated, **all of the MapReduce indexes in the same document are invalidated** and become unusable until they are rebuilt, The same rule *does not* apply to Cloudant Search indexes - only changed indexes are invalidated.\n3. DELETE - When a Design Document is deleted, the indexes are invalidated and the disk space occupied by its indexes will be recovered in due course.\n\nThere are some subtleties too:\n\n- Two indexes that have an identical definition but are defined in different design documents will be shared. Two indexes are deemed to be identical if the hash of their definitions are the same, including whitespace in JavaScript functions. A shared index is only actually deleted when all of its defining design documents are removed.\n- If a Design Document is modified to remove a single index definition, any other MapReduce definitions (that aren't defined elsewhere) will be invalidated. Be careful that in tidying up unwanted indexes you don't temporarily invalidate other indexes.\n- As Cloudant Search indexes are updated independently, it makes sense to have all of your Cloudant Search definitions in a single design document.\n- Design Documents have the same create/update/delete semantics as normal Cloudant documents: they are updated and deleted with a known revision token and may become conflicted if modified differently on disconnected copies.\n- When replicating databases to another Cloudant cluster, the design documents will be replicated too and will begin building on the target side. If design documents are not required at the target side, they can be filtered out with a [Cloudant Query selector](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-design-documents#the-_selector-filter).\n\nThe above semantics can used to ensure an orderly transition from one version of an index to another.\n\n## Design Document change control\n\nWith a large data set and significant index build times, care must be taken when deploying new versions of design documents if the the views are to be available at all times. Taking advantage of the subtle behaviour of design documents and their indexes, the following algorithm can be implemented to avoid downtime for the indexes defined by design document \"ddoc\":\n\n1. Copy old design document to `ddoc_OLD`.\n2. Save new design document to `ddoc_NEW`.\n3. Trigger the view/search to make sure it builds.\n4. Poll the view to see if it has finished building.\n5. Copy `ddoc_NEW` to the `ddoc`.\n6. Delete `ddoc_NEW`\n7. Delete `ddoc_OLD`\n\nThis relatively convoluted series of steps has been implemented into a command-line tool [couchmigrate](https://www.npmjs.com/package/couchmigrate) which performs the design document dance on your behalf.",
    "url": "/2019/08/16/Design-Docs-For-Life.html",
    "tags": "Design Docs Query",
    "id": "70"
  },
  {
    "title": "Logging with LogDNA",
    "description": "Analyse your IBM Cloudant logs with LogDNA",
    "content": "\n\n\nCloudant is now rolling out customer-facing logging for its Cloudant services. In this post we'll explore how to set it up and how it can help you keep tabs on your Cloudant service.\n\n> Note: The IBM Cloud Log Analysis service is available in the following IBM Cloud regions: https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-log-analysis-integration.\n\n![logging]({{< param \"image\" >}})\n> Photo by [Pär Pärsson on Unsplash](https://unsplash.com/photos/yuMpfx-C0A8)\n\n## Setting up logging\n\nFirstly we need to setup an _IBM Log Analysis with LogDNA_ service in the IBM Cloud - choose the service from the Developer Tools section of the catalog:\n\n![logdna1](/img/logdna1.png)\n\nConfigure the service in the same _region_ as your Cloudant service (e.g. Dallas) and choose and appropriate resource group:\n\n![logdna2](/img/logdna2.png)\n\nOnce provisioned, choose the \"Configure Platform Services\" button to direct your IBM Cloud platform logs to your LogDNA service:\n\n![logdna3](/img/logdna3.png)\n\nThat's it! Click the View LogDNA button to see a live stream of your logs:\n\n![logdna3](/img/logdna4.png)\n\nYou should see all of your API calls listed in the live stream, with a short delay as the data makes its way to LogDNA,\n\n## Logging data structure\n\nThe data is logged as a JSON object containing everything you need to know about the request and how it was handled:\n\n```js\n{\n  \"accountName\": \"mycloudantaccount\",\n  \"httpMethod\": \"GET\",\n  \"httpRequest\": \"/cities/_all_docs?limit=10\",\n  \"responseSizeBytes\": 1221,\n  \"clientIp\": \"94.194.94.13\",\n  \"clientPort\": 63072,\n  \"statusCode\": 200,\n  \"terminationState\": \"",
    "url": "/2019/09/13/Cloudant-Logging-with-LogDNA.html",
    "tags": "Logging Analysis",
    "id": "71"
  },
  {
    "title": "Paginating _all_docs",
    "description": "Paging through all_docs and views",
    "content": "\n\n\nIf you are using the `GET /<db>/_all_docs` endpoint to fetch documents in bulk, then you may have come across the `limit` and `skip` parameters which allow you to define how many documents you would like, and the offset into the range you want to start from. Using this `skip`/`limit` pattern to iterate through a result set works, but it gets progressively slower the larger the value of `skip`. \n\n![pages]({{< param \"image\" >}})\n> Photo by [Anastasia Zhenina on Unsplash](https://unsplash.com/photos/XOW1WqrWNKg)\n\nThere's a better way, and this blog post shows how it's done.\n\n## What is the \\_all\\_docs endpoint?\n\nThe `GET /<db>/_all_docs` is used to fetch data from a Cloudant database's _primary index_, that is the index that keeps each document's `_id` in order. The `_all_docs` endpoint takes a number of optional parameters which configure the range of data requested and whether to return each document's body or not, With no parameters provided, `_all_docs` streams all of a database's documents, returning only the document `_id` and its current `_rev` token.\n\n```js\ncurl \"$URL/mydb/_all_docs\"\n{\n  \"total_rows\": 23515,\n  \"rows\": [\n    {\n      \"id\": \"aardvark\",\n      \"key\": \"aardvark\",\n      \"value\": {\n        \"rev\": \"3-be42a3233372a6a2dff84a65fdd9cbab\"\n      }\n    },\n    {\n      \"id\": \"alligator\",\n      \"key\": \"alligator\",\n      \"value\": {\n        \"rev\": \"1-3256046064953e2f0fdb376211fe78ab\"\n      }\n    }\n...\n```\n\nIf `include_docs=true` is supplied, then an additional `doc` attribute is added to each \"row\" in the result set containing the document body.\n\n## Limit, startkey, endkey\n\nTo access data from `_all_docs` in reasonably sized pages, we need to supply the `limit` parameter to tell Cloudant how many documents to return:\n\n```\n# get me 10 documents\nGET /mydb/_all_docs?limit=10\n```\n\nWe can also limit the range of document `_id`s we want by supplying, one or more of `startkey` or `endkey`.\n\n```sh\n# get me 100 documents from _id bear onwards\nGET /mydb/_all_docs?limit=100&startkey=\"bear\"\n\n# get me 5 documents between _id bear --> frog\nGET /mydb/_all_docs?limit=100&startkey=\"bear\"&endkey=\"frog\"\n\n# get me 5 documents up to _id moose\nGET /mydb/_all_docs?limit=100&endkey=\"moose\"\n```\n\nThis gives us the ability to define the size of the data set to return and the range of the `_id` field to return, but that isn't quite the same as pagination.\n\n> Note that the `startkey`/`endkey` values are in double quotes. This is because they are expected to be JSON-encoded and `JSON.stringify('moose') === \"moose\"`.\n\n## Pagination\n\nIn order to iterate through a range of documents in an orderly and performant manner, we need to devise an algorithm to page through the range. Let's say we need to page through _all_docs in blocks of 10. \n\nWe have two options:\n\n### Option 1 - Fetch one document too many\n\nInstead of fetching ten documents (`limit=10`), fetch eleven instead (`limit=11`) but hide the eleventh document from your users. The `_id` of the eleventh document becomes the `startkey` of your request for the next page of results.\n\n```\n# first request\nGET /mydb/_all_docs?limit=11\n{\n  \"total_rows\": 10000,\n  \"rows\": [\n    { \"id\": \"aardvark\" ....},\n    { \"id\": \"alligator\" ....},\n    { \"id\": \"antelope\" ....},\n    { \"id\": \"badger\" ....},\n    { \"id\": \"bear\" ....},\n    { \"id\": \"cat\" ....},\n    { \"id\": \"doormouse\" ....},\n    { \"id\": \"donkey\" ....},\n    { \"id\": \"elephant\" ....},\n    { \"id\": \"frog\" ....},\n    { \"id\": \"gazelle\" ...}   // <-- this is the 11th result we use as the startkey of the next request\n   ]\n}    \n```\n\n```\n# second request\nGET /mydb/_all_docs?limit=11&startkey=\"gazelle\"\n{\n  \"total_rows\": 10000,\n  \"rows\": [\n    { \"id\": \"gazelle\" ....},\n    { \"id\": \"ibis\" ....},\n    ...\n   ]\n} \n```\n\nThis works but we end up fetching n+1 documents when only n are required.\n\n### Option 2 - the \\u0000 trick\n\nIf we are determined to only fetch `n` documents each time, then we need to calculate a value of `startkey` which means \"the next id after the last _id in the result set\" e.g. if the last document in our first page of results is \"frog\", what should the `startkey` of the next call to _all_docs be? It can't be \"frog\", otherwise we'd get the same document id again. It turns out that you can append `\\u0000` to the end of a key string to indicate the \"next key\" (\\u0000 is a Unicode null character which becomes `%00` when encoded into a URL). \n\n```\n# first request\nGET /mydb/_all_docs?limit=10\n{\n  \"total_rows\": 10000,\n  \"rows\": [\n    { \"id\": \"aardvark\" ....},\n    { \"id\": \"alligator\" ....},\n    { \"id\": \"antelope\" ....},\n    { \"id\": \"badger\" ....},\n    { \"id\": \"bear\" ....},\n    { \"id\": \"cat\" ....},\n    { \"id\": \"doormouse\" ....},\n    { \"id\": \"donkey\" ....},\n    { \"id\": \"elephant\" ....},\n    { \"id\": \"frog\" ....} // <-- append \\u0000 to this to get the startkey of the next request\n  ]\n}  \n```\n\n```\n# second request\nGET /mydb/_all_docs?limit=10&startkey=\"frog%00\"\n{\n  \"total_rows\": 10000,\n  \"rows\": [\n    { \"id\": \"gazelle\" ....},\n    { \"id\": \"ibis\" ....},\n    ...\n   ]\n} \n```\n\n## Pagination of views\n\nMapReduce views, secondary indexes which are defined by key/value pairs emitted from user-supplied JavaScript functions, can be queried in a similar way to the `_all_docs` endpoint but with the `GET /<db>/_design/<ddoc>/_view/<view>` endpoint instead. You can:\n\n- spool all the data from a view with no parameters.\n- includes document bodies by supplying `include_docs=true`.\n- choose the range of keys required using `startkey`/`endkey`, but in this case the data type of the keys may not be a string.\n\nAnother complication is that unlike the primary index, where every `_id` is unique, there may be many entries in a secondary index with the same key e.g. lots of entries where the key is `\"mammal\"`. This makes pagination using only `startkey`/`endkey` tricky, so there are other parameters to help: `startkey_docid`/`endkey_docid`. \n\n```\n# get first page of cities by country\nGET /cities/_design/mydesigndoc/_view/bytype?limit=10&reduce=false&startkey=\"mammal\"&endkey=\"mammal\"&include_docs=true\n\n# get next page of cities by country\nGET /cities/_design/mydesigndoc/_view/bytype?limit=10&reduce=false&startkey=\"mammal\"&endkey=\"mammal\"&include_docs=true&startkey_docid=horse%00\n```\n\nIn other words the second request has a value of `startkey_docid` that is the last document id from the previous page of results (horse) plus the magic `\\u0000` character (which becomes `horse%00` in the URL).\n\n> Note: `startkey_docid` only works if a `startkey` is supplied and where all index entries share the same key. If they don't share the same key, then pagination can be achieved with manipulation of `startkey`/`endkey` parameters only. Also note that the `startkey_docid` parameter is NOT JSON encoded.\n\n## Pagination with bookmarks\n\n[Cloudant Query](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-query) and [Cloudant Search](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-search) both use _bookmarks_ as the key to unlock the next page of results from a result set. This is described in full [in this blog post]({{< ref \"/2019-05-31-Paging-with-Cloudant-Bookmarks.md\" >}}) and is easier to manage as there is no key manipulation to formulate the request for the next result set, simply pass the _bookmark_ received in the first response in the second request. ",
    "url": "/2019/10/25/Paginating-all_docs-and-views.html",
    "tags": "Pagination All docs",
    "id": "72"
  },
  {
    "title": "Improve and then improve some more",
    "description": "The journey our team took to move a modern CI using Jenkins Pipeline.",
    "content": "\n\n\nModern software development requires a constant feedback loop in which software component correctness and whole-stack integration success can be efficiently validated. This feedback loop is realized through automation and continuous integration (CI). Well-implemented CI is imperative -- it increases transparency amongst developers for all aspects of the software development lifecycle (SLDC) by yielding the ability to rapidly build, test, and iterate on issues that might arise. Rapid iteration is the keystone of an agile workflow and is critical to business success, especially when you're a startup that gets acquired by a company like IBM. \n\nAs a newcomer to IBM Cloudant's release engineering team the challenges of maintaining and extending our CI were apparent from the get-go. Cloudant's approach towards CI centered on the popular Jenkins CI platform which was configured by Chef. This had long been sufficient but our old workflow began to present some serious headaches. To keep configuration management tight we tracked Jenkins \"freestyle\" job definitions as XML ERB (Embedded RuBy) templates in our Chef assets repository. We found this aged rather poorly as XML templates began to bloat -- tracking numerous build steps often with e.g. Bash scripts embedded in the XML. An example of this is shown below. Please note some sections of the template have been ommitted from this example for clarity.\n\n```xml\n<?xml version='1.0' encoding='UTF-8'?>\n<matrix-project plugin=\"matrix-project@1.4.1\">\n  <description>Builds the PAAS &amp; LOCAL versions of DBNEXT</description>\n  ...many top-level XML tags removed for readability\n  <axes>\n    ...multiple Cloudant build variants by architecture, Platform-as-a-service and Cloudant Local options\n  </axes>\n  <builders>\n  <hudson.tasks.Shell>\n    <command>\n# install pre-reqs\nsudo apt-get -y install lsb-release\n\n# fetch erlang over private network\nwget -q http://files.cloudant.net/private/erlang/releases/$(uname -m)/$(lsb_release -cs)/OTP-${ERLANG_VERSION}.tar.gz\n\n# install erlang\nsudo tar zxf OTP-${ERLANG_VERSION}.tar.gz -P\nexport PATH=/opt/erlang/OTP-${ERLANG_VERSION}/bin:$PATH\n\nif [ \"$type\" = \"paas\" ];\nthen\n    NAME=dbcore\n    PREFIX=/$NAME\nelse\n    NAME=cloudant\n    PREFIX=/opt/$NAME\nfi\nBRANCH=`echo $GIT_BRANCH| cut -d'/' -f 2`\n./configure $type \\\n  --user $NAME \\\n  --prefix $PREFIX \\\n  --db-dir /srv/db \\\n  --view-dir /srv/view_index \\\n  --search-dir /srv/search_index \\\n  --geo-dir /srv/geo_index \\\n  --node-name $NAME \\\n  --extdeps\nmake ci || exit 1\nif [ -z &quot;$RELEASE_TAG&quot; ]\nthen\n    make BUILD_NUMBER=$BUILD_NUMBER BRANCH=-$BRANCH branch\n    if [ $BRANCH = &quot;master&quot; ]\n    then\n        make BUILD_NUMBER=$BUILD_NUMBER BRANCH=-$BRANCH latest\n    fi\nelse\n    make RELEASE_TAG=$RELEASE_TAG BUILD_NUMBER=$BUILD_NUMBER BRANCH=-$BRANCH release\nfi</command>\n</hudson.tasks.Shell>\n  </builders>\n  <publishers>\n    ...many post-build publication strategies defined via XML\n    <org.jenkins__ci.plugins.flexible__publish.FlexiblePublisher plugin=\"flexible-publish@0.14.1\">\n      <publishers>\n        ...other minor publishers\n        <org.jenkins__ci.plugins.flexible__publish.ConditionalPublisher>\n          <condition class=\"org.jenkins_ci.plugins.run_condition.contributed.ShellCondition\" plugin=\"run-condition@1.0\">\n            <command> #!/bin/sh\nset -e\nset +x\nif [ -z &quot;$type&quot; ]\nthen\n    AXIS_PATH=$JENKINS_HOME/jobs/dbnext/workspace/label\n    JESSIE_PAAS_STATUS=`cat $AXIS_PATH/docker-local-jessie/type/paas/paas.status | awk &apos;{print tolower($0)}&apos;`\n\n    if [ &quot;$JESSIE_PAAS_STATUS&quot; = &quot;success&quot; ]\n    then\n      exit 0\n    else\n      echo &quot;Paas build failed. Not triggering downstream jobs!&quot;\n      exit 1\n    fi\nfi</command>\n          </condition>\n          <publisherList>\n            <hudson.plugins.parameterizedtrigger.BuildTrigger plugin=\"parameterized-trigger@2.25\">\n              <configs>\n                <hudson.plugins.parameterizedtrigger.BuildTriggerConfig>\n                  <configs>\n                    <hudson.plugins.parameterizedtrigger.PredefinedBuildParameters>\n                      <properties>UPSTREAM_BUILD_NUMBER=$BUILD_NUMBER\nUPSTREAM_BUILD_BRANCH=$GIT_BRANCH\nUPSTREAM_BUILD_COMMIT_SHA=$GIT_COMMIT\nUPSTREAM_RELEASE_TAG=$RELEASE_TAG</properties>\n                    </hudson.plugins.parameterizedtrigger.PredefinedBuildParameters>\n                  </configs>\n                  <projects>test-cluster-dbnext-deploy</projects>\n                  <condition>ALWAYS</condition>\n                  <triggerWithNoParameters>false</triggerWithNoParameters>\n                </hudson.plugins.parameterizedtrigger.BuildTriggerConfig>\n              </configs>\n            </hudson.plugins.parameterizedtrigger.BuildTrigger>\n          </publisherList>\n          ...more ConditionalPublisher settings\n        </org.jenkins__ci.plugins.flexible__publish.ConditionalPublisher>\n      </publishers>\n    </org.jenkins__ci.plugins.flexible__publish.FlexiblePublisher>\n  </publishers>\n</matrix-project>\n```\n\nPretty isn't it? As development demands increased post-acquisition it was a real challenge to both maintain these mission-critical CI systems and extend them with new service verification needs. Our product was scaling up as we integrated with the IBM Cloud ecosystem and starting in 2017 the IBM Cloudant team set out to make major improvements to our integration approach.\n\n## Enter Jenkins Pipeline\n\nOur approach of tracking Jenkins job business logic in the Chef-rendered XML templates was cumbersome for new developers to learn and difficult to extend. Important components embedded in the XML files were not accessible for individual execution, testing, or static analysis. However, a new approach to Jenkins CI implementation had emerged in recent years, known as [Jenkins Pipeline](https://jenkins.io/doc/book/pipeline/).\n\nA different model had gained popularity in the past couple of years which expresses a project's build/test/deploy stages with assets tracked in the project's repo, using a Jenkins Pipeline. The cost to benefit ratio was paying itself back from the day it was introduced. Pipeline expresses the CI stages in a Domain Specific Language or DSL, which, being tracked in the repo, is much more natural for developers to maintain. Scripts can be included alongside the CI definition which will allow us to execute them individually, test them, etc. The same XML template, shown above, could be rendered using DSL (Groovy) like so. Please note the mapping between this example and that of the XML template is approximate as we have been since extending and updating our pipeline.  \n\n```groovy\n stage(\"Build - ${name} ${variant}\") {\n        sh \"\"\"\n            export PATH=/opt/erlang/OTP-${erl_version()}/bin:$PATH\n\n            make compile\"\"\"\n    }\n\n    try {\n        stage(\"EUnit Test Suite - ${name} ${variant}\") {\n            sh \"\"\"\n                export PATH=/opt/erlang/OTP-${erl_version()}/bin:$PATH\n\n                make check\"\"\"\n        }\n\n        stage(\"Dialyzer Type Check - ${name} ${variant}\") {\n            sh \"\"\"\n                export PATH=/opt/erlang/OTP-${erl_version()}/bin:$PATH\n\n                make type-check\"\"\"\n        }\n\n        stage(\"Elixir Tests - ${name} ${variant}\") {\n            installElixir()\n            installHaproxy()\n            sh \"\"\"\n                export PATH=/usr/sbin:/usr/sbin/haproxy:/opt/erlang/OTP-${erl_version()}/bin:$PATH\n\n                python3 dev/run -n 1 -a adm:pass --no-eval elixir/run || true\"\"\"\n        }\n    } finally {\n        junit \"src/**/.eunit/*.xml\"\n    }\n\n    stage(\"Prepare build artifacts for persistence\" - ${name} ${variant}\") {\n        // This is just a handy metadata file\n        def propspath = \"rel/dbcore/releases/${buildNumber}/build.props\"\n        def props = \"\"\"\\\n            builder: ${name}\n            variant: ${variant}\n            sha: ${BUILD_COMMIT}\n            branch: ${env.BRANCH_NAME}\n            erlang: ${erl_version()}\n            \"\"\".stripIndent()\n        writeFile file: propspath, text: props\n\n        if (env.BRANCH_NAME == \"master\") {\n            sh \"\"\"\n                export PATH=/opt/erlang/OTP-${erl_version()}/bin:$PATH\n                make BUILD_NUMBER=${buildNumber} BRANCH=${env.BRANCH_NAME} branch\n                make BUILD_NUMBER=${buildNumber} BRANCH=${env.BRANCH_NAME} latest\"\"\"\n        } else {\n            sh \"\"\"\n            export PATH=/opt/erlang/OTP-${erl_version()}/bin:$PATH\n            make BUILD_NUMBER=${buildNumber} BRANCH=${env.BRANCH_NAME} branch\"\"\"\n        }\n    }\n    stage (\"Triggering Fileserver Deploy for ${platform}\") {\n        if (\"$env.TRIGGER_DOWNSTREAM_JOB\".toBoolean() || env.BRANCH_NAME == \"master\") {\n            build job: 'db-fileservers-deploy',\n                parameters: [\n                    [$class: 'StringParameterValue', name: 'DB_BUILD_NUMBER', value: \"${buildNumber}\"],\n                    [$class: 'StringParameterValue', name: 'BUILD_BRANCH', value: \"${env.BRANCH_NAME}\"],\n                    [$class: 'BooleanParameterValue', name: 'TRIGGER_DOWNSTREAM_JOB', value: false],\n                    [$class: 'StringParameterValue', name: 'PLATFORM', value: \"${platform}\"]\n                ],\n                wait: true\n        } else {\n            echo \"Not triggering file server deploy jobs.\"\n        }\n    }\n}\n```\n\nIt's immediately clear that the Pipeline scripted language approach is natural for developers to use. Engineers don't have to maintain complex or error-prone XML templates. One of the greatest benefits of adopting the Pipeline DSL has been the buy-in our team received from the wider developer community in our organization. From the beginning our team members have been able to skill up on the Pipeline DSL and become immediately productive.\n\nAutomation achieved with our new pipeline helped us to alleviate previously laborious manual testing. One example of this is shown by what we call the \"poker\" test. During this test our team would manually run a script that was developed to open and close databases as quickly as possible. While that script would run we would open a new shell and attempt to upgrade our database to a new release candidate version for testing. We would manually inspect all the results as this was all running in the background. \n\nManual Steps\n",
    "url": "/2019/11/01/Improve-and-then-improve-some-more.html",
    "tags": "CI CD",
    "id": "73"
  },
  {
    "title": "Case-sensitivity in queries",
    "description": "Making Cloudant query, search and MapReduce case sensitive or case-insensitive",
    "content": "\n\n\nBy default, some Cloudant operations are _case sensitive_ - the query parameter must match the value in the document exactly for it to be included in search results - but if you need a _case insensitive_ query then there are number of solutions.\n\n![font]({{< param \"image\" >}})\n> Photo by [Amador Loureiro on Unsplash](https://unsplash.com/photos/BVyNlchWqzs)\n\nLet's see how each of the Cloudant query mechanisms handle case sensitivity with a database of cars whose documents look like this which we need to query by the \"model\" attribute:\n \n```js\n{\n  \"_id\": \"15c08063d9b1f382e2b865f50216e350\",\n  \"_rev\": \"1-9ae2a466f4fab57060a3080ed006809f\",\n  \"year\": 1987,\n  \"make\": \"Buick\",\n  \"model\": \"Skylark\"\n}\n```\n\n## MapReduce\n\nIf we create a MapReduce index with a map function that looks like this:\n\n```js\nfunction(doc) {\n  emit(doc.model, null)\n}\n```\n\nWe can expected to be able to retrieve documents by the indexed key: `model` with a query like so:\n\n```\nGET /cars/_design/mydesigndoc/_view/bymodel?key=\"Skylark\"\n```\n\nbut the index will be _case sensitive_ and only queries supplying the original case will match. So a supplied key of `skylark` (lowercase 'S') would yield no results.\n\nTo make a MapReduce index _case insensitive_, the index data should be coerced to be lowercase and all queries treated the same way. So our map function becomes:\n\n```js\nfunction(doc) {\n  emit(doc.model.toLowerCase(), null)\n}\n```\n\nand our query:\n\n```\nGET /cars/_design/mydesigndoc/_view/bymodel?key=\"skylark\"\n```\n\nAs long as we remember to lowercase the key at query-time, we have a *case-insensitive* MapReduce index.\n\n## Cloudant Search\n\nCloudant Search is a different beast because it is built for free-text matching, and its default behaviour is to build *case-insensitive* indexes. So if we build our index with a map function:\n\n```js\nfunction(doc) {\n  index('model', doc.model)\n}\n```\n\nand query it with\n\n```\n# lowercase\nGET /cars/_design/mydesigndoc/_search/modelsearch?q=model%3Askylark\n```\n\nor\n\n```\n# uppercase\nGET /cars/_design/mydesigndoc/_search/modelsearch?q=model%3ASkylark\n```\n\nwe should get the same results. \n\nThe way Cloudant Search behaves with supplied text depends on the _analyzer_ used to pre-process and split the string into indexable tokens. The default Cloudant Search _analyzer_ is the _standard analyzer_ which removes punctuation, splits strings into words, removes stop words and lowercases everything prior to indexing. [Other analyzers]({{< ref \"/2018-10-19-Search-Analyzers.md\" >}}) are available which pre-process the data in different ways. If you need a _case sensitive_ Cloudant Search, then you can opt to use the _whitespace analyzer_ instead.\n\n## Cloudant Query\n\n### Cloudant Query - type=json\n\nWe can create an a Cloudant Query index on the `model` field with the `POST /cars/_index` endpoint:\n\n```\nPOST /cars/_index\n{\"index\":{\"fields\":[\"model\"]},\"type\":\"json\"}\n```\n\nand query it with the `POST /cars/_find` endpoint:\n\n```\nPOST /cars/_find\n{\"selector\":{\"model\":\"Skylark\"}}\n```\n\nThe `type: json` indexes are _case sensitive_ (which isn't surprising as they are backed by MapReduce technology under-the-hood)\n\n### Cloudant Query - type=text\n\nThe `type: text` are built for free-text searching because they are backed by the same Lucene-based indexed used by Cloudant Search. So creating a index with:\n\n```\nPOST $URL/cars/_index\n{\"index\":{\"fields\":[{\"name\":\"model\",\"type\":\"string\"}]},\"type\":\"text\"}\n```\n\nallows a query\n\n```\nPOST /cars/_find\n{\"selector\":{\"$text\":\"Skylark\"}}\n```\n\nto be _case-insenstive_ (notice the use of the `$text` operator to indicate we want to engage free-text comparison of the supplied string with the indexed data).\n\nIf, however, you query with the \"equality\" operator `$eq`, a _case sensitive_ match will be performed:\n\n```\nPOST /cars/_find\n{\"selector\":{\"$eq\":{\"model\":\"Skylark\"}}}\n```\n\nNote: if you need to change the _analyzer_ used by in `type: text` Cloudant Query index, then this is [possible at index-time](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-query).\n\n## How _not_ to do it - Regular Expressions\n\nIt is possible to get a case-insensitive result out of Cloudant Query, without employing a `type: text` index by using the `$regex` operator and constructing a suitable regular expression. This is **not** the recommended approach because Cloudant cannot optimise the query, even if the searchable field is indexed. Each document in the database would have to be examined in turn to see it matched the provided regular expression. This process becomes very inefficient as the size of the data set increases and leads to very slow performance.",
    "url": "/2019/11/08/Case-sensitivity-in-queries.html",
    "tags": "Query Search MapReduce",
    "id": "74"
  },
  {
    "title": "Best & Worst Practice",
    "description": "Advice for new users",
    "content": "\n\n\nSo you’re new to Cloudant, but you’re not new to database systems. As an offering manager and engineer for Cloudant the past four years, I’ve had the chance to see the product from all angles: the customers who use it, the engineers that run it, and the folks who support and sell it.\n\n![image]({{< param \"image\" >}})\n\nIn many ways, this article is inspired by [Dimagi’s perspective](https://www.dimagi.com/blog/what-every-developer-should-know-about-couchdb/) as users. I’d like to add our perspective as providers of the database service, and summarize the best — and worst! — practices we see most often in the field.\n\n## Rule 0: Understand the API you are targeting\n\nYou may use [Java](https://github.com/IBM/cloudant-java-sdk), [Python](https://github.com/IBM/cloudant-python-sdk), [Go](https://github.com/IBM/cloudant-go-sdk) or [Node.js](https://github.com/IBM/cloudant-node-sdk) or some other use-case specific language or platform that likely comes with convenient client-side libraries that integrate Cloudant access nicely, following the conventions you expect for your tools. This is great for programmer efficiency, but it also hides the API from view.\n\nThis abstraction is what you want, of course — the whole reason for using a client library is to save yourself repeated, tedious boiler-plating — but understanding the underlying API is *vital* when it comes to troubleshooting, and when reporting problems. When you report a suspected problem to Cloudant, it helps us help you if you can provide a way for us to reproduce the problem.\n\nThis does not mean cutting and pasting a hefty chunk of your application’s Java source verbatim into a support ticket, as we’re probably not able to build it. Also, your client-side code introduces uncertainties as to where the problem may be — your side or our side?\n\nInstead, Cloudant’s support teams will usually ask you to provide the set of API calls, ideally as a set of [curl](https://curl.haxx.se/) commands that they can run, that demonstrates the issue. Adopting this approach to troubleshooting as a general rule also makes it easier for you to pinpoint where issues are failing. If your code is behaving unexpectedly, try to reproduce the problem using only direct access to the API.\n\nIf you can’t, the problem isn’t with the Cloudant service itself.\n\nIf you're investigating a performance issue, do consult the logs provided by IBM Cloud. If the logs show that your requests are handled quickly\nby Cloudant, but your application is slow, the root of that problem lies with your client-side application code. See the rule about logging and\nmonitoring below.\n\nIf you suspect that a problem you’ve encountered lies with an officially supported client library, then try to construct a small, self-contained code example that demonstrates the issue, with as few other dependencies as possible. If you’re using Java, it is helpful to us if you can use a minimal [test harness](https://github.com/mikerhodes/java-cloudant-minimal) to highlight library issues.\n\nOccasionally Cloudant receives support tickets that state that “Cloudant is broken because my application is slow” without much in terms of supporting evidence. Nearly always this is traced back to issues in the application code on the client side, or misconceptions about how Cloudant works.\n\nNot always, but *nearly* always.\n\nBy understanding the API better, you also gain experience in how Cloudant behaves, especially in terms of performance. If you're using a\nclient library, you should aim to at least know how to find out which HTTP requests are generated by a given function call.\n\n- Cloudant API [docs](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-api-reference-overview).\n- Logging [integration](https://cloud.ibm.com/docs/Cloudant?topic=cloudant-log-analysis-integration).\n- Blog post on [logging]({{< ref \"/2019-09-13-Cloudant-Logging-with-LogDNA.md\" >}}).\n\n## Rule 1: Documents should group data that mostly change together\n\nWhen you start to model your data, sooner or later you’ll run into the issue of how your documents should be structured. You’ve gleaned that Cloudant doesn’t enforce any kind of normalisation and that it has no transactions of the type you’re used to from, say, [Postgres](https://www.postgresql.org/), so the temptation can be to cram as much as possible into each document, given that this would also save on HTTP overhead.\n\nThis is often a bad idea.\n\nIf your model groups information together that doesn’t change together, you’re more likely to suffer from update conflicts.\n\nConsider a situation where you have users, each having a set of orders associated with them. One way might be to represent the orders as an array in the user document:\n\n```js\n{ // DON'T DO THIS\n    \"customer_id\": 65522389,\n    \"orders\": [ {\n      \"order_id\": 887865,\n      \"items\": [ {\n          \"item_id\": 9982,\n          \"item_name\": \"Iron sprocket\",\n          \"cost\": 53.0\n        }, {\n          \"item_id\": 2932,\n          \"item_name\": \"Rubber wedge\",\n          \"cost\": 3.0\n        }\n      ]\n    }\n  ]\n}\n```\n\nTo add a new order, I need to fetch the complete document, unmarshal the JSON, add the item, marshal the new JSON, and send it back as an update. If I’m the only one doing so, it may work for a while. If the document is being updated concurrently, or being replicated, we’ll likely see update conflicts.\n\nInstead, keep orders separate as their own document type, referencing the customer id. Now the model is immutable. To add a new order, I simply create a *new* order document in the database, which cannot generate conflicts.\n\nTo be able to retrieve all orders for a given customer, we can employ a view, which we’ll cover later.\n\nAvoid constructs that rely on updates to parts of existing documents, where possible. Bad data models are often extremely hard to change once you’re in production.\n\nThe pattern above can be solved efficiently using partitioned databases, which are covered in greater detailed below.\n\n- Cloudant guide to [data modelling](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-five-tips-for-modeling-your-data-to-scale).\n- Database [partitions](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-database-partitioning).\n\n## Rule 2: Keep documents small\n\nCloudant imposes a max doc size of 1 MB. This does not mean that a close-to-1-MB document size is a *good* idea. On the contrary, if you find you are creating documents that exceed single-digit KB in size, you should probably revisit your model. Several things in Cloudant becomes less performant as documents grow. JSON decoding is costly, for example.\n\nGiven Rules 1 and 2, it's worth stressing that models that rely on updates have an upper volume limit of 1MB -- the cut-off for document size. This isn't what you want.\n\n## Rule 3: Avoid using attachments\n\nCloudant has support for storing attachments alongside documents, a long-standing feature it inherits from CouchDB. It can be handy to be able to store small icons and other static assets such as CSS and JavaScript files with the data if you’re using Cloudant as a backend for a web application.\n\nThere are a few things to consider before using attachments in Cloudant today, especially if you’re looking at larger assets such as images and videos:\n\n1. Cloudant is expensive as a block store\n2. Cloudant’s internal implementation is not efficient in handling large amounts of binary data\n\nSo: slow and expensive.\n\nIt’s ok for small assets and/or occasional use, but as a rule, if you need to store binary data alongside Cloudant documents, it’s better to use a separate solution more suited for this purpose, and store only the attachment *metadata* in the Cloudant document. Yes, that means some extra code you need to write to upload the attachment to a suitable block store of your choice, verify that it succeeded before storing the token or URL to the attachment in the Cloudant document.\n\nYour databases will be smaller, cheaper, faster, and easier to replicate.\n\n- Cloudant docs on [attachments](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-attachments).\n- Detaching Cloudant attachments to [Object Storage](https://medium.com/ibm-watson-data-lab/detaching-cloudant-attachments-to-object-storage-with-serverless-functions-99b8c3c77925).\n\n## Rule 4: Fewer databases are better than many\n\nIf you can, limit the number of databases per Cloudant account to 500 or fewer. Whilst there is nothing magic about this particular number (Cloudant can safely handle more), there are several use cases that are adversely affected by large numbers of databases in an account.\n\nThe replicator scheduler has a limited number of simultaneous replication jobs it is prepared to run. That means that as the number of databases grow, the replication latency is likely to increase if you try to replicate everything contained in an account.\n\nThere is an operational aspect which is the flip side of the same coin: Cloudant’s operations team relies on replication, too, in order to move accounts around. By keeping down the number of databases you help us help you, should you need to shift your account from one location to another.\n\nSo when should you use a single database and distinguish between different document types using views, and when should you use multiple databases to model your data? Cloudant can’t federate views across multiple databases, so if you have data that is unrelated to the extent that they will never be “joined” or queried together, then that data could be a candidate for splitting across multiple databases.\n\nNote that if you have an ever-growing dataset (like a log, or sensor readings or other types of time-series), it's also not a good idea to create a single, ever-growing, massive database. This kind of use-case requires time-boxing, which we'll cover in more detail below.\n\n## Rule 5: Avoid the “database per user” anti-pattern like the plague\n\nIf you’re building out a multi-user service on top of Cloudant, it is tempting to let each user store their data in a separate database under the application account. That works well, mostly, if the number of users is small.\n\nNow add the need to derive cross-user analytics. The way you do that is to replicate all the user databases into a single analytics DB. All good. Now, this app suddenly becomes successful, and the number of users grow from 150 to 20,000. Now we have 20,000 replications just to keep the analytics DB current. If we also want to run in an active-active DR setup, we add another 20,000 replications and basically the system will stop functioning.\n\nInstead, multiplex user data into fewer databases, or shard users into a set of databases or accounts, or both. That way there is no need to replicate to provide an analytics DB, but auth becomes more complicated as Cloudant only provides authentication at the database level.\n\nIt’s worth stating that the “database-per-user” approach is tempting because Cloudant permissions are “per database” — it’s not really the users’ fault that this pattern has emerged.\n\n## Rule 6: Avoid writing custom JavaScript reduce functions\n\nThe map-reduce views in Cloudant are awesome. However, with great power comes great responsibility. The map-part of a map-reduce view is built incrementally, so shoddy code in the map impacts only indexing time, not query time. The reduce-part, unfortunately, will execute at query time. Cloudant provides a set of built-in reduce functions that are implemented internally in [Erlang](https://www.erlang.org/), which are performant at scale, and which your hand-crafted JavaScript reduces are not.\n\nIf you find yourself writing reduce functions, stop and consider if you could re-organise your data so that this isn’t necessary, or so that you’re able to rely on the built-in reducers. Note that views on partitioned databases do not support custom reduces, which is one factor contributing to the significant speed-up queries on such views can offer.\n\n- Cloudant docs on [reduces](https://console.bluemix.net/docs/services/Cloudant/api/creating_views.html#reduce-functions)\n\n\n## Rule 7: Understand the trade-offs in emitting data or not into a view\n\nAs the document referenced by a view is always available using `include_docs=true`, it is possible to do something like:\n\n```js\nemit(doc.indexed_field, null);\n```\n\nin order to allow lookups on `indexed_field`. This has advantages and disadvantages.\n\n1. The index is compact — this is good, as index size contributes to storage costs\n2. The index is robust — as the index does not store the document, you can access any field without thinking ahead of what to store in the index\n3. The disadvantage is that getting the document back is more costly than the alternative of emitting data into the index itself, as the database first has to look up the requested key in the index, and then read the associated document. Also, if you’re reading the whole document, but actually need only a single field, you’re making the database read and transmit data you don’t need.\n\nThis also means that there is a potential race condition here — the document may have changed, or been deleted between the index and document read (although unlikely in practice).\n\nEmitting data into the index (a so-called “projection” in relational algebra terms) means that you can fine-tune the exact subset of the document that you actually need. In other words, you don’t need to emit the whole document. Only emit a value which represents the data you need in the app i.e. a cut-down object with minimal details, for example:\n\n```js\nemit(doc.indexed_field, {name: doc.name, dob: doc.dob});\n```\n\nOf course, if you change your mind on what fields you want to emit, the index will need rebuilding.\n\nCloudant Query’s (CQ) JSON indexes use views this way under the hood. CQ can be a convenient replacement for some types of view queries, but not all. Do take the time to understand when to use one or the other.\n\n- Cloudant Query [docs](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-query)\n- Cloudant guide to using [views](https://cloud.ibm.com/docs/services/Cloudant/api?topic=cloudant-using-views#using-views).\n- Performance implications of using [include_docs](https://cloud.ibm.com/docs/services/Cloudant/api?topic=cloudant-using-views#multi-document-fetching).\n\n## Rule 8: Never rely on the default behaviour of Cloudant Query’s no-indexing\n\nIt’s tempting to rely on CQs ability to query without creating explicit indexes. This is extremely costly in terms of performance, as every lookup is a full scan of the database rather than an indexed lookup. If your data is small, this won’t matter, but as the dataset grows, this will become a problem for you, and for the cluster as a whole. It is likely that we will limit this facility in the near future. The Cloudant dashboard allows you to create indexes in an easy way.\n\nCreating indexes and crafting CQs that take advantage of them requires some flair. To identify which index is being used by a particular query, send a POST to the `_explain` endpoint for the database, with the query as data.\n\n- Cloudant Query [docs](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-query)\n\n## Rule 9: In Cloudant Search (or CQ indexes of type text), limit the number of fields\n\nCloudant Search and CQ indexes of type `text` (both of which are Apache Lucene under the hood) allow you to index any number of fields into the index. We've seen some examples where this is abused either deliberately, or most often fat-fingered. Plan your indexing to comprise only the fields required by your actual queries. Indexes take space and can be costly to rebuild if the number of indexed fields are large.\n\nThere’s also the issue of which fields you store in a Cloudant Search. Stored fields are retrieved in the query without doing `include_docs=true` so the trade-off is similar to Rule 7.\n\n- Cloudant Search [docs](https://cloud.ibm.com/docs/services/Cloudant/api?topic=cloudant-search#search).\n\n## Rule 10: Avoid conflicts\n\nCloudant is designed to treat conflicts as a natural state of data in a distributed system. This is a powerful feature that helps a Cloudant cluster be highly available at all times. However, the assumption is that conflicts are still reasonably rare. Keeping track of conflicts in Cloudant’s core has significant cost associated with it.\n\nIt is perfectly possible (but a bad idea!) to just ignore conflicts, and the database will merrily carry on operating by choosing a random, but deterministic revision of conflicted documents. However, as the number of unresolved conflicts grows, the performance of the database goes down a black hole, especially when replicating.\n\nAs a developer, it’s *your* responsibility to check for, and to resolve, conflicts — or even better, employ data models that make conflicts impossible.\n\nIf you routinely create conflicts, you should really consider model changes: even if you resolve your conflicts diligently, the conflict branches in the revision tree will remain, and there is no easy way to tidy that up.\n\n- Cloudant guide to [conflicts](https://cloud.ibm.com/docs/services/Cloudant/guides?topic=cloudant-conflicts#conflicts).\n- Cloudant guide to versions and [MVCC](https://cloud.ibm.com/docs/services/Cloudant/guides?topic=cloudant-document-versioning-and-mvcc#document-versioning-and-mvcc).\n- Three-part blog series on [conflicts](https://developer.ibm.com/dwblog/2015/cloudant-document-conflicts-one/).\n\n## Rule 11: Deleting documents won’t delete them\n\nDeleting a document from a Cloudant database doesn’t actually purge it. Deletion is implemented by writing a new revision of the document under deletion, with an added field `_deleted: true`. This special revision is called a `tombstone`. Tombstones still take up space and are also passed around by the replicator.\n\nModels that rely on frequent deletions of documents are not suitable for Cloudant.\n\n- Cloudant tombstone [docs](https://cloud.ibm.com/docs/services/Cloudant/api?topic=cloudant-documents#-tombstone-documents).\n\n## Rule 12: Be careful with updates\n\nIt is more expensive in the longer run to mutate existing documents than to create new ones, as Cloudant will always need to keep the document tree *structure* around, even if internal nodes in the tree will be stripped of their payloads. If you find that you create long revision trees, your replication performance will suffer. Moreover, if your update frequency goes above, say, once or twice every few seconds, you’re more likely to produce update conflicts.\n\nPrefer models that are immutable.\n\nThe obvious question after rules 11 and 12 is — won’t the data set grow unbounded if my model is immutable? If you accept that deletes don’t fully purge the deleted data and that updates are actually not updating in place, in terms of data volume growth there is not much difference. Managing data volume over time requires different techniques.\n\nThe only way to truly reclaim space is to delete *databases*, rather than documents. You can replicate only winning revisions to a new database and delete the old to get rid of lingering deletes and conflicts. Or perhaps you can build it into your model to regularly start new databases (say ‘annual data’) and archive off (or remove) outdated data, if your use case allows.\n\n## Rule 13: Replication isn’t magic\n\n> “So let’s set up three clusters across the world, Dallas, London, Sydney, with bi-directional synchronisation between them to provide real-time collaboration between our 100,000 clients.”\n\nNo. Just... no.\n\nCloudant is good at replication. It’s so effortless that it can seem like magic, but note that it makes no latency guarantees. In fact, the whole system is designed with eventual consistency in mind. Treating Cloudant’s replication as a real time messaging system will not end up in a happy place. For this use case, put a system in between that was designed for this purpose, such as [Apache Kafka](https://kafka.apache.org/).\n\nIt’s difficult to put a number on replication throughput — the answer is always “it depends”. Things that impact replication performance include, but are not limited to:\n\n1. Change frequency\n2. Document size\n3. Number of simultaneous replication jobs on the cluster as a whole\n4. Wide (conflicted) document trees\n5. Your reserved throughput capacity settings\n\n- Blog post on [replication topology](https://dx13.co.uk/articles/2017/11/7/cloudant-replication-topologies-and-failover.html).\n- Cloudant [guide to replication](https://cloud.ibm.com/docs/services/Cloudant/guides?topic=cloudant-replication-guide#replication).\n- Updates to the [replication scheduler](https://developer.ibm.com/dwblog/2017/replicator-apache-couchdb-cloudant/).\n\n## Rule 14: Use the bulk API\n\nCloudant has nice API endpoints for bulk loading (and reading) many documents at once. This can be much more efficient than reading/writing many documents one at a time. The write endpoint is:\n\n```\n${database}/_bulk_docs.\n```\n\nIts main purpose is to be a central part in the replicator algorithm, but it's available for your use, too, and it’s pretty awesome.\n\nWith `_bulk_docs`, in addition to creating new docs you can also update and delete. Some client libraries, including [PouchDB](https://pouchdb.com/), implement create, update and delete even for single documents this way for fewer code paths.\n\nHere is an example creating one new, updating a second existing, and deleting a third document:\n\n```sh\ncurl -XPOST 'https://ACCT.cloudant.com/DB/_bulk_docs' \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"docs\":[{\"baz\":\"boo\"}, \\\n         {\"_id\":\"463bd...\",\"foo\":\"bar\"}, \\\n         {\"_id\":\"ae52d...\",\"_rev\":\"1-8147...\",\"_deleted\": true}]}'\n```\n\nYou can also fetch many documents at once by issuing a `POST` to `_all_docs`(there is also a newish endpoint called `_bulk_get`, but this is probably not what you want  —  it’s there for a specific internal purpose).\n\nTo fetch a fixed set of docs using `_all_docs`, `POST` with a `keys` body:\n\n```sh\ncurl -XPOST 'https://ACCT.cloudant.com/DB/_all_docs' \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"keys\":[\"ab234....\",\"87addef...\",\"76ccad...\"]}'\n```\n\nNote also that Cloudant (at the time of writing) imposes a max request size of 11 MB, so `_bulk_docs` requests exceeding this size will be rejected with a `413: Payload Too Large` error.\n\n- Cloudant [bulk operations docs](https://cloud.ibm.com/docs/services/Cloudant/api?topic=cloudant-documents#bulk-operations).\n- Cloudant [request and doc size limits](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-ibm-cloud-public#request-and-document-size-limits)\n\n## Rule 15: Eventual Consistency is a harsh taskmaster (a.k.a. don’t read your writes)\n\nEventual consistency is a great idea on paper, and a key contributor to Cloudant’s ability to scale out in practice. However, it’s fair to say that the mindset required to develop against an eventually consistent data store does not feel natural to most people.\n\nYou often get stung when writing tests:\n\n1. Create a database\n2. Populate the database with some test data\n3. Query the database for some subset of this test data\n4. Verify that the data you got back is the data you expected to get back\n\nNothing wrong with that? That works on every other database you’ve ever used, right?\n\nNot on Cloudant.\n\nOr rather, it works 99 times out of 100.\n\nThe reason for this is that there is a (mostly) small inconsistency window between writing data to the database, and this being available on all nodes of the cluster. As all nodes in a cluster are equal in stature, there is no guarantee that a write and a subsequent read will be serviced by the same node, so in some circumstances the read may be hitting a node before the written data has made it to this node.\n\nSo why don’t you just put a short delay in your test between the write and the read? That will make the test less likely to fail, but the problem is still there.\n\nCloudant has no transactional guarantees, and whilst document writes are atomic (you’re guaranteed that a document can either be read in its entirety, or not at all), there is no way to close the inconsistency window. It’s there by design.\n\nA more serious concern that should be at the forefront of every developer’s mind is that you can’t safely assume that data you write will be available to anyone else at a specific point in time. This takes some getting used to if you come from a different kind of database tradition.\n\n**Testing Tip**: what you *can* do to avoid the inconsistency window in testing is to test against an single-node instance of Cloudant or CouchDB running say in Docker (docker stuff [here](https://hub.docker.com/_/couchdb/)). A single node removes the eventual consistency issue, but beware that you are then testing against an environment that behaves differently to what you will target in production. *Caveat Emptor*.\n\n## Rule 16: Don’t mess with Q, R and N unless you really know what you are doing\n\nCloudant’s quorum and sharding parameters, once you discover them, seem like tempting options to change the behaviour of the database.\n\n*Stronger consistency — surely just set the write quorum to the replica count?*\n\nNo! Recall that there is no way to close the inconsistency window in a cluster.\n\nDon’t go there. The behaviour will be much harder to understand especially during network partitions. If you’re using Cloudant-the-service, the default values are fine for the vast majority of users.\n\nThere are times when tweaking the shard count for a database is essential to get the best possible performance, but if you can’t say why this is, you’re likely to make your situation worse.\n\n## Rule 17: Design document (ddoc) management requires some flair\n\nAs your data set grows, and your number of views goes up, sooner or later you will want to ponder how you organise your views across ddocs. A single ddoc can be used to form a so-called `view group`: a set of views that belong together by some metric that makes sense for your use case. If your views are pretty static, that makes your view query URLs semantically similar for related queries. It's also more performant at index time because the index loads the document once and generates multiple indexes from it.\n\nDdocs themselves are read and written using the same read/write endpoints as any other document. This means that you can create, inspect, modify and delete ddocs from within your application. However, even small changes to ddocs can have big effects on your database. When you update a ddoc, *all* views in it become unavailable until indexing is complete. This can be problematic in production. To avoid it you have to do a crazy ddoc-swapping dance (see [couchmigrate](https://github.com/glynnbird/couchmigrate)).\n\nIn most cases, this is probably not what you want to have to deal with. As you start out, it is most likely more convenient to have a one-view-per-ddoc policy.\n\nAlso, in case it isn’t obvious, views are code and should be subject to the same processes you use in terms of source code version management for the rest of your application code. How to achieve this may not be immediately obvious. You could version the JS snippets and then cut & paste the code into the Cloudant dashboard to deploy whenever there is a change, and yes, we all resort to this from time to time.\n\nThere are better ways to do this, and this is one reason to use some of the tools surrounding the [couchapp](http://docs.couchdb.org/en/2.0.0/couchapp/) concept. A couchapp is a self-contained CouchDB web application that nowadays doesn’t see much use. Several couchapp tools exist that are there to make the deployment of a couchapp — including its views, crucially — easier.\n\nUsing a couchapp tool means that you can automate deployment of views as needed, even when not using the couchapp concept itself.\n\n- See for example [couchapp](https://github.com/couchapp/couchapp) and [situp](https://github.com/drsm79/situp).\n- Cloudant guide to [design doc management](https://cloud.ibm.com/docs/services/Cloudant/guides?topic=cloudant-design-document-management#design-document-management).\n\n## Rule 18: Cloudant is rate limited — let this inform your code\n\nCloudant-the-service (unlike vanilla CouchDB) is sold on a “reserved throughput capacity” model. That means that you pay for the *right to use* up to a certain throughput, rather than the throughput you actually end up consuming. This takes a while to sink in. One somewhat flaky comparison might be that of a cell phone contract where you pay for a set number of minutes regardless of whether you end up using them or not.\n\nAlthough the cell phone contract comparison doesn’t really capture the whole situation. There is no constraint on the sum of requests you can make to Cloudant in a month; the constraint is on how *fast* you make requests.\n\nIt’s really a promise that you make to Cloudant, not one that Cloudant makes to you: you promise to not make more requests per second than what you said you would up front. A top speed limit, if you like. If you transgress, Cloudant will fail your requests with a status of `429: Too Many Requests`. It's your responsibility to look out for this, and deal with it appropriately, which can be difficult when you've got multiple app servers. How can they coordinate to ensure that they collectively stay below the requests-per-second limit?\n\nCloudant’s official client libraries have some built-in provision for this that can be enabled (note: this is switched off by default to force you to think about it), following a “back-off & retry” strategy. However, if you rely on this facility alone you will eventually be disappointed. Back-off & retry only helps in cases of temporary transgression, not a persistent butting up against your provisioned throughput capacity limits.\n\nYour business logic *must* be able to handle this condition. Another way to look at it is that you get the allocation you pay for. If that allocation isn’t sufficient, the only solution is to pay for a higher allocation.\n\nProvisioned throughput capacity is split into three different buckets: *Lookups*, *Writes* and *Queries*. A *Lookup* is a “primary key” read — fetching a document based on its `_id`. A *Write* is storing a document or attachment on disk, and a *Query* is looking up documents via a secondary index (any API endpoint that has a `_design` or `_find` in it).\n\nYou get different allocations of each and the ratios between them are fixed. This fact can be used to optimise for cost. As you get 20 *Lookups* for every 1 *Query* (per second), if you find that you’re mainly hitting the *Query* limit but you have plenty headroom in *Lookups*, it may be possible to reduce the reliance on *Queries* through some remodelling of the data or perhaps doing more work client-side.\n\nThe corollary here though is that you can’t assume that any 3rd party library or framework will optimise for cost ahead of convenience. Client-side frameworks that support multiple persistence layers via plugins are unlikely to be aware of this, or perhaps even incapable of make such trade-offs.\n\nChecking this before committing to a particular tool is a good idea.\n\nIt is also worth understanding that the rates aren’t directly equivalent to HTTP API endpoint calls. You should expect that for example a bulk update will count according to its constituent document writes.\n\n- Cloudant docs on [plans and pricing](https://cloud.ibm.com/docs/services/Cloudant/offerings?topic=cloudant-ibm-cloud-public#ibm-cloud-public) on IBM Public Cloud.\n\n## Rule 19: Partitioned queries are faster and cheaper\n\nOpting to create a _partitioned database_ (as opposed to an unpartitioned database) means that Cloudant uses a _partition key_ to decide on which shard each of your documents resides. Documents with the same _partition key_ will be located on the same database shard. Requests for `_all_docs`, MapReduce views, Cloudant Query `_find` queries and Cloudant Search operations can be directed to a single partition instead of having to interrogate all shards in a \"scatter/gather\" pattern, which is the case for _global queries_. \n\nThese _partitioned queries_ exercie only one shard of the database making them faster to execute than global queries and for billing purposes are classified as \"read\" requests instead of the more expensive \"query\" requests, allowing you to get more usable capacity from the same Cloudant plan.\n\nNot all data designs lend themselves to a partitioned design, but if your data can be molded into a `<partition key>:<document key>` pattern, then your application can benefit in terms of performance and cost.\n\n- [Partitioned Databases documentation](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-database-partitioning)\n- [Partitioned Databases - Introduction blog]({{< ref \"/2019-03-05-Partition-Databases-Introduction.md\" >}})\n\n## Rule 20: Use timeboxed databases for ever-growing data sets\n\nIt's generally _not_ a good idea to have an ever-growing database in Cloudant. Very large databases can be difficult to backup, require \"re-sharding\" to maintain good performance as they grow and suffer from long index build times. \n\nOne way of mitigating this problem is to have several smaller databases instead, with a very common pattern being _timeboxed databases_: a large data set is split into smaller databases, each representing a time window e.g. a month.\n\n- `orders_2019_01`\n- `orders_2019_02`\n- `orders_2019_02`\n\nNew data is written to _this month's database_ and queries for historical data can be directed to previous months' databases. When a month's data is no longer of interest, it can be archived to Object Storage, the monthly Cloudant database deleted and the disk space recovered.\n\n- [Time-series Data Storage blog]({{< ref \"/2019-04-08-Time-series-data-storage.md\" >}})\n\n## Rule 21: Logging helps you see what's going on\n\nCloudant's logs indicating each API call made, what was requestede and how long it took to respond can be automatically spooled to LogDNA for analysis and reporting for IBM Cloud-based services. This data is useful to keeping an eye on request volumes, performance and whether your application is exceeding your Cloudant service's provisioned capacity.\n\nThe logging service is easy to setup and free to get started. Paid-for plans allow data to be parsed, retained and archived to Object Storage. Slices and aggregations of your data can be built up into visual dashboards to give you an at-a-glance view of your Cloudant traffic.\n\n- [Cloudant Logging with LogDNA blog]({{< ref \"/2019-09-13-Cloudant-Logging-with-LogDNA.md\" >}})\n\n## Rule 22: Compress your HTTP traffic\n\nCloudant will compress its JSON responses to you if you supply an HTTP header in the request indicating that your code can handle data in this format:\n\n```\nRequest:\n> GET /cars/_all_docs?limit=5&include_docs=true HTTP/2\n> Host: myhost.cloudant.com\n> Accept: */*\n> Accept-Encoding: deflate, gzip\n\nResponse:\n\n< HTTP/2 200 \n< content-type: application/json\n< content-encoding: gzip\n```\n\nCompressed content occupies a fraction of the size of the uncompressed equivalent, meaning that it takes a shorter time to transport the data from Cloudant's servers to your application.\n\n> Note you may also choose to compress HTTP _request_ bodies too by using the [Content-encoding](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Encoding) header. This help lower data transfer times when writing documents to Cloudant.\n\n## Rule 23: Treat the primary index as a free search index\n\nA default Cloudant document `_id` is a 32 character string, encoding 128 bits of \nrandom data. The `_id` attribute is used to construct the database's _primary index_ which used by Cloudant to retreive documents by `_id` or _ranges_ of keys when the user supplies a `startkey`/`endkey` pair. We can leverage this fact to pack _our data_ into the `_id` field  and use it as \"free\" index which we can query for ranges of values.\n\nHere are some examples:\n\n- Use time-sortable document ids so that your documents are sorted into rough date/time order. This makes it easy to retrieve recent additions to the database. See [Time Sortable Document Ids]({{< ref \"/2018-08-24-Time-sortable-document-ids.md\" >}}).\n- Pack searchable data into your `_id` field e.g. `<customerid>~<date>~<orderid>` can be used to retrieve data by customer, customer/date or customer/date/orderid.\n- In a partitioned database, the judicious choice of _partition key_ allows an entire database to be winnowed down to a handful of documents for a known partition key. Make sure your partitioning schema solves your most common use-case.\n- In a partitioned database, the two parts of the key have to contain your own user-supplied data (there's no auto-generated \\_ids) so it's best to use it optimally e.g. in an IoT application `<sensorid>:<time-sortable-id>` allows data to be sorted by sensor and time without a secondary index. Implement this schema with _time-boxed databases_ for best results.\n\n## Thanks\n\nThanks to Glynn Bird, Robert Newson, Richard Ellis, Mike Broberg, and Jan Lehnardt for helpful comments. Any remaining errors are mine alone, and nothing to do with them.\n",
    "url": "/2019/11/21/Best-and-Worst-Practices.html",
    "tags": "Best-practice",
    "id": "75"
  },
  {
    "title": "Timing HTTP Requests",
    "description": "Detailed HTTP timings for your Node.js application",
    "content": "\n\n\nBuilding a Cloudant-based application with Node.js means that your app will making many HTTP requests to your Cloudant service. It's important to understand how such HTTP traffic is coordinated in Node and to be able to measure the anatomy of each request so that you can measure latencies and eliminate unwanted delays.\n\n![connections]({{< param \"image\" >}})\n> Photo by [John Barkiple on Unsplash](https://unsplash.com/photos/l090uFWoPaI)\n\nIn this post we'll be using the [Official Node.js Cloudant library](https://www.npmjs.com/package/@cloudant/cloudant), exploring HTTP connection pooling and finding out how to instrument your Node.js app.\n\n## The anatomy of an HTTP request\n\nWhen your app decides to make an HTTP request, a number of tasks are performed in sequence:\n\n- DNS lookup - your Cloudant domain name is resolved to an IP address using a DNS resolver.\n- Socket connection - a TCP socket is established between your app and the remote Cloudant service on port 443, the default port for HTTPS traffic.\n- TLS handshake - the client and server negotiate which version of Transport Layer Security (TLS) is to be used to encrypt the traffic, keys are exchanged and the server's certificate is tested for authenticity.\n- Client request - the HTTP request, headers and body are sent to the server over the encrypted connection.\n- Client response - the server's response is sent back to the client.\n\nWhen your app decides to make a second request, it can _reuse_ the existing socket and avoid the DNS lookup, socket connection and TLS handshake steps. \n\n## HTTP Connection Pooling\n\nAn HTTP socket can only host one HTTP request at anyone one time, but a Node.js app can utilise several sockets at once - known as a connection pool. By default, the Cloudant Node.js library is configured to use a pool of six sockets - this means that six HTTP connections can be in flight at any one time. \n\nWhen you issue a Cloudant API call in your Node,js app, it queues inside the app until a socket becomes free in the pool. \n\n![http request 1](/img/httprequest1.png)\n\nIf you have more requests in flight than you have sockets in the pool, then some requests will be delayed in your app waiting for a socket to become available. The key to discovering if you are adding latency to your requests by not having enough free sockets is to record detailed metrics.\n\n## Recording HTTP metrics\n\nWhen we instantiate the `@cloudant/cloudant` Node.js library we can switch on detailed timings by supplying `{time: true}` to the `requestDefaults` object:\n\n```js\nconst cloudantOpts = {\n  url: process.env.COUCH_URL,\n  requestDefaults: {\n    time: true\n  }\n}\nconst Cloudant = require('@cloudant/cloudant')\nconst cloudant = Cloudant(cloudantOpts)\n```\n\nTo access this timing information we need to add a custom plugin to the Cloudant library to intercept each request:\n\n```js\nconst Cloudant = require('@cloudant/cloudant')\nconst { BasePlugin } = Cloudant \n\n/**\n * Timer plugin.\n * Has access to fine-grained timings for each HTTP request.\n */\nclass TimerPlugin extends BasePlugin {\n  constructor(client, cfg) {\n    super(client, cfg)\n    this.id = 'timer'\n  }\n\n  onResponse(state, response, callback) {\n    response.request.on('end', function () { \n      console.log(response.request.method, response.request.path, response.statusCode)\n      console.log(response.timingPhases)\n    })\n    callback(state)\n  }\n\n  onError(state, error, callback) {\n    callback(state)\n  }\n}\n\nmodule.exports = TimerPlugin\n```\n\n> Note: Requires version 4.2.3 or above of the [Node.js Cloudant](https://www.npmjs.com/package/@cloudant/cloudant) library.\n\nWe need to tell the Cloudant library about our plugin at startup:\n\n```js\nconst timerPlugin = require('./timerplugin.js')\nconst cloudantOpts = {\n  url: process.env.COUCH_URL,\n  requestDefaults: {\n    time: true\n  },\n  plugins: [timerPlugin]\n}\nconst Cloudant = require('@cloudant/cloudant')\nconst cloudant = Cloudant(cloudantOpts)\n```\n\nOur plugin's `onResponse` function is called when Cloudant responds to an HTTP request. At that point we set up a function to wait for the `end` event (signifying the end of the of HTTP response) which then `console.log`s a `response.timingPhases` object containing timings for each phase of the HTTP request:\n\n\n```js\n{\n  wait: 14.766608999999988,\n  dns: 66.622751,\n  tcp: 127.91263899999998,\n  firstByte: 288.22381900000005,\n  download: 54.45547700000003,\n  total: 551.981295\n}\n```\n\nIf two HTTP requests are dispatched in sequence in our app, the second request should have a quite different `timingPhases` profile:\n\n```js\n{\n  wait: 0.5625499999999874,\n  dns: 0,\n  tcp: 0,\n  firstByte: 139.43548199999998,\n  download: 30.81753900000001,\n  total: 170.81557099999998\n}\n```\n\nIt has zero or near zero delays in the wait/dns/tcp phases because it reuses the socket opened during the first request.\n\n> Note: In my example code I am simply outputting timings to the console, but in your case you would want to store the details of the request and the timing data in your chosen monitoring package.\n\n## Analyzing the timings\n\nTo minimise latencies, your application needs to be as geographically close to its Cloudant service, preferably in the same data centre. \n\nIt's also important to note that some Cloudant HTTP requests are intentionally long e.g. long-polling the Cloudant changes feed. Be careful not to include such requests in your average calculations!\n\n### wait\n\nIf you are seeing values of `wait` greater than a millisecond for anything but the app's first request (per socket) then you may have more HTTP requests in flight than you have available sockets. See the next section on _increasing maxSockets_ to remedy this. \n\nYou may also see longer values of `wait` if you are [Blocking the Event Loop](https://nodejs.org/ru/docs/guides/dont-block-the-event-loop/) by executing long-running synchronous code which doesn't allow Node.js enough processor cycles to service network IO.\n\n### dns\n\nThe DNS lookup is a one-time operation per socket and should only take a few tens of milliseconds.\n\n### tcp \n\nThis is another one-off cost per socket whose speed will depend on the proximity of your Cloudant service. A transatlantic TLS negotiation may take 150ms.\n\n### firstByte\n\nThis is a measure of how long it took to get your app's request to Cloudant plus how long it took for Cloudant to begin to respond. Note that it includes network latency between you and Cloudant, so a close proximity to Cloudant will help to minimise this number. A large value here may indicate an inefficient query that takes Cloudant a while to answer or a large request size (e.g. uploading a binary attachment).\n\n### download\n\nThis is a measure of the time taken to spool Cloudant's response to you. It depends on the network latency and the volume of data being requested from Cloudant. The Node.js Cloudant library asks Cloudant to _zip_ JSON responses, which helps to minimise this measurement.\n\n### total\n\nThe entire HTTP timing from start to finish. \n\n## This works for any HTTP request\n\nThis timing trick isn't exclusive to the Node.js Cloudant library - it's actually baked into the Node.js `request` library, so any HTTP request you do can be timed in this way:\n\n```js\nconst request = require('request').defaults({time: true})\nrequest.get('https://google.com', function(err, res, body) {\n  console.log(res.timingPhases)\n})\n```\n\n## Increasing maxSockets\n\nIf you anticipate having more than six HTTP requests in flight in your Node.js app, or you are seeing significant values of `wait` from your HTTP latency monitoring, then you may need to increase the size of `maxSockets` - the maximum number of sockets used in the HTTP connection pool.\n\nIt is easily configured within the Node.js Cloudant library:\n\n```js\nconst timerPlugin = require('./timerplugin.js')\nconst https = require('https')\nconst myagent = new https.Agent({\n  keepAlive: true,\n  keepAliveMsecs: 30000,\n  maxSockets: 50\n})\nconst cloudantOpts = {\n  url: process.env.COUCH_URL,\n  requestDefaults: {\n    time: true,\n    agent: myagent\n  },\n  plugins: [timerPlugin]\n}\n```\n\nWe create our own `https.Agent` configured to keep sockets alive for thirty seconds and a custom value of `maxSockets`, in this case `50`.\n\n\n\n\n",
    "url": "/2019/12/06/Timing-HTTP-Requests-in-Nodejs.html",
    "tags": "HTTP Performance",
    "id": "76"
  },
  {
    "title": "Filtered Replication",
    "description": "Replicating while leaving behind deletions, design docs or anything you like.",
    "content": "\n\n\nCloudant's replication protocol allows data to flow from one Cloudant database to another, on the same Cloudant service or to an entirely separate service on the other side of the world. The replication protocol is also understood by [Apache CouchDB](https://couchdb.apache.org/) and [PouchDB](https://pouchdb.com/) allowing hybrid and mobile apps to be created with Cloudant acting as the cloud-based source of truth. The changes feed itself is also used to stream data to external services such as [couchwarehouse](https://github.com/glynnbird/couchwarehouse).\n\n![filter]({{< param \"image\" >}})\n> Photo by [Karl Fredrickson on Unsplash](https://unsplash.com/photos/TYIzeCiZ_60)\n\nIn the cases where not all the data in a Cloudant database is required to be replicated, a JavaScript _filter_ or Cloudant Query _selector_ can be defined to act as a gatekeeper to decide which documents are replicated. In this post we'll see how such selectors are setup and some common use-cases.\n\n## Setting up replication\n\nInitiating a replication is a simple as sending a JSON document to your Cloudant's `_replicator` database. The document defines the `source` database and the `target` database together with any authentication credentials required.\n\n```js\n{\n  \"_id\": \"myfirstreplication\",\n  \"source\" : \"http://<username1>:<password1>@<account1>.cloudant.com/<sourcedb>\",\n  \"target\" : \"http://<username2:<password2>@<account2>.cloudant.com/<targetdb>\"\n}\n```\n\n- The username/password for the source database needs a minimum of `_reader` & `_replicator` roles.\n- The username/password for the target database needs  a minimum of `_reader`/`_writer` roles. If you intend design documents to be replicated too, then the target user needs the `_admin` role too. \n- It's good practice to specify the `_id` value so that you can see at-a-glance which replication job is which.\n\nInstead of running replications as your Cloudant account's admin user, it's better to create [API Keys](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-authorization#api-keys) which can be used as the username/password for your replication process. It's always best to run with the minimum permissions needed to do the job.\n\nThe replication will start in due course and you can watch the progress by pulling the document (`GET /_replicator/myfirstreplication`) and examining the [extra attributes](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-replication-guide#monitoring-replication-status). The replication can be stopped by deleting the replication document (`DELETE /_replicator/myfirstreplication?rev=<rev>`).\n\n## Adding a replication selector\n\nTo only allow a subset of documents to be replicated, a `selector` object can be added to your replication document when creating the replication document:\n\n```js\n{\n  \"_id\": \"myfirstreplication\",\n  \"source\" : \"http://<username1>:<password1>@<account1>.cloudant.com/<sourcedb>\",\n  \"target\" : \"http://<username2:<password2>@<account2>.cloudant.com/<targetdb>\",\n  \"selector\": {\n    \"$or\": {\n      \"author\": \"Virginia Woolf\",\n      \"year\": {\n         \"$lt\": 1900\n      }\n    }\n  }\n}\n```\n\nIn this case only document which have an 'author' attribute of `Virginia Woolf` or a `year` attribute _less than_ 1900 will be replicated. The selector can contain any valid [Cloudant Query syntax](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-query#selector-syntax), and as it operates on every document in the the changes feed, it doesn't have to be backed by a suitable index. \n\n> Note: A _selector_-based replication filter is more efficient than the [JavaScript-based filter functions](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-advanced-replication#filtered-replication-adv-repl) as Cloudant can evaluate whether a document passes a selector without having to spin up a JavaScript process.\n\n## Ignoring deletions\n\nCloudant stores deletions as an additional revision to an existing document. This means that a [tombstone document](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-documents#tombstone-documents) remains after document deletion. To clean up tombstones, a database can be replicated to a new empty database, but _ignoring deleted documents_. This leaves the target database free of tombstones, using less disk space and with a de-cluttered primary index.\n\nA _selector_ to filter out tombstones is:\n\n```js\n\"selector\": {\n  \"_deleted\": {\n    \"$exists\": false\n  }\n}\n```\n\nwhich translates as \"only replicate documents where the attribute `_deleted` is not present\".\n\n## Ignoring design documents\n\nSometimes the purpose of replication is to take a backup of the _data_ in the database, but the design documents need to be filtered out so that they don't trigger the building of indexes on the target service.\n\nYou _could_ use a  selector to filter out Design Documents:\n\n```js\n\"selector\": {\n  \"$not\": {\n    \"_id\": {\n      \"$regex\": \"^_design\"\n    }\n  }\n}\n```\n\nwhich translates as \"only replicate documents whose _id fields does NOT begin with _design\", but a more common approach is to rely on the fact that to be able to write design documents at the target end a user/api-key with an `_admin` role is required. So one way of ensuring that design documents ARE NOT written is to run the replication as a non-admin user by creating an [API Key](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-authorization#api-keys) with only `_reader`/`_writer` roles. \n\n## Custom selectors\n\nIf you only need to replicate a sub-set of data to the target, then you can devise any Cloudant Query selector you need e.g.\n\n```js\n\"selector\": {\n  \"type\": \"order\",\n  \"status\": \"complete\",\n  \"date\": { \"gte\": \"2019-01-01\" }\n}\n```\n\nYou can combine your custom selector with an off-the-shelf selector to filter out deletions:\n\n```js\n\"selector\": {\n  \"$and\": [\n     \"_deleted\": {\n      \"$exists\": false\n     },\n    \"type\": \"order\",\n    \"status\": \"complete\",\n    \"date\": { \"gte\": \"2019-01-01\" }\n  ]\n}\n```\n\nThis can be run as a non-admin user to filter out design documents too.\n\n## Measure twice, cut once\n\nBefore running a filtered replication on your production data, it's worth making sure your logic works on a smaller data set:\n\n- Create a new database `a` containing a suitable document that you would expect to be replicated, a document that you would not expect to be replicated, a design document and a deleted document.\n- Kick off a replication to a new empty database `b` with your custom selector.\n- Ensure that your target database has the right documents in it. \n\nIf all is well, you can move on to your live replication.",
    "url": "/2019/12/13/Filtered-Replication.html",
    "tags": "Replication Filter",
    "id": "77"
  },
  {
    "title": "Fast Data Transfer",
    "description": "Copying data faster than replication",
    "content": "\n\n\nCloudant replication is the database's method of of choice for transferring data from a _source_ to a _target_ database. It's main use-cases are:\n\n- Mobile apps - keeping mobile application data synched with a server-side copy where the data can be modified at both sides.\n- Cross-region sync - a database can exist in two geographies and traffic can be load balanced between them, routed so that a customer is directed to their nearest copy or can be used for disaster recovery fail-over.\n- One-off data transfer - if data needs to be moved from one CouchDB/Cloudant service to another.\n- Removing tombstones - deleted Cloudant documents leave a \"tombstone\" document behind which can't be deleted without replicating data to a new database and ignoring the deletions.\n- Backup - taking a copy of valuable data: in some cases only the data and not the design documents that trigger index builds need to be retained.\n\nReplication is easy to set up, can be run as a one-off or continuous operation and can be resumed from its last position. For all of its good points, replication does have some drawbacks:\n\n- Conflict removal - any [conflicted documents]({{< ref \"/2018-07-25-Removing-Conflicts.md\" >}}) on the source are carefully retained on the target side.\n- Document alteration - the documents can not be changed in flight. If, for example, you wish to replicate data from a non-partitioned database to a partitioned database, altering the `_id` field on the way, this cannot be achieved with replication.\n- Speed - replication's focus on transferring everything from the source to the target (winning revisions, conflicts, deletions and attachments) makes it precise but not as fast as simple data transfer could be.\n\n![fire hose]({{< param \"image\" >}})\n> Photo by [Juliana Kozoski on Unsplash](https://unsplash.com/photos/X3-IypGOGSE)\n\n## How fast is replication?\n\nIt depends how many documents you have, how big they are, how many of them are conflicted, how many attachments there are, how big _they_ are, how many deletions you have, the reads-per-second capacity at the source, the writes-per-second capacity at the target, network bandwidth, how geographically close the source and target are etc.\n\nAs an indicative example, I was able to transfer 500,000 documents (around 700 bytes each) in around 300 seconds. This number is highly dependent on the provisioned capacity of the Cloudant service you have. A free \"Lite\" account, for example,  can only transfer 20 documents per second because it is rate-limited to 20 reads per second. \n\nIf we're prepared to make some assumptions and drop some of replication's features, we can achieve a faster data transfer than that using _couchfirehose_, a command-line utility that transfers data from a source to a target database without using replication.\n\n## How do I install couchfirehose?\n\nSimply run:\n\n```sh\n> npm install -g couchfirehose\n```\n\n## How do a I transfer data using couchfirehose?\n\nLet's set up our Cloudant URL, including authentication credentials, in an environment variable:\n\n```sh\n> export URL=\"https://myusername:mypassword@mycloudantservice.cloudant.com\"\n```\n\nWe can then use the `URL` variable in our next command:\n\n```sh\n> couchfirehose --source \"$URL/mysourcedb\" --target \"$URL/mytargetdb\"\n```\n\nAs well as `--source` and `--target` there are other parameters we can use to customise the data transfer:\n\n- `--source`/`-s` - the URL of the source Cloudant database, including authentication.\n- `--target`/`-t` - the URL of the target Cloudant database, including authentication.\n- `--batchsize`/`-b` - the number of documents per write request. (Default 500)\n- `--concurrency`/`-c` - the number of writes in flight. (Default 2)\n- `--maxwrites`/`-m` - the maximum number of write requests to issue per second. (Default 5)\n- `--filterdesigndocs`/`--fdd` - whether to omit design documents from the data transfer. (Default false)\n- `--filterdeletions`/`--fd` - whether to omit deleted documents from the data transfer. (Default false)\n- `--resetrev`/`-r` - omits the revision token, resetting the targets revisions to `1-xxx'. (Default false)\n- `--transform` - the path of synchronous JavaScript transformation function. (Default null)\n- `--selector` - a selector query used to filter the source's documents. (Default null)\n\ne.g.\n\n```sh\n> # filter out deleted documents\n> couchfirehose --source \"$URL/mysourcedb\" --target \"$URL/mytargetdb\" --fd true\n> # larger batch size\n> couchfirehose --source \"$URL/mysourcedb\" --target \"$URL/mytargetdb\" -b 2000\n> # ensure that only five writes are made per second\n> couchfirehose --source \"$URL/mysourcedb\" --target \"$URL/mytargetdb\" -m 5\n> # allow eight writes to be in flight at any one time\n> couchfirehose --source \"$URL/mysourcedb\" --target \"$URL/mytargetdb\" -c 8\n```\n\nBy experimenting with these parameters, it's possible to see a four-fold increase in speed compared with replication, all though it's worth remembering that this **is not replication**:\n\n- conflicts, attachments and optionally deletions/design-docs are left behind.\n- there maybe unexpected results if the source data is non-static or the target database is not empty.\n- the _couchfirehose_ process is not \"resumable\".\n\n## Advanced features\n\nWe don't have to transfer all of the source data to the target if we don't want to. By supplying a [Cloudant Query Selector](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-query#selector-syntax) as the `--selector` parameter, the source data will be filtered according to the query e.g.\n\n```sh\n> # only transfer completed orders\n> couchfirehose --source \"$URL/s\" --target \"$URL/t\" --selector '{\"status\":\"completed\"}'\n> # only transfer last year's data that is not null\n> couchfirehose --source \"$URL/s\" --target \"$URL/t\" --selector '{\"year\":2018,\"value\":{\"$ne\":null}}'\n```\n\nWe can also supply a custom JavaScript function to the `--transform` parameter, where the function transforms the source document prior to it being written to the target. Create your function in a file:\n\n```js\nmodule.exports = (doc) => {\n  // delete unwanted field\n  delete doc.deprecatedAttribute\n  // add a new field\n  doc.newAttribute = 'new'\n  // coerce the type of a field\n  if (typeof doc,price === 'string') {\n     doc.price = parseFloat(doc.price)\n  }\n  // modify the _id as we're moving to a partitioned database\n  doc,_id = doc.userid + ':' + doc._id\n  // ignore zero value transactions\n  if (doc.price === 0) {\n     // if we return null, the document is not written to the target\n     return null\n  }\n  return doc\n}\n\nmodule.exports = f\n```\n\nThe _transform_ feature can be used to correct data, add new attributes, remove unwanted attributes and for migrations from a non-partitioned database to a partitioned database, modify the `_id` field to contain a partition key. \n",
    "url": "/2020/01/17/Fast-Data-Transfer.html",
    "tags": "Replication Transfer",
    "id": "78"
  },
  {
    "title": "Building a Store Finder",
    "description": "Sorting documents be distance from a point",
    "content": "\n\n\nIn this post we'll be creating a web-based _store finder_ using Cloudant. A user visits your website and wants to know which of your branches is closest to their current location.\n\n![shop]({{< param \"image\" >}})\n> Photo by [Sherzod Max on Unsplash](https://unsplash.com/photos/edZ_WxeUlWc)\n\nTo build this need a search index that can sort search results by distance but first, we need a database of branches.\n\n## Data preparation\n\nLet's say we have a list of branches that looks like this:\n\n```txt\n130 Turves Road, Cheadle Hulme, Cheadle, Cheshire, SK8 6AW\n2-8 High Street, Witney, Oxfordshire, OX28 6HA\n34 Church Wk, Caterham, Surrey, CR3 6RT\n49 Calverley Road, Tunbridge Wells, Kent, TN1 2UU\n8 Pitsea Centre, Northlands Pavement, Pitsea, Basildon, Essex, SS13 3DU\n36 The Broadway, London, E15 1NG\n14-16 Station Road, West Drayton, Middlesex\n```\n\nOur first task is to split the addresses into meaningful columns:\n\n| address1                                              | address2                         | address3                 | city                 | county             | postcode |\n|",
    "url": "/2020/05/01/Building-A-Store-Finder.html",
    "tags": "Search Geo",
    "id": "79"
  },
  {
    "title": "Optimising Cloudant Queries",
    "description": "Making the best index to match your query",
    "content": "\n\n\n[Cloudant Query](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-query) is a JSON-based query language inspired by MongoDB. It allows the developer to express the slice of data they need from a database using a mixture of logical and comparison operators.\n\nFor example, if the following JSON is sent to the database's `_find` endpoint:\n\n```js\nPOST /orders/_find\n\n{\n  \"selector\": {\n    \"$and\": [\n      { \"date\": { \"$gte\": \"2018-01-01\" } },\n      { \"date\": { \"$lt\": \"2019-01-01\" } },\n      { \"status\": \"cancelled\" },\n      { \"user\": \"bob@aol.com\" }\n    ],\n  },\n  \"limit\": 5\n}\n```\n\nthen the first five documents that match all of the following clauses will be returned:\n\n- The order must have been created in 2018.\n- Have a status of 'cancelled'.\n- Belong to a known user `bob@aol.com`.\n\nBy default, when given this query Cloudant will have to scan each of the database's documents in turn to see if they match the _selector_ until it has the five results that it needs. If there are fewer than five such documents in the database, then Cloudant will have performed a _full database scan_ to get the answer. \n\n![haystack]({{< param \"image\" >}})\n> Photo by [Peter Kleinau on Unsplash](https://unsplash.com/photos/oP_xD70TpsI)\n\nThis may be fine during development of your application, but if you want great performance as your data size and traffic increases, then we need to define a suitable _secondary index_ to help Cloudant get the answer without reading every document in the database.\n\n## Explain - which index is being used?\n\nHow can we tell if a query is using a secondary index or \"flying blind\" and scanning each document in turn? We can send our query to the database's `_explain` endpoint (instead of `_find`):\n\n```js\nPOST /orders/_explain\n\n{\n  \"selector\": {\n    \"$and\": [\n      { \"date\": { \"$gte\": \"2018-01-01\" } },\n      { \"date\": { \"$lt\": \"2019-01-01\" } },\n      { \"status\": \"cancelled\" },\n      { \"user\": \"bob@aol.com\" }\n    ],\n  },\n  \"limit\": 5\n}\n```\n\nThe returned data contains an explanation of which index, if any,  _would_ be selected:\n\n```js\n{\n  ...\n  \"index\": {\n    \"ddoc\": null,\n    \"name\": \"_all_docs\",\n    \"type\": \"special\",\n    \"def\": {\n      \"fields\": [\n        {\n          \"_id\": \"asc\"\n        }\n      ]\n    }\n   }\n   ...\n}\n```\n\nIn the above example, no secondary index is capable of helping the query - Cloudant is falling back on `_all_docs`, that is scanning each document in turn.\n\n## Secondary indexing\n\nA secondary index is an extra data structure that sits alongside the core database documents and is ordered by one or more attributes in the document. If we were extracting data by `date` then we may elect to create an index on the `date` field, by posting some JSON to the database's `_index` endpoint:\n\n```js\nPOST /orders/_index\n\n{\n  \"index\": {\n    \"fields\": [\"date\"]\n  },\n  \"name\": \"ordersByDate\",\n  \"type\": \"json\"\n}\n```\n\nThis is an instruction for Cloudant to begin creating a secondary index on the `date` field. It does so by doing a on-off, full-database scan to create a data structure that links `date` to the document's `_id`. \n\n![indexing](/img/indexing.png)\n\nOnce built, this date-ordered index can help reduce query times when asking for:\n\n- Orders on a supplied date.\n- Orders before a supplied date.\n- Orders after a supplied date.\n- Orders created between two dates.\n\nBut is \"date\" the right field to index in this case?\n\n## Which attribute should I index?\n\nThe choice of attribute or attributes to index depends on the distribution of values within your database and the query you're making. \n\nFor a database containing 100,000 orders, let's look at the distribution of orders by year in this dataset: \n\n| year | # orders |\n|",
    "url": "/2020/05/20/Optimising-Cloudant-Queries.html",
    "tags": "Querying Indexing",
    "id": "80"
  },
  {
    "title": "Automatic Cross-region Failover",
    "description": "Using CIS Edge functions switch between Cloudant services",
    "content": "\n\n\nIBM Cloudant NoSQL database offers high availability through in-region automatic data redundancy by storing data in triplicate across three servers within a single region. In a true sense, achieving high availability and eliminating single point failure couldn’t be accomplished using a single region Cloudant setup. So, to address this, we have to consider doing two things:\n\n1. Configure cross-region redundancy by setting up Cloudant accounts in two or more regions, create databases in each region, and set up bidirectional continuous replications between the corresponding databases in each account.\n2. Configure automatic fail-over between IBM Cloudant regions using Cloud Internet Service (CIS) Edge function in IBM Cloud and will discuss more on this.\n\n![]({{< param \"image\" >}})\n> Photo by [NASA on Unsplash](https://unsplash.com/photos/Q1p7bh3SHj8)\n\n> **Note:** For this article, I assume that the cross-region redundancy (with two regions) is already setup with active-passive configuration. For more information about setting this up, see [Configuring IBM Cloudant for cross-region disaster recovery](https://cloud.ibm.com/docs/Cloudant?topic=cloudant-configuring-ibm-cloudant-for-cross-region-disaster-recovery#configuring-ibm-cloudant-for-cross-region-disaster-recovery). We will consider setting up active-passive configuration because of the following two regions:\n> -\tThe active-active configuration could cause an availability issue if one of the sites goes down.\n> -\tApplications that work within an active-active configuration must have a strategy for handling conflicts to avoid problems with multiple copies of data.\n\n## Target architecture with auto-failover mechanism:\n\n![](/img/failover.png)\n\n## How does this setup work?\n\nFirst, we have to configure two CIS load balancers (LB) which points to primary and secondary Cloudant regions respectively. In addition to this, we have to explicitly set the database hostname as the \"Host Header Override\" value inside the \"Page Rules\" option in the CIS dashboard. Without this, Cloudant won’t honour the request as it would expect the host header value as \"Cloudant database hostname\", whereas it will have the value as \"CIS load balancer hostname\". Adding \"Host Header Override\" solves one issue but creates another. As the value inside it is hardcoded, it couldn’t be dynamically changed, so, effectively the load balancer starts behaving like a reverse proxy and can’t load balance between the primary or secondary Cloudant cluster. To get around this problem, we could use the CIS Edge function which could intercept any incoming request for a specific LB URL, before it reaches the actual LB and we could inject custom routing logic in there.\n\nAs shown in the above diagram, the CIS Edge function intercepts the primary CIS LB URL and checks if the primary site/region is up or not by pinging a database document inside it. If the primary site is up, CIS Edge function forwards the call to the primary Cloudant site. If the primary site is nonresponsive and the ping fails, then the request would be redirected to the secondary site/region automatically. As this check would be done on every incoming request to the CIS LB URL, it would add network latency, which could be minimized by adding a monitoring logic which would only check if the site/region is up or not in a specific time interval. For this example, we would use 15 seconds time interval, which means the CIS Edge function will check every 15 seconds if the site is up or not and accordingly forward the request to either primary or secondary Cloudant site.\n\n## Steps Involved:\n\n### Step 1\n\nConfigure primary CIS LB with Cloudant URL for the primary site and set the database hostname as the \"Host Header Override\" value inside \"Page Rules\" option in the CIS dashboard.\nOn the CIS dashboard, you'll see three lists that show the load balancers, origin pools, and health checks.\n\n1. First create a \"health check\"\n2. Create an origin pool and select the health check entry created in the above step.\n3. Create load balancer and select the origin pool created in the above step\n4. Create a page rule to set the Host Header Override for the LB URL\n   * In the CIS dashboard navigate to Performance -> Page Rules.\n   * Create a page rule for the LB URL, for example, https://primarycislb.ibm.net/*\n   * Select the Rule Behaviour setting Host Header Override.\n   * Set as the Cloudant database hostname, for example, primaryacct.cloudant.com.\n\n> **Note :** For more information on setting up the load balancer, follow this [Link](https://cloud.ibm.com/docs/cis?topic=cis-set-up-and-configure-your-load-balancers)\n\n### Step 2\n\nConfigure secondary CIS LB with Cloudant URL for the secondary site and set the database hostname as the \"Host Header Override\" value inside \"Page Rules\" option in the CIS dashboard.\nFollow the Steps mentioned above.\n\n### Step 3\n\nCreate an Action under the CIS Edge function, which would check if the primary Cloudant site is up or not in a specific interval of time. e.g 15 seconds. If it is up, then the request is routed to the primary Cloudant site, if it is down than the request automatically gets forwarded to the secondary site.\n\n- Open the \"Edge Functions\" page from the CIS instance dashboard.\n- Click the \"Create\" icon on the \"Actions\" tab.\n- Enter \"cloudant-db-route\" in the action name field.\n- Paste the following source code into the action body section and click the \"Save\" button.\n\n```javascript\n\naddEventListener('fetch', event => {\n  event.respondWith(handleRequest(event.request));\n});\n\n// Global variable.\nlet lastPingTime = 0;\nlet result = '';\n\nconst headerInfo = {\n  'Authorization': 'Base64 encoded Cloudant credential'\n};\nconst config = {\n  method: 'HEAD',\n  headers: headerInfo\n};\n\nasync function handleRequest(request) {\n  let currentPingTime = Math.floor(Date.now() / 1000);\n  if ((currentPingTime - lastPingTime) >= 15) {\n    lastPingTime = currentPingTime;\n    // Ping database document in Primary cluster\n    let dbPing = await fetch('https://primaryacct.cloudant.com/cloudant-db-name/document-id', config);\n    if (dbPing.status == 200) {\n      result = true; /* Switch to Primary */\n    } else {\n      result = false; /* Switch to Secondary */\n    }\n  }\n    \n  if (result) {\n    return fetch(request);\n  } else {\n    // Considering secondarycislb.ibm.net as the second CIS LB URL\n    let url = request.url.replace('primarycislb',  'secondarycislb');\n    let new_url = new URL(url);\n    const modifiedRequest = new Request(new_url, {\n      method: request.method,\n      headers: request.headers\n    });\n    return fetch(modifiedRequest);\n  }\n} \n```\n\n### Step 4\n\nCreate a Trigger under the CIS Edge function which would intercept the primary CIS LB URL and invoke the Action we have defined in the earlier step.\n\n-\tSelect the \"Triggers\" panel from the Edge Functions page.\n-\tClick the \"Add trigger\" icon.\n-\tSet the Trigger URL to https://primarycislb.ibm.net/*\n-\tSelect the \"cloudant-db-route\" action from the drop-down menu.\n-\tClick the \"Save\" button.\n\n### Step 5\n\nTo check if the failover is working or not, you could temporarily delete the db document in the primary Cloudant cluster which CIS Edge function pings and will return error code other than 200. This would automatically cause the traffic route to secondary cluster within 15 seconds in this case. You could select the db monitoring interval as per the NFR defined for your project.",
    "url": "/2020/07/01/Automatic-failover-between-Cloudant-regions.html",
    "tags": "Failover HA",
    "id": "81"
  },
  {
    "title": "Selective restoration from backup",
    "description": "Only restore the Cloudant documents you need",
    "content": "\n\n\nAs part of the disaster recovery process, the Cloudant NoSQL database offers the database backup and restore through the command-line utilities such as couchbackup & couchrestore respectively.\n\n- couchbackup - Dumps the JSON data from a database to a backup text file.\n- couchrestore - Restores data from a backup text file to a database.\n\nThe couchrestore utility has one limitation: it can restore the full database but can’t selectively restore specific records. So, to recover data caused by accidental deletion, we have to go with approaches such as restoring the data from the backup file to a new database and use either [filtered replication](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-advanced-replication#filtered-replication-adv-repl) or [named document replication](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-advanced-replication#named-document-replication) to retrieve the selected records.\n\nIn both approaches, it is required to restore the database first. This would make the data recovery process painfully slow in case the database contains millions of records and the database backup file size is in hundreds of GBs.\n\nTo address the problem of accidental deletion of database records, without following the lengthy process of restoring the full Cloudant database, we could use a stand-alone Node.js script. This script filters through the database backup file and find the relevant record/records and writes it to a file. Once we have the data in JSON format, we could insert those back to the database either manually or programmatically.\n\n![]({{< param \"image\" >}})\n> Photo by [Anthony Martino on Unsplash](https://unsplash.com/photos/6AtQNsjMoJo)\n\n> Note: The script could be reprogramed to insert the selective record/records back to the target database, instead of writing it to a file.\n\n## Sample NodeJS Script\n\n```javascript\n\nconst exec = require('child_process').exec;\nconst fs = require('fs');\n\n// File name to be read\nvar fileName = 'cloudant_db_backup.txt';\n\n// Get the id or ids from the command line arguments. 'val' contains the search string.\nprocess.argv.forEach(function(val, index, array) {\n  if (index > 1) {\n    // Construct the shell script command.\n    var shellCommand = 'grep -w ' + val + ' ' + fileName;\n    // increase buffer size if the backup file is pretty huge. For this example buffer size is set to 1500 KB.\n    exec(shellCommand, {\n      maxBuffer: 1024 * 1500\n    }, (err, stdout, stderr) => {\n      if (err) {\n        console.error('>>> ' + err);\n        return;\n      }\n      var lineArray = JSON.parse(stdout);\n      // Match the _id of individual document.\n      lineArray.filter(function(item) {\n        if (item._id == val) {\n          // Write the output to the file.\n          fs.writeFile('doc_' + item._id + '.json', JSON.stringify(item, null, 2), function(err) {\n            if (err) throw err;\n          });\n          console.log('>>> File created successfully');\n        }\n      });\n    });\n  }\n});\n```\n\n> Note: The above Node.js script uses shell script grep command instead of any available node module for searching text patterns because it is way faster compared to all and significantly reduces the search time for very very large database backup files.\n\n## How does the script work?\n\nTo demonstrate this example we'll need a database with some sample data in your account.\n\n### Step 1\n\nCreate a new database `mydb` and populate it with data.\n\n### Step 2 \n\nLogin to shell(should have nodejs installed) and take a backup of the above-mentioned database using \"couchbackup\" command.\n\ne.g. \n\n```sh\ncouchbackup --url https://username:password@account-name.cloudant.com --db mydb > /tmp/mydb.txt\n```\n\n### Step 3\n\nNow to demonstrate the situation of accidental deletion of data, we can delete two documents movies-demo database randomly and the script could restore the missing documents. For example delete the documents having `_id` _\"70f6284d2a395396dbb3a60b4ce22805\"_ and _\"70f6284d2a395396dbb3a60b4ce6aa97\"_.\n\n### Step 4\n\nIn the above Node.js script, change the `fileName` variable to _\"mydb.txt\"_, which is in this case is the name of the backup file and save it with name \"find-document.js\"\n\n### Step 5\n\nNow let's try to get back the deleted documents using the below command. This would produce two files with the content of the deleted documents in JSON format.\n\n```sh\nnode find-document.js 70f6284d2a395396dbb3a60b4ce22805 70f6284d2a395396dbb3a60b4ce6aa97\n```\n\n### Step 6\n\nNow we could copy the content of these two files and create the documents manually or using curl command in the mydb database. This script makes a difference and saves a lot of time when it comes to searching for records in backup files containing millions of records.\n",
    "url": "/2020/07/01/Selective-restoratio-from-backup.html",
    "tags": "Backup Restore",
    "id": "82"
  },
  {
    "title": "How to recover a deleted document",
    "description": "How to recover a document in Cloudant that has been deleted or overwritten",
    "content": "\n\n\n## Introduction\n\nThis article describes how you might be able to recover data in Cloudant after it has been deleted or overwritten.\n\n![image]({{< param \"image\" >}})\n\n> Photo by [Joshua Coleman on Unsplash](https://unsplash.com/photos/Cj8h7-b47ko)\n\nDeleting a Cloudant document leaves behind a so-called [tombstone](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-documents#tombstone-documents) - a shell of the original document containing only an `_id`/`_rev` pair and a `_deleted: true` flag. Soon after deletion (or after updating a document), the previous revision's document body is removed in a process called \"compaction\". This process runs automatically from time to time in the Cloudant service as an essential part of database maintenance. There is however, a short time window between updating/deleting a document and its body being compacted - if you know what you're doing, and you're quick, it's possible to recover the old document body before it is lost forever. \n\n## Examples\n\nTo follow the examples in this section, replace:\n\n-   `$USER` with the Cloudant account name\n-   `$PASS` with the password of $USER\n-   `$DB` with the name of the database.\n\n### How to recover a document that has been deleted\n\nHere are example steps which demonstrate how you might be able to recover a document after it has been deleted.\n\n1. Write a new document as an example:\n\n```shell\ncurl -u $USER:$PASS -X POST https://$USER.cloudant.com/$DB \\\n      -H \"Content-Type: application/json\" \\\n      -d '{   \n              \"_id\": \"example\",\n              \"data\": \"Your data here.\"\n          }'\n```\n  \nThe output I got:\n  \n```js\n{\"ok\":true,\"id\":\"example\",\"rev\":\"1-4a5958602638984def83a2075a86bc7a\"}\n```\n\nindicates that the revision of the new document in this example is: `1-4a5958602638984def83a2075a86bc7a`\n\n\n2. Delete the document: \n\n```shell\ncurl -u $USER:$PASS -X DELETE https://$USER.cloudant.com/$DB/example?rev=1-4a5958602638984def83a2075a86bc7a\n```\nThe output I got:\n  \n```js\n{\"ok\":true,\"id\":\"example\",\"rev\":\"2-45a4676f3cae54b3e7346d3a09dda771\"}\n```\n\nindicates that the new revision of the (now deleted) document in this example is: `2-45a4676f3cae54b3e7346d3a09dda771`\n\n\n3. These command outputs confirm that the document is now deleted:\n  \n```shell\n$ curl -s -u $USER:$PASS https://$USER.cloudant.com/$DB/example | jq .\n{\n  \"error\": \"not_found\",\n  \"reason\": \"deleted\"\n}\n```\n\n```shell\n$ curl -s -u $USER:$PASS https://$USER.cloudant.com/$DB/example?deleted=true | jq .\n{\n  \"_id\": \"example\",\n  \"_rev\": \"2-45a4676f3cae54b3e7346d3a09dda771\",\n  \"_deleted\": true\n}\n```\n\n4. This command uses the `revs_info=true` parameter to get the status of the document revisions:\n\n```shell\n$ curl -s -u $USER:$PASS https://$USER.cloudant.com/$DB/example?deleted=true\\&revs_info=true | jq .\n```\n  \nHere is the output I got:\n  \n```js\n{\n  \"_id\": \"example\",\n  \"_rev\": \"2-45a4676f3cae54b3e7346d3a09dda771\",\n  \"_deleted\": true,\n  \"_revs_info\": [\n    {\n      \"rev\": \"2-45a4676f3cae54b3e7346d3a09dda771\",\n      \"status\": \"deleted\"\n    },\n    {\n      \"rev\": \"1-4a5958602638984def83a2075a86bc7a\",\n      \"status\": \"available\"\n    }\n  ]\n}\n```\n  \nIt shows that the revision which immediately precedes the deleted revision is `1-4a5958602638984def83a2075a86bc7a`.<br>\nBecause compaction of this document has not yet run since the document was deleted, revision `1-4a5958602638984def83a2075a86bc7a` has status `\"available\"`. If that revision were no longer available, its status would be `\"missing\"`.\n  \n\n5. If its status is `\"available\"` you can still get the contents of revision `1-4a5958602638984def83a2075a86bc7a`:\n\n```shell\n$ curl -s -u $USER:$PASS https://$USER.cloudant.com/$DB/example?rev=1-4a5958602638984def83a2075a86bc7a | jq .\n{\n  \"_id\": \"example\",\n  \"_rev\": \"1-4a5958602638984def83a2075a86bc7a\",\n  \"data\": \"Your data here.\"\n}\n```\n\n6. Now you can write the contents of the revision back just as it was before the document was deleted. Do not include the `_rev` field.\n  \n```shell\ncurl -u $USER:$PASS -X POST https://$USER.cloudant.com/$DB \\\n      -H \"Content-Type: application/json\"  \\\n      -d '{\n              \"_id\": \"example\",\n              \"data\": \"Your data here.\" \n          }'\n```\n  \nThe output I got:\n\n```js\n{\"ok\":true,\"id\":\"example\",\"rev\":\"3-a95d2245a9f11e5fa62390c600204d18\"}     \n```\n\nindicates that the new revision in this example is: `3-a95d2245a9f11e5fa62390c600204d18     `\n\n7. This command confirms that the document is now live (not deleted):\n\n```shell\n$ curl -s -u $USER:$PASS https://$USER.cloudant.com/$DB/example | jq .\n{\n  \"_id\": \"example\",\n  \"_rev\": \"3-a95d2245a9f11e5fa62390c600204d18\",\n  \"data\": \"Your data here.\"\n}\n```  \n  \n### How to recover a document that has been overwritten\n\nHere are example steps which demonstrate how you might be able to recover the original data after a document has been updated.\n\n1. Update the example document, replacing the original data with new data.\n\n```shell\n  curl -u $USER:$PASS -X POST https://$USER.cloudant.com/$DB \\\n          -H \"Content-Type: application/json\" \\\n          -d '{   \n                  \"_id\": \"example\",\n                  \"_rev\": \"3-a95d2245a9f11e5fa62390c600204d18\",\n                  \"data\": \"New data that replaces the original data.\"\n              }'\n```\nThe output I got:\n\n```js\n{\"ok\":true,\"id\":\"example\",\"rev\":\"4-9515f5aa01411766cc8aed181af12c1c\"}\n```\n\nindicates that the new document revision in this example is: `4-9515f5aa01411766cc8aed181af12c1c`\n\n2. This command confirms that the original data in the document has been replaced by the new data:\n\n```shell\n$ curl -s -u $USER:$PASS https://$USER.cloudant.com/$DB/example | jq .\n{\n  \"_id\": \"example\",\n  \"_rev\": \"4-9515f5aa01411766cc8aed181af12c1c\",\n  \"data\": \"New data that replaces the original data.\"\n}\n```\n\n3. This command uses the `revs_info=true` parameter to get the status of the document revisions now:\n\n```shell\n$ curl -s -u $USER:$PASS https://$USER.cloudant.com/$DB/example?revs_info=true | jq .\n```\n  \nHere is the output I got:\n\n```js\n{\n  \"_id\": \"example\",\n  \"_rev\": \"4-9515f5aa01411766cc8aed181af12c1c\",\n  \"data\": \"New data that replaces the original data.\",\n  \"_revs_info\": [\n    {\n      \"rev\": \"4-9515f5aa01411766cc8aed181af12c1c\",\n      \"status\": \"available\"\n    },\n    {\n      \"rev\": \"3-a95d2245a9f11e5fa62390c600204d18\",\n      \"status\": \"available\"\n    },\n    {\n      \"rev\": \"2-45a4676f3cae54b3e7346d3a09dda771\",\n      \"status\": \"deleted\"\n    },\n    {\n      \"rev\": \"1-4a5958602638984def83a2075a86bc7a\",\n      \"status\": \"available\"\n    }\n  ]\n}\n```\n  \nIt shows that the revision which immediately precedes the latest revision is `3-a95d2245a9f11e5fa62390c600204d18`.<br>\nBecause compaction of this document has not yet run since the document was updated, revision `3-a95d2245a9f11e5fa62390c600204d18` has status `\"available\"`. If that revision were no longer available, its status would be `\"missing\"`.\n  \n\n4. If its status is `\"available\"` you can still get the contents of revision `3-a95d2245a9f11e5fa62390c600204d18`:\n\n```shell\n$ curl -s -u $USER:$PASS https://$USER.cloudant.com/$DB/example?rev=3-a95d2245a9f11e5fa62390c600204d18 | jq .\n{\n  \"_id\": \"example\",\n  \"_rev\": \"3-a95d2245a9f11e5fa62390c600204d18\",\n  \"data\": \"Your data here.\"\n}\n```\n\n5. Now you can write the contents of the revision back just as it was before the document was updated. This time you must include the latest `_rev` in the document you write.\n  \n```shell\ncurl -u $USER:$PASS -X POST https://$USER.cloudant.com/$DB \\\n      -H \"Content-Type: application/json\"  \\\n      -d '{\n              \"_id\": \"example\",\n              \"_rev\": \"4-9515f5aa01411766cc8aed181af12c1c\",\n              \"data\": \"Your data here.\" \n          }'\n```\n  \nThe output I got:\n\n```json\n  {\"ok\":true,\"id\":\"example\",\"rev\":\"5-0bdac581d633b76a696c2b4b3972c87d\"}\n```\n\nindicates that the new revision in this example is: `5-0bdac581d633b76a696c2b4b3972c87d`     `\n\n6. This command confirms that the document now contains the original data, as it was before it was updated:\n\n```shell\ncurl -s -u $USER:$PASS https://$USER.cloudant.com/$DB/example | jq .\n{\n  \"_id\": \"example\",\n  \"_rev\": \"5-0bdac581d633b76a696c2b4b3972c87d\",\n  \"data\": \"Your data here.\"\n}\n```  \n\n### How to find what documents have been deleted or overwritten\n\nTo find out what document ids have been deleted or overwritten, you can use the [changes feed](https://cloud.ibm.com/docs/services/Cloudant?topic=cloudant-databases#get-changes), which returns a list of changes that have been made to documents in the database, including insertions, updates, and deletions.\n\n\nFor example:\n\n```shell\ncurl -s -u $USER:$PASS https://$USER.cloudant.com/$DB/_changes\n```   \n",
    "url": "/2020/07/17/How-to-recover-a-deleted-document.html",
    "tags": "Compaction",
    "id": "83"
  },
  {
    "title": "JSON Schema Validation",
    "description": "Validating incoming JSON schemas with VDU functions",
    "content": "\n\n\n[JSON Schema](https://json-schema.org/) is a standard that allows you to specify the form of your JSON and allow programmatic validation of JSON against the specification.\n\nIn your application there would be a formal definition of the types of JSON being stored (e.g. users, orders, products etc) which could be used to verify objects prior to being allowed into the database. \n\nHaving a formal schema definition has several advantages:\n\n- It can be used to validate documents prior to being stored in the databases. If it doesn't match the specification, it's not allowed in.\n- The schema can be used to automatically generate documentation and coded to access the defined objects.\n- API frameworks like [Fastify](https://www.fastify.io/docs/latest/Getting-Started/#serialize-your-data) can use JSON Schema definitions to speed up the parsing of JSON payloads.\n\nJSON Schemas are clear, unambiguous, machine & human readable definitions of the objects that your application needs. \n\n![]({{< param \"image\" >}})\n> Photo by [Tim Arterbury on Unsplash](https://unsplash.com/photos/VkwRmha1_tI)\n\n## What does a JSON Schema look like?\n\nAs we're looking at Cloudant databases which store JSON objects, let's focus on a schema describing a JavaScript _Object_, in this case an object representing a person:\n\n```js\n{\n  \"_id\": \"abc123\",\n  \"type\": \"user\",\n  \"name\": \"Bob Smith\",\n  \"email\": \"bob.smith@aol.com\",\n  \"password\": \"1f6b5d0e151388786d3820cded9408e2\",\n  \"salt\": \"43614d9b1dec23da34a5b6f4eb71fb59\",\n  \"active\": true,\n  \"email_verified\": true,\n  \"address\": \"19 Front Street, Darlington, DL5 1TY\",\n  \"joined\": \"2020-07-23T11:50:17.809Z\"\n}\n```\n\nA JSON Schema representation of this object could be:\n\n```js\n{\n  \"$schema\": \"https://json-schema.org/draft/2019-09/schema\",\n  \"$id\": \"http://glynnbird.com/person\",\n  \"type\": \"object\",\n  \"properties\": {\n    \"_id\": { \"type\": \"string\" },\n    \"_rev\": { \"type\": \"string\" },\n    \"type\": { \"type\": \"string\", \"enum\": [\"user\"] },\n    \"name\": { \"type\": \"string\" },\n    \"email\": { \"type\": \"string\", \"format\": \"email\" },\n    \"password\": { \"type\": \"string\" },\n    \"salt\": { \"type\": \"string\" },\n    \"active\": { \"type\": \"boolean\" },\n    \"email_verified\": { \"type\": \"boolean\" },\n    \"address\":  { \"type\": \"string\" },\n    \"joined\": { \"type\": \"string\", \"format\": \"date-time\"}\n  },\n  \"additionalProperties\": false,\n  \"required\": [\"type\", \"name\", \"email\", \"password\", \"salt\", \"active\", \"joined\"]\n}\n```\n\nNote that each property's _data type_ is specified with an optional `format` or `enum` for further validation. JSON Schema has a number of built in types (email/URL/date/time etc) and can also handle regular expression validation of other patterns.\n\nAs `additionalProperties` is set to `false`, no extra properties other than those defined in the schema are allowed. The `required` array lists the properties that must be present - all others are optional.\n\nTry schema validation yourself using [this online tool](https://www.jsonschemavalidator.net/). Paste the schema in the left pane and the JSON in the right. Note how validation fails if there is a type/format mismatch, a missing mandatory property or the presence of any additional property.\n\n## Implementing JSON Schema\n\nThere are [numerous implementations](https://json-schema.org/implementations.html) of JSON Schema validators in a range of programming languages. I was drawn to the [cfworker/json-schema](https://github.com/cfworker/cfworker/blob/master/packages/json-schema/README.md) JavaScript implementation which is designed to run on Cloudflare serverless workers with no dependencies. \n\nIt would make sense to add JSON Schema validation in your application layer to prevent invalid JSON documents making it to Cloudant:\n\n```js\nimport { Validator } from '@cfworker/json-schema'\nconst validator = new Validator(myPersonSchema)\nconst result = validator.validate(myObject)\n// { valid: true }\nif (result.valid) {\n  await db.insert(myObject)\n}\n```\n\nFrameworks such as [Fastify](https://www.fastify.io/) have JSON Schema support baked in and use them to get the best performance, as well as for schema validation.\n\nNext we'll look at how schema validation could work _inside_ the Cloudant/CouchDB database.\n\n## Validate Document Update functions\n\nCloudant and its open-source cousin Apache CouchDB have the ability to run user-defined JavaScript [Validate Document Update (VDU)](https://docs.couchdb.org/en/stable/ddocs/ddocs.html?highlight=validate_doc_update#validate-document-update-functions) functions which decide whether an incoming document makes it to the database or not.\n\nCreate a Cloudant database with a Design Document with the following content:\n\n```js\n{\n  \"_id\": \"_design/vdu\",\n  \"validate_doc_update\": \"function (newdoc) { throw({ forbidden: 'schema validation failed' })  }\"\n}\n```\n \nThis VDU function is executed before every regular document insert/update/delete operation and if it throws an error, then the document change is **not** stored. As this particular VDU function _always_ throws an error, we are unable to write any further documents to the database:\n\n![vdu1](/img/vdu1.png)\n\nWe can write our own custom logic into that VDU function to, say, reject documents that contain a property \"b\":\n\n```js\n{\n  \"_id\": \"_design/vdu\",\n  \"validate_doc_update\": \"function (newdoc) { if (typeof newdoc.b !== 'undefined') throw({ forbidden: 'schema validation failed })  }\"\n}\n```\n\nNow any document is valid unless it contains `b` property. \n\nWe can keep extending this VDU logic to ensure that only documents that match our schema are allowed, but _that's what JSON Schema is for_. If only there was a way to run a JSON Schema validator in a VDU function...\n\n## Adding JSON Schema validation into a VDU function\n\nCouchDB allows JavaScript functions to be \"required\" in from elsewhere in the Design Document, so if we store a JSON Schema validator in there, we are able to access it from our VDU function.\n\n> Note: writing JavaScript in Design Documents is difficult, prone to error and almost impossible to debug. Things get gnarly from here.\n\nFirst we need to add the [cfworker/json-schema](https://github.com/cfworker/cfworker/blob/master/packages/json-schema/README.md) validator into our design document together with the schema(s) to validate against and our VDU function.\n\nThe Design Document has the following shape:\n\n```js\n{\n  \"_id\": \"_design/validate\",\n  \"views\": {\n    \"lib\": {\n      \"validator\": \"<JSON Schema validator code goes here>\",\n      \"person\": \"<JSON Schema for a 'person' object goes here>\"\n    }\n  },\n  \"validate_doc_update\": \"<VDU code goes here>\",\n}\n```\n\nThe finished code is difficult to read as the JavaScript is represented as [JSON strings in the Design Document](https://gist.github.com/glynnbird/87e5e8ec01a04b4982c25c2bbda8d3ab):\n\n<script src=\"https://gist.github.com/glynnbird/87e5e8ec01a04b4982c25c2bbda8d3ab.js\"></script>\n\nLet's look at the VDU function in more detail:\n\n```js\nfunction (newdoc) { \n  var Validator = require('views/lib/validator').Validator; \n  var schema = require('views/lib/person'); \n  var validator = new Validator(schema); \n  var r = validator.validate(newdoc); \n  if (!r.valid) { \n    throw({'forbidden':'schema does not match'})\n  }  \n}\n```\n\n- it uses `require` to pull in the validator function from the design document.\n- it uses `require` to pull in the JSON schema we wish to test. If your database is storing documents of different types, it could pull in the correct schema dynamically here.\n- if the incoming `newdoc` fails to pass schema validation an error is thrown and the document is not allowed in.\n\nWith this Design Document in place, the database only accepts objects that match the definition of our _person_ object by testing the incoming object against the schema. \n\n",
    "url": "/2020/07/24/JSON-Schema-Validation.html",
    "tags": "Schema Validation",
    "id": "84"
  },
  {
    "title": "Compliance automation via Auditree",
    "description": "Auditree - open source compliance automation for the cloud",
    "content": "\n\n\nOver the past few years Cloudant has been engaging in more and more [compliance related work][cloudant-compliance]. This is a good thing; it gives customers confidence in the systems holding their data, and it gives us a way to critically evaluate & improve our product & processes.\n\nA compliance accreditation is made up of controls. In some cases [lots of controls][nist], all of which need to be consistently & continually executed, with evidence of successful (or otherwise) execution collected for audit. This is complicated by the size & dynamism of a Cloud estate, especially for a service the size of Cloudant.\n\nAuditors need to see a lot of evidence data, and can \"sample\" any device at any point within the audit period. As we got more involved in compliance activities we quickly realised that we needed some automation to scale collection and verification of evidence if we were going to be successful in our compliance activities.\n\nWe have built an automation framework to address the problems compliance at scale presents. It ensures a continuity of evidence, while also helping operationally manage compliance posture by verifying that evidence as it is collected.\n\nThe good news: the [framework][] & tools we've built for our compliance automation is [now open source][auditree]!\n\n## Introducing Auditree\n\nFrom the [Auditree homepage][auditree]:\n\n> Auditree is an opinionated set of tools to enable the digital transformation of compliance activities. It is designed to be familiar to DevOps teams and software engineers, using tools they are likely already interacting with daily.\n\nWhat does that mean? Well, we've built a framework & set of associated tools that helps us turn compliance problems into engineering problems. Instead of clipboards and spreadsheets we have unit tests and data in git. Cloudant has been using this since 2017, and other teams in IBM have been adopting it since 2018.\n\nIt's no secret that Cloudant [uses Chef][chef] & a range of CI/CD tools to manage our estate. We also write a lot of things in Python. We've used ideas from CI/CD & configuration management in Auditree.\n\nEvidence is collected from APIs by \"[fetchers][]\" and verified by \"[checks][]\". These are just decorated Python unit tests, so generally easy for developers to build while also being flexible and capable. Checks produce reports, and their status (pass/fail/warn) can be sent to notifiers; Slack, issue trackers etc.\n\nThose decorators manage writing/reading evidence in a \"[locker][]\". The locker is a git repository with some additional tracking metadata provided by the framework. We chose git because it is tamper evident, gives us history of data in an efficient manner and, most importantly, familiar to engineers. If data hasn't changed since the last execution, git won't update the file, hence we track metadata such as last retrieval time alongside the evidence. Evidence is given a TTL (time to live) in the locker and only retrieved if that has expired to avoid unnecessary and possibly expensive calls to the providers of evidence.\n\nThe framework is designed to fail & recover cleanly, with each execution being a fresh install of both the framework & fetchers/checks. We run the set of fetchers/checks every 8 hours. This is key for two reasons. Firstly, it allows us to build up a consistent & comprehensive body of evidence. Secondly, it means that our global team has opportunity to identify & remediate issues or deviations before the next run, while not being \"noisy\".\n\n## Ecosystem\n\nWhile the execution of the framework is key, providing both operational oversight & a body of evidence, we've found there are other parts of the compliance puzzle that need specific tools to address.\n\n### Harvest\n\n[Harvest][harvest] collates evidence (well, any file in git) over time, which allows you to build & run reports & analysis over the contents of the locker, beyond what checks are doing. This is useful when answering a data request over a period of time, or if you want to have periodic roll ups of data for reporting to the organisation.\n\n### Plant\n\nWhile automated data collection is key to scaling compliance activities, there are some pieces of evidence, for example a penetration test report, that may not be retrievable in this manner. [Plant][plant] provides a way to manually correctly place data in the locker, so that it can be verified by checks.\n\n### Prune\n\nOne of the first checks we built was [`test_abandoned_evidence`][tae]. This is important because it will catch valid data that hasn't been updated for a long time - a possible sign of a misconfiguration somewhere. However, while this is important for detecting problems is also creates an issue: if I no longer use a service I won't collect evidence from it, and this check will start to fire. I also can't just remove the evidence files - they may still be relevant for future audits. We need a way to correctly manage the retirement of evidence, and that is [Prune][prune].\n\n## Over to you\n\nAuditree is a set of building blocks to help engineers address compliance issues at scale. We're open sourcing it because we think it is useful, but also to see how others are addressing the same problems. We are building a public library of fetchers & checks (and a fairly long backlog of things to bring into the open source domain) in [Arboretum][arboretum], and would welcome contributions in any size or shape there, or in any of the repositories, from the community.\n\n[auditree]: https://auditree.github.io/\n[nist]: https://nvd.nist.gov/800-53/Rev4\n[framework]: https://github.com/ComplianceAsCode/auditree-framework\n[chef]: {{< ref \"/2019-11-01-Improve-and-then-improve-some-more.md\" >}}\n[cloudant-compliance]: https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-compliance\n[fetchers]: https://github.com/ComplianceAsCode/auditree-framework/blob/main/compliance/fetch.py\n[checks]: https://github.com/ComplianceAsCode/auditree-framework/blob/main/compliance/check.py\n[locker]: https://github.com/ComplianceAsCode/auditree-framework/blob/main/compliance/locker.py\n[harvest]: https://github.com/ComplianceAsCode/auditree-harvest\n[plant]: https://github.com/ComplianceAsCode/auditree-plant\n[prune]: https://github.com/ComplianceAsCode/auditree-prune\n[arboretum]: https://github.com/ComplianceAsCode/auditree-arboretum/\n[tae]: https://github.com/ComplianceAsCode/auditree-arboretum/blob/main/arboretum/technology/auditree/checks/test_abandoned_evidence.py\n",
    "url": "/2020/07/30/compliance-automation-auditree.html",
    "tags": "Automation Compliance Open Source",
    "id": "85"
  },
  {
    "title": "Automated Daily Backups",
    "description": "Using serverless functions to backup to Object Storage",
    "content": "\n\n\nCloudant is already reslient in that it stores data in triplicate across a region's three availability zones but that's not the same thing as having a _backup_:\n\n- what if you delete or modify a bunch of documents and wish to restore them later? (Note that you can access the previous revision of a document for a short time, but it will be eventually _compacted_ so this behaviour isn't to be relied upon).\n- what if you accidentally delete a whole database in error and need to restore it? (Note that [Cloudant on Transaction Engine has an undelete function](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-databases#undelete) that allows deleted databases to be restored within a time window)\n- it's good practice to have another copy of your data elsewhere in the event of a disaster.\n\n![backup]({{< param \"image\" >}})\n> Photo by [Ashim D’Silva on Unsplash](https://unsplash.com/photos/B5j_W25e1JU)\n\nIn this article we'll create a serverless backup utility which can be triggered to run periodically to backup a Cloudant database to IBM Cloud Object Storage:\n\n![backup](/img/cloudant_daily_backup.png)\n\nTo follow this tutorial you'll need:\n\n- an IBM Cloud account.\n- an IBM Cloudant service containing a database to backup.\n- an IBM Cloud Object Storage service with a new empty bucket inside.\n- the IBM Cloud [command line interface](https://cloud.ibm.com/docs/cli) with the [IBM Cloud Functions CLI plugin](https://cloud.ibm.com/functions/learn/cli).\n\n## Couchbackup\n\nThe [couchbackup](https://www.npmjs.com/package/@cloudant/couchbackup) utility is a command-line tool that allows a Cloudant database to be turned a text file.\n\n```sh\n# turn our \"animals\" database into a text file\ncouchbackup --db animals > animals.txt\n```\n\nIt comes with a matching utility which does the reverse: restores a text file back to a Cloudant database.\n\n```sh\n# restore our animals backup to a new database\ncat animals.txt | couchrestore --db animals2\n```\n\nAll we need to do is trigger `couchbackup` periodically to perform a backup. To do this we'll use IBM Cloud Functions, which allows code to run _serverlessly_, meaning you pay only for the execution time and nothing while your application stands idle.\n\n## Creating a suitable Docker image\n\nTo create a Docker image suitable for running inside the IBM Cloud Functions platform we need a file called `Dockerfile` containing the definition of our image:\n\n```\n# based on the IBM Cloud Functions Node.js runtime\nFROM ibmfunctions/action-nodejs-v10\n\n# with additional npm modules for Cloudant & COS\nRUN npm install ibm-cos-sdk@1.6.1\nRUN npm install @cloudant/couchbackup\n```\n\nThe image is based on the existing IBM Cloud Functions Node.js runtime but adds two additional Node.js modules:\n\n- [ibm-cos-sdk](https://www.npmjs.com/package/ibm-cos-sdk) - to allow us to write data to IBM Cloud Object Storage.\n- [@cloudant/couchbackup](https://www.npmjs.com/package/@cloudant/couchbackup) - to perform the Cloudant backup operation\n\nAssuming you have a [DockerHub](https://hub.docker.com/) account (let's assume a username of `x`), you can build the image with the following terminal commands:\n\n```sh\ndocker build -t x/cloudant_backup .\ndocker push x/cloudant_backup:latest\n```\n\nIf you don't want to build the Docker image yourself, there is one already built using this definition at [https://hub.docker.com/repository/docker/choirless/backup](https://hub.docker.com/repository/docker/choirless/backup) which we'll use in the rest of the tutorial.\n\n## Making a backup script\n\nThe Docker image provides the operating system, the IBM Cloud Functions scaffolding and the libraries we need - all that is required is to add our own custom code to allow us to backup a Cloudant database to Object Storage programmatically. Here's an example:\n\n```js\nconst couchbackup = require('@cloudant/couchbackup')\nconst AWS = require('ibm-cos-sdk')\nconst stream = require('stream')\n\nconst main = async function (args) {\n  // combine bare URL with source database\n  if (!args.CLOUDANT_URL || !args.CLOUDANT_DB) {\n    return Promise.reject(new Error('missing Cloudant config'))\n  }\n  const fullURL = args.CLOUDANT_URL + '/' + args.CLOUDANT_DB\n\n  // COS config\n  if (!args.COS_ENDPOINT || !args.COS_API_KEY || !args.COS_SERVICE_INSTANCE_ID || !args.COS_BUCKET) {\n    return Promise.reject(new Error('missing COS config'))\n  }\n  const COSConfig = {\n    endpoint: args.COS_ENDPOINT,\n    apiKeyId: args.COS_API_KEY,\n    ibmAuthEndpoint: 'https://iam.ng.bluemix.net/oidc/token',\n    serviceInstanceId: args.COS_SERVICE_INSTANCE_ID\n  }\n  const cos = new AWS.S3(COSConfig)\n  const streamToUpload = new stream.PassThrough({ highWaterMark: 67108864 })\n  const key = `${args.CLOUDANT_DB}_${new Date().toISOString()}_backup.txt`\n  const bucket = args.COS_BUCKET\n  const uploadParams = {\n    Bucket: args.COS_BUCKET,\n    Key: key,\n    Body: streamToUpload\n  }\n  console.log(`Backing up DB ${fullURL} to ${bucket}/${key}`)\n\n  // return a Promise as this may take some time\n  return new Promise((resolve, reject) => {\n    // create a COS upload operation hanging on a stream of data\n    cos.upload(uploadParams, function (err, data) {\n      if (err) {\n        return reject(new Error('could not write to COS'))\n      }\n      console.log('COS upload done')\n      resolve(data)\n    })\n\n    // then kick off a backup writing to that stream\n    couchbackup.backup(fullURL, streamToUpload, { iamApiKey: args.CLOUDANT_IAM_KEY },\n      function (err, data) {\n        if (err) {\n          return reject(err)\n        }\n        console.log('couchbackup done')\n      }\n    )\n  })\n}\n\nmodule.exports = {\n  main\n}\n```\n\nMost of the above code involves the the handling of the incoming parameters which are listed here:\n\n - `CLOUDANT_IAM_KEY` e.g. 'abc123'\n - `CLOUDANT_URL` e.g 'https://myservice.cloudantnosqldb.appdomain.cloud'\n - `CLOUDANT_DB`: e.g. 'mydata'\n - `COS_API_KEY` e.g. 'xyz456'\n - `COS_ENDPOINT` e.g 's3.private.eu-gb.cloud-object-storage.appdomain.cloud',\n - `COS_SERVICE_INSTANCE_ID` e.g. 'crn:v:w:x:y:z::'\n - `COS_BUCKET` e.g. 'mybucket'\n\nThe values you'll need can be found in your Cloudant and Cloud Object Storage service credentials, apart from `COS_ENDPOINT` which can be found [here](https://cloud.ibm.com/docs/cloud-object-storage?topic=cloud-object-storage-endpoints). Note that if the IBM Cloud Functions region and IBM Cloud Object Storage region are the same, then a \"private\" endpoint can be chosen to keep the traffic within the data centre.\n\nIf our code is written to an `index.js` file, we can deploy to IBM Cloud Functions with:\n\n```sh\n# create a new Cloud Functions package\nibmcloud fn package create cron\n# create a function in the cron package based on the Docker image\nibmcloud fn action update cron/backup --docker choirless/backup:latest index.js\n```\n\nWe now have an IBM Cloud Function called `cron/backup` which is based on our Docker image and adds our `index.js` script.\n\n## Executing a backup\n\nTo avoid having to pass all of the configuration options in with each execution, we're going to bind _most_ of the configuration (all items except the database name) to the function. Create a JSON file `opts.json` containing your configuration:\n\n```js\n{\"CLOUDANT_IAM_KEY\":\"abc123\",\"CLOUDANT_URL\":\"https://myservice.cloudantnosqldb.appdomain.cloud\",\"COS_API_KEY\":\"xyz456\",\"COS_ENDPOINT\":\"s3.private.eu-gb.cloud-object-storage.appdomain.cloud\",\"COS_SERVICE_INSTANCE_ID\":\"crn:v:w:x:y:z::\",\"COS_BUCKET\":\"mybucket\"}\n```\n\nThen bind this configuration to your action:\n\n```sh\nibmcloud fn action update cron/backup --param-file opts.json\n```\n\nThis means that when we execute the action we only need provide the one missing parameter: the name of the database to backup:\n\n```sh\nibmcloud fn action invoke cron/backup --result --param CLOUDANT_DB mydb\n```\n\n## Scheduling a backup to run periodically\n\nWe can tell IBM Cloud Functions to run our action once every 24 hours for each database we need to backup:\n\n```sh\n# backup each database after midnight\n\n# mydb1 database\nibmcloud fn trigger create mydb1BackupTrigger --feed /whisk.system/alarms/alarm --param cron \"5 0 * * *\" --param trigger_payload \"{\\\"CLOUDANT_DB\\\":\\\"mydb1\\\"}\" \nibmcloud fn rule create mydb1BackupRule mydb1BackupTrigger cron/backup\n\n# mydb2 database\nibmcloud fn trigger create mydb2BackupTrigger --feed /whisk.system/alarms/alarm --param cron \"10 0 * * *\" --param trigger_payload \"{\\\"CLOUDANT_DB\\\":\\\"mydb2\\\"}\" \nibmcloud fn rule create mydb2BackupRule mydb2BackupTrigger cron/backup\n```\n\nScheduling an invocation of an IBM Cloud Function is a two-part process:\n\n- create a _trigger_ which fires at a known time using [cron format](https://crontab.guru/) to indicate which times to fire. The trigger includes the \"payload\" which will be supplied to our function during execution, containing the database name to backup.\n- add a _rule_ which associates the invocation of your function with the firing of the _trigger_.\n\nIn the above example `mydb1` is backed up at five past midnight every day and `mydb2` five minutes later.\n\n## Limitations\n\nThis approach is reasonable for small Cloudant databases but IBM Cloud Functions has a ten minute execution limit so very large databases would not complete in time.\n\nFor very large databases, consider consuming a database's [changes feed](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-databases#get-changes), passing in a last known `since` value to get incremental changes.   ",
    "url": "/2020/10/09/Automated-Daily-Backups.html",
    "tags": "Backup Serverless",
    "id": "86"
  },
  {
    "title": "HAProxy as a reverse proxy",
    "description": "Using two Cloudant instances in active-passive mode.",
    "content": "\n\n\nThe IBM Cloudant helps you build a flexible disaster recovery capability into your applications through a cross-region disaster recovery setup using either active-passive or active-active configuration. For either configuration, IBM Cloudant doesn't provide a facility to manage explicitly any failover or reroute requests between Cloudant regions and has to be implemented within the application logic itself, or by using a load balancer/proxy. To enable auto-failover, we can put an HTTP proxy server in front of IBM Cloudant and then configure our application to talk to the proxy server rather than the IBM Cloudant instance directly. This would be easy, efficient, and save time compared to doing it through application logic.\n\n> **Prerequisite** : Set up an active-passive configuration that uses two IBM Cloudant accounts, one in each region. Active-passive configuration is preferred as in the case of active-active configuration we have to additionally create a strategy for managing conflicts to avoid problems with multiple copies of data. For more information about setting this up, see [Configuring IBM Cloudant for cross-region disaster recovery](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-configuring-ibm-cloudant-for-cross-region-disaster-recovery)\n\n![crossroads]({{< param \"image\" >}})\n> Photo by [Ira Huz on Unsplash](https://unsplash.com/photos/sMFWkx07fXI)\n\nIn this article, we will use HAProxy or High Availability Proxy to direct traffic to one of two Cloudant instances(active-passive configuration) with auto failover to the backup Cloudant instance.\n\n### Target architecture with HAProxy as a reverse proxy\n\n![image](/img/haproxy1.png)\n\n### Key Considerations for the setup\n\n1) HA Proxy should route traffic to the primary Cloudant instance as long as it is up.\n2) HA Proxy should automatically route traffic to secondary Cloudant instance if the primary instance goes down.\n3) HA Proxy should automatically route traffic back to the primary Cloudant instance as soon as it is up again.\n\n### How to check if the Cloudant instance is up?\n\nCloudant offers a simple way to know if it is available or not through the `_up` endpoint. So all we have to do is ping this endpoint as a 'heartbeat at a regular interval of time. For example, a simple GET request(let's say in every 5 seconds) sent to this endpoint to see if the \"200\" response code is returned. As long 200 response code is received, traffic is routed to the primary Cloudant instance. If HAProxy receives a status code other than 200, then it would automatically route traffic to the secondary Cloudant instance.\n\n### Let's get started with the configuring HA Proxy\n\n**Step - 1)** Install HA Proxy\n\nIt is really easy to install HA Proxy and for this article, we will be using Ubuntu OS. Commands to install a specific version of HA proxy are as follows.\n\n```console\nadd-apt-repository ppa:vbernat/haproxy-2.2\napt-get update\napt-get install haproxy / sudo apt-get install -y haproxy\n```\n> **Note:** Choose the latest HA Proxy stable version.\n\nVerify the installation by checking the HAProxy server version\n\n```console\nhaproxy -version\n```\n\n**Step - 2)** Configure frontend, backend and enable statistics for HAProxy\n\nGo to the path **\"/etc/haproxy\"** and edit the **haproxy.cfg** file in any editor e.g vi or vim and add frontend, backend and enable statistics as shown in the below configuration.\n\n> For simplicity, I would demonstrate it using HTTP, but it could also be configured for HTTPS.\n\n```file\nglobal\n(...)\n\ndefaults\n(...)\n\nlisten stats\n\tbind *:9090\n  \tstats enable\n  \tstats uri /stats\n  \tstats auth admin:admin@123\n  \tstats refresh 30s\n\t\nfrontend db_frontend\n\tbind *:80\n\tmode http\n    \tuse_backend db_cluster\n\t\nbackend db_cluster\n\toption httpchk GET /_up HTTP/1.1\n\thttp-check send hdr Authorization Basic\\ xXxXxXxX(base64encoded)xxXxXxxXxx=\n\thttp-check expect status 200\n    \thttp-send-name-header Host\n\tserver primaryacct.cloudant.com 111.0.0.1:443 check inter 5s rise 2 fall 2 ssl verify none\n    \tserver secondaryacct.cloudant.com 111.0.0.2:443 check inter 5s rise 2 fall 2 ssl verify none backup\n```\n\nMake relevant changes to the haproxy.cfg file and save it. Now let's verify and reload the HAProxy configuration by using the below commands.\n\n```console\nhaproxy -c -f /etc/haproxy/haproxy.cfg\nservice haproxy reload\n```\n\n### Let's understand the HAProxy configuration\n\n```file\nlisten stats\n\tbind *:9090\n  \tstats enable\n  \tstats uri /stats\n  \tstats auth admin:admin@123\n  \tstats refresh 30s\n```\nThe above configuration would enable the HAProxy dashboard to show various information & statistics.\n\n- _bind_: Sets which address and port you'll use to access the dashboard.\n- _enable_: Enables the statistics.\n- _stats uri_: Set the path to access the dashboard. e.g. server_ip_address:9090/stats.\n- _stats auth_: Set the credentials to access the dashboard\n- _stats refresh_: Configures how often the dashboard will automatically refresh within your browser, which in this case is 30 seconds.\n\n```file\nfrontend db_frontend\n\tbind *:80\n\tmode http\n    \tuse_backend db_cluster\n```\n### HAProxy dashboard\n![image](/img/haproxy2.png)\n\n> **Note:** Here the server with green color is the primary server and the server with blue color is the backup server. All the requests are served by the primary server unless it is down.\n\nA _**frontend**_ is what a client connects to. As requests enter the load balancer and as responses are returned to the client, they pass through the frontend. In the above configuration, HAProxy listens on default port 80 and forward requests to the backend.\n\n```file\nbackend db_cluster\n\toption httpchk GET /_up HTTP/1.1 \n\thttp-check send hdr Authorization Basic\\ wcm9maWxlOm5(base64encoded)AxMjM0NTY=\n\thttp-check expect status 200\n    \thttp-send-name-header Host\n\tserver primaryacct.cloudant.com 111.0.0.1:443 check inter 5s rise 2 fall 2 ssl verify none\n    \tserver secondaryacct.cloudant.com 111.0.0.2:443 check inter 5s rise 2 fall 2 ssl verify none backup\n```\nA _**backend**_ consists of a pool of servers and the requests are routed between the servers in a round-robin fashion making it a load balancer by default. To make HA Proxy behave like a reverse proxy rather than a load balancer, we have to make one server as active and the other server as passive. This could be achieved by using the keyword **\"backup\"** for the server. e.g. in the above configuration, secondaryacct.cloudant.com is the passive server, which will not receive any traffic unless the active server(primaryacct.cloudant.com) is down. And As soon as the active server comes back up, all the requests will fail back to it again. In this example configuration HAProxy polls the server on a fixed interval of 5 seconds which is described by the attributes `check inter 5s` and if not specified, the default value is 2 seconds. The attributes `rise 2 fall 2` signifies the number of consecutive valid health checks before considering the server as UP and DOWN. The attribute `ssl verify none` instructs HAProxy not to check the validity of a server certificate.\n\nIn this example, we are configuring two different Cloudant services(one active, one passive) having different hostname. In order for HAProxy to communicate to Cloudant, we need to provide a HOST header having the value of Cloudant database hostname, without which Cloudant won't honour the request. This could be achieved through the attribute **http-send-name-header**. By using the \"Host\" header with this option(e.g. http-send-name-header Host), the existing Host header would be overwritten with the new value from the server name which in this case are primaryacct.cloudant.com & secondaryacct.cloudant.com. The first three lines inside _**backend**_ are responsible for making a GET call to the `_up` end point and expecting a response code **\"200\"** by passing credential through **Authorization** header.",
    "url": "/2020/10/27/HAProxy-as-a-reverse-proxy.html",
    "tags": "Proxy HA",
    "id": "87"
  },
  {
    "title": "Migrating a Cloudant Account",
    "description": "Using replication to copy data from one account to another",
    "content": "\n\n\nThere are several reasons why you'd want to copy data from one Cloudant account to another:\n\n1. To copy data to a different region as you want to setup a cross-region database service (e.g. Dallas and Frankfurt).\n2. To move from self-hosted CouchDB service to a hosted Cloudant account.\n3. To migrate from a Cloudant dedicated service to a cloud-based, multi-tenant Cloudant service.\n4. To move from a Cloudant Standard account to Cloudant on Transaction Engine account.\n\nIn this post we'll look at how to undertake such a move and some considerations you may want to take into account while you do so. We'll assume that we are migrating from one Cloudant account to new empty Cloudant account in a different region. \n\n![migration]({{< param \"image\" >}})\n> Photo by [Lesly Derksen on Unsplash](https://unsplash.com/photos/kPvFE0ZOTz8)\n\n## Replication primer\n\n\n### What is replication?\n\nCloudant replication is a one-way data copy from a _source_ database to a _target_ database. \n\n![mig1](/img/migration1.png)\n\nThe source and target can be on different Cloudant accounts which can exist in different regions across the world. The _mediator_ of the replication job can be the source account, the target account or even a third Cloudant account. Best practice is to have the quietest Cloudant account be the mediator of the replication job - in this case the _target_ account, as it is a new empty service should be configured to \"pull\" the data from the source database.\n\n### One off vs continuous\n\nReplication jobs can be a one-off operation (the default) or run [continuously](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-replication-api#continuous-replication), spooling each change from source to target until the job is cancelled.\n\n### Replication speed\n\nReplication jobs can run at different speeds depending on the values [performance-related options](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-advanced-replication#performance-related-options) supplied during replication setup. You may wish to throttle the speed at which replication proceeds so as not to put too much load on a production service - it's important that your source account isn't swamped by reads leaving no capacity for other workloads or similarly, that the target account isn't swamped by writes. [This document](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-ibm-cloud-public#consumption-of-read-and-write-operations-by-replication) provides approximate documents/sec rates for different combinations of the performance parameters in practice.\n\n### Many databases\n\nA replication job only copies one database's data from source to target. If you need to migrate multiple databases, you'll need a replication job per database. For many databases, it's usually not advisable to have lots of replications running at once. The [couchreplicate](https://www.npmjs.com/package/couchreplicate) tool allows many databases to be copied, with a configurable maximum number of replications running at any one time.\n\n![mig1](/img/migration4.png)\n\n### Indexing\n\nReplication only copies documents, not secondary index data. As index definitions arrive at the target (in [Design Documents](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-design-documents)), the process of indexing will begin and the indexes will build asynchronously. Make sure indexes have finished building before directing production traffic to the target service.\n\nFurther reading:\n\n- [Replication Guide](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-replication-guide#what-is-replication)\n- [Advanced Replication](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-advanced-replication)\n\n## Filtering documents\n\nReplicating to a new Cloudant service is a good opportunity to remove unwanted documents at the same time. Filtered replication involves supplying a Cloudant Query selector when creating a replication job - documents that meet the criteria of the selector will be replicated, everything else will be left behind.\n\n> Note that unlike Cloudant Query, a replication filter selector does not require a matching secondary index. Each changed document is passed through the selector to see if it qualifies to be replicated.\n\nFiltered replication is useful for:\n\n- Only replicating newer data. Supply a filter on a date field and only allow a data range through. Old data from the source can be archived to Cloud Object Storage.\n- Removing deletions. Deleted Cloudant documents leave behind a \"tombstone\" document which can't be deleted. A replication filter can omit these tombstones leaving the target as a pristine data set containing only non-deleted documents. A database with fewer tombstones is cheaper to host and faster to index & replicate. \n\nFurther reading:\n\n- [Replication with a selector](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-replication-api&origin_team=T4NN71GAU#the-selector-field)\n- [Filtered Replication]({{< ref \"/2019-12-13-Filtered-Replication.md\" >}})\n\n## Database sharding\n\nA Cloudant Standard database is broken into `q` shards, with three copies of each shard being stored around the cluster. Usually the default value of `q` is fine, but if you have a very large database or many very small databases, then a custom value is justified. In these circumstances creating the target database with the correct value of `q` is important before replicating data across.\n\n- For most users, the default value of `q` is fine.\n- Databases with only a few thousand documents can be created with a `q` value of `1` for a boost in query performance.\n- Databases with more than 10GB of data per shard copy should seek the advice of the Cloudant Support team to see if a larger value of `q` would help their use-case. There are complex trade-offs to consider when operating with a larger value of `q` - a good rule of thumb is to avoid an ever-growing data set if possible. \n\n> Note Cloudant on Transaction Engine automatically handles database sharding, so there's no need to worry about `q`.\n\n## Live switchover\n\nCareful use of replication can allow an application to move from one geography to another without downtime. [Continuous replication](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-replication-api&origin_team=T4NN71GAU#continuous-replication) is set up to copy data from the source to the target forever. \n\nThis allows application traffic to be directed to the source, the target or to both. When you're happy that the target is performing correctly, the replication can be deleted and the source service deleted.\n\nIf you want to hedge your bets and allow for the possibility of switching back to the source then two replications can be setup to transfer data from source-->target and target-->source. If the switchover to the new target service doesn't work out, the source database will still be there and also be updated with any changes that were written to the target side.\n\n![mig1](/img/migration2.png)\n\n## Advanced filtering\n\nIf your application relies on Cloudant deletions, and you also want to purge unwanted historical deletions during replication, how do you purge only the historical replications and keep any deletions that occur during the migration process as your application continues to delete documents? In short, this is the procedure to follow:\n\n1. Hit the source database's top-level endpoint e.g. `GET /<dbname>` and make a note of the `update_seq` field.\n2. Setup a one-off [filtered replication]({{< ref \"/2019-12-13-Filtered-Replication.md\" >}}) that omits all deleted documents. Wait until replication completes.\n3. Setup a continuous replication (without a filter) but with a value of `since_seq` equal to the sequence token fetched in Step 1. This will perform a top-up replication, taking any changes since we began the first replication including any deletions that occurred.\n4. When you're ready to move traffic to the new service, delete the replication from Step 3. The target database will only contain only non-deleted documents and any documents deleted since the migration process began.\n\n",
    "url": "/2020/11/25/Migrating-a-Cloudant-Account.html",
    "tags": "Replication Migration",
    "id": "88"
  },
  {
    "title": "Repairing a database with conflicts",
    "description": "Three ways to eliminate conflicts from a Cloudant database",
    "content": "\n\n\n[Cloudant conflicts](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-conflicts) occur when disconnected replicas of a database are updated in different ways at the same time. The replicas could be:\n\n- Copies of a database hosted in different regions that are replicating continuously to each other, while both accepting writes.\n- A mobile application going offline and replicating its updates to the cloud later, only to find that some of its changes clash with the cloud-based copy.\n- Copies of the same database shard in a single-region cluster. If the same document is updated over and over in a short time window, conflicts can be unwittingly introduced.\n\n> Note that Cloudant on Transaction Engine does not generate conflicts for in-region write, eliminating the third of the above bullet points.\n\nCloudant conflicts can be [resolved](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-conflicts#how-to-resolve-conflicts) by deleting unwanted conflicted revisions and optionally writing a new revision (the choice of conflict resolution algorithm will vary between applications: merging the bodies of conflcting documents, being one example) but there are consequences for having a database that has conflicted documents because Cloudant retains information for each branch in the revision tree - the wider the revision tree, the more work is required to navigate it for update, retrieval and indexing operations.\n\nIn short, highly conflicted documents are a performance drag **even if your application has dilligently resolved conflicts as they arise**.\n\n![pic]({{< param \"image\" >}})\n> Photo by [jeshoots.com on Unsplash](https://unsplash.com/photos/VdOO4_HFTWM)\n\nThis post describes three ways in which a database that contains problematic documents can be \"repaired\", so as to not impact performance. They all involve creating a new copy of the data in a new, freshly-created database.\n\n## Repairing via replication - winning_revs_only\n\nFrom October 2022 onwards, we can replicate data from the conflicted source database to a new, empty database and only copy the winning revisions, leaving the conflicts behind. The trick is to use the `winning_revs_only: true` flag in the replication document:\n\n```js\n{\n  \"source\": \"https://a.cloudant.com/source\",\n  \"target\": \"https://a.cloudant.com/target\",\n  \"winning_revs_only\": true\n}\n```\n\nThis is the recommended technique for ridding a database from unwanted conflicts. Other methods are outlined in the rest of this post but this method is the best.\n\n> Note: The `winning_revs_only` flag should only be used for the purposes of repairing conflicted databases and should be not used for other replication jobs.\n\n## Repairing via replication - with a selector\n\n> Note: this technique has been superceded by the `winning_revs_only` approach.\n\nWe can replicate data from the source database to a new empty database, with a replication filter that omits conflicted documents. A `selector` object is added to the replication document to set up the filter:\n\n```js\n\"selector\": {\n  \"_conflicts\": {\n    \"$exists\": false\n  }\n}\n```\n\nThe above selector reads as \"allow documents that _don't_ have a `_conflicts` attribute to be replicated\". \n\nThe replication document in the `_replicator` database would look something like this:\n\n```js\n{\n  \"source\": \"https://a.cloudant.com/source\",\n  \"target\": \"https://a.cloudant.com/target\",\n  \"selector\": {\n    \"_conflicts\": {\n      \"$exists\": false\n    }\n  }\n}\n```\n\nReplication makes light work of copying over documents _without_ conflicts, but if you also need the winning revisions of the source database's _conflicted documents_, a separate script would have to be written. It would fetch the winning revision of each conflicted document and write them to the target database using [the bulk_docs API](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-documents#bulk-operations) with `new_edits=false` to retain the original document's revision token.\n\nTo help identify conflicted documents a [view can be created](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-conflicts#finding-conflicts) in the source database.\n\n## Repairing conflicts via replication & VDU\n\n> Note: this technique has been superceded by the `winning_revs_only` approach.\n\nThis technique can be used to repair conflicted documents by replicating everything to a new database but ensuring that the _conflicted documents_ only keep the branch of changes that contains the winning revision - all of the conflicts will not make it to the target database. Here's how it works:\n\n1. [Delete all of the conflicting revisions](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-conflicts#how-to-resolve-conflicts) in the source database.\n2. Create a new target database\n3. Create a new design document in the target database containing:\n\n```js\n{ \"_id\": \"_design/validator\", \"validate_doc_update\": \"function(newDoc, oldDoc, userCtx) { // any update to an existing doc is OK\\n if(oldDoc) {\\n return;\\n }\\n \\n // reject tombstones for docs we don';t know about\\n if(newDoc[\\\"_deleted\\\"]) {\\n throw({forbidden : \\\"We';re rejecting tombstones for unknown docs\\\"});\\n }\\n}\\n\" }\n```\n\n> This is a \"VDU\" function - a special JavaScript function which is the gatekeeper for which writes are accepted by the database. It contains logic which will reject branches of a revision tree which end in a deleted document - the upshot of which leaves conflicted documents with only the winning branch retained.\n\n4. Replicate data from the source database to the new target database. Use `continuous=true` if you would like the replication to run until it is no longer needed.\n5. Once complete and indexes are built, any applications using the source database can be pointed towards the target.\n\n## Repairing by backup\n\n> Note: this technique has been superceded by the `winning_revs_only` approach.\n\nAnother method is to backup the affected database to a file and to restore it to a new, freshly created database. The [couchbackup tool has a \"shallow\" mode](https://github.com/cloudant/couchbackup#what-is-shallow-mode) which only copies over the winning revisions of each document it finds. Restoring only the winning revisions to the target has the effect of eliminating the conflict history.\n\nThe procedure is as follows:\n\n```sh\n# backup the source database using shallow mode\ncouchbackup --db source --mode shallow > backup.txt\n\n# restore to a new empty database\ncurl -X PUT \"$COUCH_URL/target\"\ncat backup.txt | couchrestore --db target\n```\n\n> Note: couchbackup does not backup attachments, so this method may not be suitable for such databases.",
    "url": "/2020/11/26/Repairing-a-Database-With-Conflicts.html",
    "tags": "Conflicts Replication",
    "id": "89"
  },
  {
    "title": "GitHub Webhooks and Cloud Functions",
    "description": "Syncing GitHub JSON files to Cloudant using Webhooks",
    "content": "\n\n\n\nSome GitHub repositories are not just source code for apps, they can also store data files holding JSON, YAML, XML or any other file format e.g.\n\n- A collection of reference data such as this [countries of the world GeoJSON](https://github.com/glynnbird/countriesgeojson).\n- \"Infrastructure as code\" files, such as [Terraform configuration](https://github.com/futurice/terraform-examples/tree/master/aws/aws_lambda_api).\n\nIn this blog post we'll create an IBM Cloud Function that is triggered by a commit to a GitHub repository, which stores a copy of JSON data from GitHub in a Cloudant database. \n\n![pic2](/img/githubsync2.png)\n\nThe Cloudant-based mirror of the data can then be indexed, searched and used for operational use-cases while staying in near real-time sync with the GitHub reference. \n\n![pic]({{< param \"image\" >}})\n> Photo by [Brady Rogers on Unsplash](https://unsplash.com/photos/ZGRB8TMT6zQ)\n\nPrequisites:\n\n- An IBM Cloud account\n- A GitHub account\n\n## Step 1 - IBM Cloud Function\n\nFirst we are going to create an IBM Cloud function which will be called for every GitHub commit.\n\n- Visit the [Functions dashboard in the IBM Cloud](https://cloud.ibm.com/functions).\n- Click on [Actions](https://cloud.ibm.com/functions/actions)\n- Then [\"Create\" ",
    "url": "/2021/02/08/GitHub-Webhooks-and-Cloud-Functions.html",
    "tags": "GitHub Serverless",
    "id": "90"
  },
  {
    "title": "Removing Tombstones",
    "description": "Expunging deleted documents from Cloudant or CouchDB databases",
    "content": "\n\n\nCloudant and its sister database Apache CouchDB store document data in revision trees. When a document is deleted, a special deletion (`\"deleted\": true`) revision is added to the head of the tree. This allows the intention that the document is to be deleted to be replicated around, whether that be to other nodes in the cluster or to other Cloudant or CouchDB services in other geographies. Without this mechanism, it would be possible for a deleted document to be unintentionally resurrected via replication from an external replica.\n\nThe downside of this approach is that deleted documents occupy space in the database which adds additional cost for storage and  backup, and slows down replication and index building. \n\nIf the number of deletions becomes onerous, it is best practice to periodically rid the database of the burden of these so-called \"tombstone\" documents: stub documents that mark the place of a formerly existing document. This blog post shows how to do just that.\n\n![tombstones]({{< param \"image\" >}})\n> Photo by [Anton Darius on Unsplash](https://unsplash.com/photos/lQMtXKvBmuw)\n\n## Filtered replication to delete tombstones\n\nAs [this blog post]({{< ref \"/2019-12-13-Filtered-Replication.md\" >}}) explains, we can create a new empty database and set up a replication from the old database to the new but with a filter that leaves the tombstones behind.\n\nReplications are started by creating a document in the `_replicator` database like so:\n\n```js\n{\n  \"_id\": \"myfirstreplication\",\n  \"source\" : \"http://<username1>:<password1>@<account1>.cloudant.com/<sourcedb>\",\n  \"target\" : \"http://<username2:<password2>@<account2>.cloudant.com/<targetdb>\",\n  \"selector\": {\n    \"_deleted\": {\n      \"$exists\": false\n    }\n  }\n}\n```\n\nwhere the `source` is the original database and the `target` is the new empty database. The `selector` is the filter that checks each document before replicating - in this case we only want documents _without_ a `deleted` attribute (an document that hasn't been deleted).\n\nThis solution works and will create you a pristine database that contains no deletions. But databases rarely stay static: what happens to documents that are deleted while the replication is in progress? Those deletions wouldn't make it to the target database, so documents that were intended to be deleted would be present in the target! This is avoidable by following the more complex procedure in the next section.\n\n## Two-phase filtered replication\n\nTo delete the unwanted deletions but to keep deletions that are occuring _during the replication_, follow these steps:\n\n![diagram](/img/tombstones.png)\n\n1. Call the `GET /<sourcedb>` endpoint. This returns meta data about the source database and will tell us the current value of `update_seq`. This very long, opaque string is a representation of a point in time in the database's changes feed. Make a note of this value - we'll need it later.\n2. Set up a one-off replication beteen the source database and the target database using a `selector` to filter out deletions, just as we did in the previous section. Let this run to completion. The target now contains no deletions.\n3. Set up a second replication between the source and the target database _without a filter_, but with an additional parameter `\"since_seq\": \"X\"` where X is the value we saved from step 1. This \"tops up\" the target database with changes that have happened _since_ the point in time when we started the first replication, including any deletions that occurred. When this replication is complete, the target database can be used to handle live traffic instead of the source database.\n\nThe third step's `_replicator` document would look something like:\n\n```js\n{\n  \"_id\": \"mytopupreplication\",\n  \"source\" : \"http://<username1>:<password1>@<account1>.cloudant.com/<sourcedb>\",\n  \"target\" : \"http://<username2:<password2>@<account2>.cloudant.com/<targetdb>\",\n  \"since_seq\": \"23599-g1AAAARXeJzLYWBgEMhgTmHQSklKzi9KdUhJMjTU\"\n}\n```\n\nThis third replication could additionally have `\"continuous\": true` added so that changes are continuously replicated from source to target. This is useful to keep the original and new databases in lock-step while the application code is updated to point to the new target database. The continuous replication can be stopped and the source database deleted after switchover.\n\n\n",
    "url": "/2021/05/21/Removing-Tombstones.html",
    "tags": "Deletion Replication",
    "id": "91"
  },
  {
    "title": "Cloudant SDK Transition",
    "description": "Introducing Cloudant's new SDKS and changes to supported SDKs",
    "content": "\n\n\nThe IBM Cloudant team is introducing new Cloudant API SDKs and announcing end-of-life (EOL) for some existing libraries.\nThe new SDKs offer more consistency in the Cloudant SDK portfolio and unification with the common IBM Cloud SDK\nexperience. We leveraged this to produce improved [API documentation](https://cloud.ibm.com/apidocs/cloudant) spanning\nthe Cloudant HTTP and SDK APIs. This makes it easier to find not only the desired API, but also code examples for your\nchosen ecosystem.\n\n![Green chrysalis, translucent chrysalis, and imago of butterflies]({{< param \"image\" >}})\n> Photo by [Suzanne D. Williams on Unsplash](https://unsplash.com/photos/VMKBFR6r_jg)\n\n## Summary of SDK changes\n\n#### Cloudant API SDKs\n\n| Ecosystem | Existing supported library | Existing library EOL | Replaced by new supported SDK |\n| ",
    "url": "/2021/06/30/Cloudant-SDK-Transition.html",
    "tags": "SDKs mobile sync java-cloudant python-cloudant nodejs-cloudant swift-cloudant cloudant-go-sdk cloudant-java-sdk cloudant-node-sdk cloudant-python-sdk",
    "id": "92"
  },
  {
    "title": "Code Engine & Cloudant",
    "description": "Running serverless services with IBM Cloud Code Engine",
    "content": "\n\n\n One of the great advantages of cloud infrastructure is that it offers the option of trying things out at zero or near-zero cost.\n\nYou can spin up services, create applications and try them out. If they don’t work or don’t get traction, you simply deprovision the infrastructure and try something else. The barrier to innovation is thus significantly lowered.\n\nIn this post, we’ll take you through a basic scenario that combines two such IBM services to build a [serverless](https://www.ibm.com/cloud/learn/serverless) polling application:\n\n- [IBM Cloud Code Engine](https://www.ibm.com/cloud/code-engine) is a fully managed, serverless platform that runs your containerized workloads, including web apps, microservices, event-driven functions or batch jobs. Code Engine even builds container images for you from your source code, and that is what we will be doing in this demo. The Code Engine experience is designed so that you can focus on writing code and not on the infrastructure that is needed to host the code. \n\n- [IBM Cloudant](https://www.ibm.com/cloud/cloudant) is a fully managed, distributed database optimized for heavy workloads and fast-growing web and mobile apps. Again, the fully-managed nature of Cloudant allows you to focus on developing your applications instead of having to worry about server infrastructure and what to do if you hit the jackpot and your app goes viral.\n\nBoth of these services have generous free tiers, which means you can use them at no cost to build prototypes or a proof-of-concept. \n\nSo, let's jump right in. It should take you no more than an hour to read this post and complete the tutorial.\n\n## The Project: Fruit Counter\n\nFruit counter is a web app that lets its visitors pick their favourite fruit from a list of options. Once they submit their choice, they can see the current running total of fruit favourites.\n\n![fruit]({{< param \"image\" >}})\n> Photo by [Amy Shamblen on Unsplash](https://unsplash.com/photos/h5yMpgOI5nI)\n\n\nThe fruit preference data will be stored in Cloudant and the app will be deployed to the Internet using Code Engine.\n\n## Prerequisites\n\nYou will need the following:\n\n- An IBM Cloud pay-as-you-go account. \n- If you want to deploy and test your application locally (which we recommend you do to get familiar with what is going on), you will need:\n- - Access to a Mac or Linux terminal\n- - Git\n- - Node.js and npm\n\n## Step 1: Provision a Cloudant service (CloudantFruitCounter)\n\nYou will need to create a Cloudant service instance and some credentials to access it from your application. To do that, [follow the steps described in this document](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-getting-started-with-cloudant).\n\nIn the Instance Name box, call your instance CloudantFruitCounter.\n\nMake a note of the apikey and url values of your service credentials. We will need those later.\n\n## Step 2: Create the Cloudant database (fruitcounter)\n\nNow you need to create a Cloudant database to store your fruit data. ][Follow steps 1 and 2 in this document](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-navigate-the-dashboard). \n\nCall your database `fruitcounter`.\n\nYou are now ready to store your fruit data.\n\n## Step 3 (Optional):  Deploy and test locally\n\n### The short version\n\nOpen a terminal window. Create some environment variables to access the Cloudant database using the values you obtained when creating the service credentials:\n\n```sh\nexport CLOUDANT_URL=<your_url>\nexport CLOUDANT_APIKEY=<your_key>\nexport DBNAME = \"fruitcounter\"\n```\n\n(If you use the above url and key variable names, the Cloudant SDK will find them in your environment without the need for any additional code.)\n\nClone the repo, install all the dependencies and then start the server:\n\n```sh\ngit clone https://github.com/danmermel/fruit-counter\ncd fruit-counter\nnpm install\nnpm run start\n```\n\nNow, open a browser and visit http://localhost:8080.\n\nYou should be able to pick your favourite fruit, submit your choice and see the running total of picks.\n\n### A bit more info\n\nThe application is a very simple Node.js application. It uses two main packages:\n\n1. [@ibm-cloud/cloudant](https://github.com/IBM/cloudant-node-sdk) to connect to IBM Cloudant and read/write data.\n2. [Express](https://expressjs.com/) to enable a simple web server that allows users to submit their choice of fruit and see a running total of our data. \n\nThere are two main files:\n\n**server.js**: This runs the web server and communicates with Cloudant. When the frontend submits a fruit choice to the /fruit route (see below), the app.route function stores a new document in Cloudant with the fruit choice and then reads back the running total to return to the frontend. \n\nThe read operation uses a Cloudant design document and a MapReduce view to aggregate documents. This is beyond the scope of this tutorial, but you can [read more about views and design documents here](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-views-mapreduce).\n\n**index.html**: This is the one and only page of the application and is using the [Vue.js](https://vuejs.org/) framework. When it loads, it will show you the available fruit options.\n\nWhen you submit your choice it will make an HTTP POST request with your choice to the /fruit route of the application (see above). A successful return from the application will contain the running total of fruit choices (including yours), which will be displayed.\n\nIf you navigate to the Cloudant console and find your database you will see some documents like this one:\n\n![codeengine1](/img/codeengine1.jpg)\n\nAs you can see, your choice of fruit was added with a timestamp. Cloudant created a unique ID for this document.\n\n## Step 4: Deploying to IBM Code Engine\n\nSo far, so good; but as this is running locally, only you can pick your favourite fruit. Now we want the whole internet to choose their fruit as well — this is where Code Engine comes in.\n\nIn a former life, if you wanted to deploy this app to the Internet, you would have had to do the following:\n\n- Buy a server and wait two weeks for it to be delivered.\n- Plug it in and find a way of connecting it to the Internet.\n- Log into it and deploy your code and all its dependencies.\n- Run the code.\n- Spend hours of your day making sure it is still up and running and deciding whether or not you need to buy more servers and repeat all the steps above as your app becomes popular.\n- If your app became popular, you'd have to buy and provision more servers and set up a load balancer — or, preferably, multiple load balancers — to handle your application's traffic.\n- And let's not forget that you would be responsible for securing the server and making sure it stays compliant.\n\nNo longer. Code Engine manages all the deployment and scaling for you if you just tell it where your code is. It does this in two steps:\n\n1. It creates a container image with your code and saves it in a container repository (known as a registry). Containers are executable units of software in which application code is packaged, along with its libraries and dependencies, in common ways so that it can be run anywhere.\n2. It then takes this image and deploys it to IBM's Cloud infrastructure. It will deploy as many or as few of these images as you specify to cope with peaks and troughs in demand. Additionally, with Code Engine, you can have it auto-scale the number of instances for you based on the amount of incoming traffic it receives. And it'll even scale down to zero instances if the application is idle.\n\nTo deploy your code with Code Engine, follow these steps:\n\n1. Open the [Code Engine](https://cloud.ibm.com/codeengine/overview) console. \n2. Select Start creating from Start from source code.\n3. Select Application.\n4. Select a project from the list of available projects. You can also [create a new one](https://cloud.ibm.com/docs/codeengine?topic=codeengine-manage-project#create-a-project). Note that you must have a selected project to deploy an app. You can [read more about projects here](https://cloud.ibm.com/docs/codeengine?topic=codeengine-manage-project).\n5. Enter a name for the application. Use a name for your application that is unique within the project.\n6. Select Source code.\n7. Click Specify build details.\n8. Select https://github.com/danmermel/fruit-counter for Source repository and main for Branch name. Click Next.\n9. Select Dockerfile for Strategy, Dockerfile for Dockerfile, 10m for Timeout and Medium for Build resources. Click Next.\n10. Select a container registry location, such as IBM Registry, Dallas.\n11. Select Automatic for Registry access.\n12. Select an existing namespace or enter a name for a new one — for example, newnamespace.\n13. Enter a name for your image and optionally a tag.\n14. Click Done.\n15. Open the Environment Variables section. You will need to add three of these (click Add for each):\n- Name: CLOUDANT_URL   Value: <your_url> (From the Cloudant service credentials step above)\n- Name: CLOUDANT_APIKEY   Value: <your_apikey> (From the Cloudant service credentials step above)\n- Name: DBNAME  Value: \"fruitcounter\"\n16. Click Create.\n\nYou will see a message like this, telling you that your assets are being created:\n\n![codeengine1](/img/codeengine2.jpg)\n\nWhen it is finished, you will see a Ready message:\n\n![codeengine1](/img/codeengine3.png)\n\nClick on Open Application URL.\n\nYou should see the fruit picker application on the web. Go ahead, pick your favourite fruit and submit. You should see the running total of fruit choices. \n\nIf you refresh the page and submit another choice, you will see the relevant counter incrementing.\n\n## Slow start\n\nYou may notice that the first time you visit the application URL (or after you haven't used it for a while) it takes a while to display. This is because Code Engine is creating and starting the first instance of your application there and then. After a period of inactivity, this instance is deprovisioned and the next visitor has to start again. This is very cost-effective (you only pay while the instances are up and running), but may not be practical for applications that need to respond in milliseconds even on the first request, like a business intelligence dashboard. \n\nYou can modify the minimum (and maximum) number of instances in the Runtime section of the Cloud Engine dashboard:\n\n![codeengine1](/img/codeengine4.jpg)\n\nIf you set it to a minimum of one, there will always be at least one instance of your application ready to receive visitors, but you will eventually run out of free tier vCPU seconds and will start to pay for your deployment.\n\n## The Dockerfile\n\nThere is one file in the project that we've glossed over: the Dockerfile. For those familiar with the way containers work, this file needs no explanation. For those who are not, the Dockerfile is the \"recipe\" that is used to create the container that will host and run your application. The Dockerfile in this project is very simple — it says that the container should be created using a starting configuration (base image) that supports Node.js version 14; that it should run the application from a certain directory (/usr/src/app); that it needs to copy all the application files into that directory and install all the dependencies (npm install); that it should listen for traffic on port 8080; and, finally, that it should start the application by running the following command:\n\n```sh\nnpm run start\n```\n\nCode Engine uses this set of instructions to build the container that runs your application.\n\n## Service bindings\n\nThe way we connected the Code Engine and Cloudant services together is good for allowing a simple way to run the application locally and generally demonstrating how things work. For more real-life secure implementations, however, it is probably better to use \"service bindings\" when combining IBM services together. That is beyond the scope of this tutorial, but you can [read more about service bindings here](https://cloud.ibm.com/docs/codeengine?topic=codeengine-service-binding).\n\n## Summary\n\nIn this tutorial, we have combined two IBM Cloud services to create an application that can capture, store and retrieve data from the IBM Cloudant database service. This application can then be deployed to the internet using IBM Code Engine in a way that is scalable and secure.\n\nBoth Cloudant and Code Engine have generous free tiers, so we encourage you to use these to prototype and try out your ideas. You have, quite literally, nothing to lose!",
    "url": "/2021/08/16/Code-Engine-and-Cloudant.html",
    "tags": "CodeEngine Serverless",
    "id": "93"
  },
  {
    "title": "CouchApps No Longer Work",
    "description": "A new HTTP header makes CouchApps inoperable.",
    "content": "\n\n\nCloudant mainly stores JSON documents in collections called _databases_, but Cloudant also has the ability to store [attachments](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-attachments) in Cloudant documents. An attachment is a binary blob of data with a file name and mime type. It could be a PDF, a JPG or a Word document.\n\nSome users employed attachments to store HTML, CSS, JavaScript files and other assets in the database so that it could be used as a webserver - applications following this pattern were known as [CouchApps](https://github.com/couchapp/couchapp). CouchApps deployed the website and the database _in the same web domain_, allowing client-side JavaScript to make calls back to the server to add, update and delete documents without having to worry about [cross-origin security restrictions](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS).\n\n![locked]({{< param \"image\" >}})\n> Photo by [Jose Fontano on Unsplash](https://unsplash.com/photos/pZld9PiPDno)\n\n## Cloudant no longer permits CouchApp scripts\n\nAs of October 2021, CouchApps using JavaScript will become inoperable on Cloudant. Fetched attachments will be served out with an additional header: `Content-Security-Policy: sandbox`. This header instructs the browser to prevent script execution on such attachments, so any JavaScript (whether in `.js` files or in `<script>` tags) will be barred from execution on the client machine.\n\nOnly Javascript-free CouchApps will continue to operate.\n\nThe reason for this change is to close a [security loophole](https://docs.couchdb.org/en/stable/cve/2021-38295.html) which could lead to privilege escalation and malicious data access.\n\n## Regular attachments will continue to work\n\nRegular document attachments will continue to work as normal, the only difference being the addition of the `Content-Security-Policy` header on attachment retrieval which should not affect normal operation.\n\n## Alternatives to CouchApps\n\nStatic websites have become very popular in recent years and there many better places for hosting static content than in a database:\n\n- [GitHub Pages](https://pages.github.com/) allows files in a git repository to be served out on the web, with custom domain name an HTTPS support for free.\n- [Netlify](https://www.netlify.com/) offers a similar git-based workflow to GitHub Pages but adds the ability to add serverless functions into the mix.\n- or, [any number of website hosting offerrings](https://www.ibm.com/uk-en/cloud/hosting/web-hosting).\n\nAny of the above solutions will mean that the website and database server will reside on _different domain names_ and by default, a web page may only access resources on the domain it was served out from. This will mean that web requests originating from web page's JavaScript, targeting a Cloudant service would not be permitted (by the web browser). \n\nCloudant can be configured to permit cross-domain requests by enabling CORS [in the Cloudant Dashboard](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-cors#dashboard): you may choose to allow requests from any domain, or to a list of specified domains.\n\nThe combination of a static hosting service and Cloudant with CORS enabled, should allow CouchApp-like functionality to be reproduced.\n",
    "url": "/2021/10/20/CouchApps-no-longer-work.html",
    "tags": "CouchApp Static",
    "id": "94"
  },
  {
    "title": "Projection",
    "description": "Storing data in an index for faster retrieval.",
    "content": "\n\n\nCloudant's MapReduce indexes give you complete control over what data from your primary JSON objects are stored in your secondary indexes. JavaScript functions are used to programmatically decide which document attributes are selected for inclusion as either the _key_ or the _value_ of the views - the JavaScript functions can even be used to process the data before it's saved in the index.\n\n![projection]({{< param \"image\" >}})\n> Photo by [Jeremy Yap on Unsplash](https://unsplash.com/photos/J39X2xX_8CQ)\n\nIn this blog post we'll look at a technique called _projection_ which produces the fastest, most efficient data selection indexes.\n\n## MapReduce basics\n\nA [MapReduce](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-creating-views-mapreduce) index is defined in two parts\n\n1. The Map function - a JavaScript function which is called for each document in the database and chooses what is stored in the index.\n2. The Reducer - in this blog post we're not concerned with aggregation of data, so we'll leave the reduce component undefined.\n\nThe Map & Reduce definitions are stored in [design documents](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-design-documents) which define the specifcation of one or more secondary indexes. Here's an example map function:\n\n```js\nfunction(doc) {\n  if (doc.status === 'complete') {\n    emit(doc.timestamp, null)\n  }\n}\n```\n\nNote that:\n\n- The function is called with a single parameter 'doc' which contains the document being indexed.\n- The function uses an `if` statement to decided whether to index something or nothing. This is called a _partial_ or _sparse_ index as it only contains data for some of the documents. This keeps the index as compact and performant as possible.\n- The data emitted has two parts: the `key` and the `value` forming the first and second parameters to the `emit` function. The `key` determines the order of the index and consequently marries with the _access pattern_ your index is serving i.e. by emitting a key of `doc.timestamp` we are creating a secondary index in date/time order allowing us to query data by a single timestamp or a range of time values. The `value` is null - the value is usually used to store data which is aggregated by the _reducer_, but for the moment we can elect to store nothing here.\n\n## Using the view\n\nOnce the above view has built, we can invoke the API endpoint associated with our view suppling parameters to define the slice of data we need:\n\n- `?key=\"2021-04-01T10:44:00.000Z\"` - find documents matching a single timestamp.\n- `?startkey=\"2021-04-01\"&endkey=\"2021-05-01\"` - find documents from April 20201.\n\n> Note: because of the `if` statement in the map function, we will only see documents where the `status` attribute is `completed`. We have baked only that subset of the documents into the secondary index.\n\nThe data returned from these API calls will include:\n\n- the \"key\" from the index\n- the matching document id\n- the \"value\" from the index - in this case `null`\n\nHere is a sample response:\n\n```js\n{\"total_rows\":21953,\"offset\":0,\"rows\":[\n{\"id\":\"K7N2T4IR5JMJGJ8F\",\"key\":\"2020-04-01T00:12:37.887Z\",\"value\":null},\n{\"id\":\"ZYAMEN3Y56M6PLGG\",\"key\":\"2020-04-01T00:25:45.818Z\",\"value\":null},\n{\"id\":\"TUVG1CHYQ5EF5T6D\",\"key\":\"2020-04-01T00:56:50.297Z\",\"value\":null},\n{\"id\":\"GFY67VNMC0J8AQBX\",\"key\":\"2020-04-01T01:32:31.487Z\",\"value\":null},\n{\"id\":\"S7VZS7A31GV1QS96\",\"key\":\"2020-04-01T01:55:19.592Z\",\"value\":null}\n]}\n```\n\nIf we want to see the document body itself for each returned row, we can add `include_docs=true` to our query and the results will also include the full document.\n\nExample response:\n\n```js\n{\"total_rows\":23169,\"offset\":0,\"rows\":[\n{\"id\":\"K7N2T4IR5JMJGJ8F\",\"key\":\"2020-04-01T00:12:37.887Z\",\"value\":null,\"doc\":{\"_id\":\"K7N2T4IR5JMJGJ8F\",\"_rev\":\"1-c2ea27ee82feb91ba80708c5e8f83d29\",\"status\":\"complete\",\"timestamp\":\"2020-04-01T00:12:37.887Z\",\"price\":72.23,\"buyer\":\"Dorathy Xiong\",\"email\":\"dorathy.xiong51243@hotmail.com\",\"address\":\"4574 Aston Street , Glossop, Shropshire, PL7 6VI\",\"delivery\":\"failed\"}},\n{\"id\":\"ZYAMEN3Y56M6PLGG\",\"key\":\"2020-04-01T00:25:45.818Z\",\"value\":null,\"doc\":{\"_id\":\"ZYAMEN3Y56M6PLGG\",\"_rev\":\"1-b215e5e7335d7ad8cac4e33073598c02\",\"status\":\"complete\",\"timestamp\":\"2020-04-01T00:25:45.818Z\",\"price\":75.3,\"buyer\":\"Yajaira Gary\",\"email\":\"yajaira_gary@yahoo.com\",\"address\":\"2784 Meal Lane , New Alresford, Montgomeryshire, GL9 0DR\",\"delivery\":\"failed\"}},\n{\"id\":\"TUVG1CHYQ5EF5T6D\",\"key\":\"2020-04-01T00:56:50.297Z\",\"value\":null,\"doc\":{\"_id\":\"TUVG1CHYQ5EF5T6D\",\"_rev\":\"1-00f9d105a31b3c86657f797f9eeb5b1b\",\"status\":\"complete\",\"timestamp\":\"2020-04-01T00:56:50.297Z\",\"price\":36.51,\"buyer\":\"Kandi Bolduc\",\"email\":\"kandi9@meetup.com\",\"address\":\"4712 Burstead Road , Knottingley, Durham, SO35 6MK\",\"delivery\":\"failed\"}},\n{\"id\":\"GFY67VNMC0J8AQBX\",\"key\":\"2020-04-01T01:32:31.487Z\",\"value\":null,\"doc\":{\"_id\":\"GFY67VNMC0J8AQBX\",\"_rev\":\"1-989788f7d9e2cb881c92c44cdc765eed\",\"status\":\"complete\",\"timestamp\":\"2020-04-01T01:32:31.487Z\",\"price\":28.08,\"buyer\":\"Santina Leal\",\"email\":\"santina_leal3384@gmail.com\",\"address\":\"6443 Ashlands Street , Eccles, Essex, KY8 9JC\",\"delivery\":\"delivered\"}},\n{\"id\":\"S7VZS7A31GV1QS96\",\"key\":\"2020-04-01T01:55:19.592Z\",\"value\":null,\"doc\":{\"_id\":\"S7VZS7A31GV1QS96\",\"_rev\":\"1-ea8cf28f1e5c9af038d4120753c513ee\",\"status\":\"complete\",\"timestamp\":\"2020-04-01T01:55:19.592Z\",\"price\":8.99,\"buyer\":\"Thomas Abreu\",\"email\":\"thomas.abreu@duo.com\",\"address\":\"3250 Turfland Road , Kidsgrove, Radnorshire, EC8 5MQ\",\"delivery\":\"delivered\"}}\n]}\n```\n\nThe trouble with `include_docs=true` is it adds extra complexity to the query: Cloudant has to first find the matching view rows from the secondary index and then in a second operation, fetch the matching document bodies. Extra complexity means extra database load and poorer performance.\n\nHow can we avoid doing `include_docs=true`? Using projection!\n\n## Projection\n\n_Projection_ is the process of storing data in the _value_ portion of the MapReduce index so that it is retrieved at query-time _without_ having to do `include_docs=true`.\n\nAn index with data projected into the index's value makes for a larger index but as the index's value is stored next to the index's keys, retrieval of the data is much faster than fetching the entire document bodies in a separate operation.\n\nProjected indexes are the fastest way of retrieving selections of data from a Cloudant database and should be used by users who want the fastest performance and the lowest server load for operational queries.\n\n### Projecting the entire document\n\nIn this example we're adding the entire document into the index:\n\n```js\nfunction(doc) {\n  if (doc.status === 'complete') {\n    emit(doc.timestamp, doc)\n  }\n}\n```\n\nThis technique is suitable for small documents (< a few KB). To be more optimal, we needn't put the document `_id`/`_rev` pair into the index as they are already part of the response so this code is better:\n\n```js\nfunction(doc) {\n  if (doc.status === 'complete') {\n    delete doc._id\n    emit(doc.timestamp, doc)\n  }\n}\n```\n\nExample response:\n\n```js\n{\"total_rows\":23169,\"offset\":0,\"rows\":[\n{\"id\":\"K7N2T4IR5JMJGJ8F\",\"key\":\"2020-04-01T00:12:37.887Z\",\"value\":{\"_id\":\"K7N2T4IR5JMJGJ8F\",\"_rev\":\"1-c2ea27ee82feb91ba80708c5e8f83d29\",\"status\":\"complete\",\"timestamp\":\"2020-04-01T00:12:37.887Z\",\"price\":72.23,\"buyer\":\"Dorathy Xiong\",\"email\":\"dorathy.xiong51243@hotmail.com\",\"address\":\"4574 Aston Street , Glossop, Shropshire, PL7 6VI\",\"delivery\":\"failed\"}},\n{\"id\":\"ZYAMEN3Y56M6PLGG\",\"key\":\"2020-04-01T00:25:45.818Z\",\"value\":{\"_id\":\"ZYAMEN3Y56M6PLGG\",\"_rev\":\"1-b215e5e7335d7ad8cac4e33073598c02\",\"status\":\"complete\",\"timestamp\":\"2020-04-01T00:25:45.818Z\",\"price\":75.3,\"buyer\":\"Yajaira Gary\",\"email\":\"yajaira_gary@yahoo.com\",\"address\":\"2784 Meal Lane , New Alresford, Montgomeryshire, GL9 0DR\",\"delivery\":\"failed\"}},\n{\"id\":\"TUVG1CHYQ5EF5T6D\",\"key\":\"2020-04-01T00:56:50.297Z\",\"value\":{\"_id\":\"TUVG1CHYQ5EF5T6D\",\"_rev\":\"1-00f9d105a31b3c86657f797f9eeb5b1b\",\"status\":\"complete\",\"timestamp\":\"2020-04-01T00:56:50.297Z\",\"price\":36.51,\"buyer\":\"Kandi Bolduc\",\"email\":\"kandi9@meetup.com\",\"address\":\"4712 Burstead Road , Knottingley, Durham, SO35 6MK\",\"delivery\":\"failed\"}},\n{\"id\":\"GFY67VNMC0J8AQBX\",\"key\":\"2020-04-01T01:32:31.487Z\",\"value\":{\"_id\":\"GFY67VNMC0J8AQBX\",\"_rev\":\"1-989788f7d9e2cb881c92c44cdc765eed\",\"status\":\"complete\",\"timestamp\":\"2020-04-01T01:32:31.487Z\",\"price\":28.08,\"buyer\":\"Santina Leal\",\"email\":\"santina_leal3384@gmail.com\",\"address\":\"6443 Ashlands Street , Eccles, Essex, KY8 9JC\",\"delivery\":\"delivered\"}},\n{\"id\":\"S7VZS7A31GV1QS96\",\"key\":\"2020-04-01T01:55:19.592Z\",\"value\":{\"_id\":\"S7VZS7A31GV1QS96\",\"_rev\":\"1-ea8cf28f1e5c9af038d4120753c513ee\",\"status\":\"complete\",\"timestamp\":\"2020-04-01T01:55:19.592Z\",\"price\":8.99,\"buyer\":\"Thomas Abreu\",\"email\":\"thomas.abreu@duo.com\",\"address\":\"3250 Turfland Road , Kidsgrove, Radnorshire, EC8 5MQ\",\"delivery\":\"delivered\"}}\n]}\n```\n\n### Projecting only the data that is needed\n\nEven better than lazily emitting the entire document into the index's value is to only emit the key/value pairs that your application _needs_. e.g.\n\n```js\nfunction(doc) {\n  if (doc.status === 'complete') {\n    var obj = {\n      p: doc.price,\n      b: doc.buyer,\n      e: doc.email,\n      d: doc.delivery\n    }\n    emit(doc.timestamp, obj)\n  }\n}\n```\n\n- Note we create a new `obj` object which only contains the items we need for our use-case.\n- We don't need to emit `_id` as it is already stored in the index.\n- We don't need `doc.status` because the index will only contain documents with a `completed` status.\n- We don't need `doc.timestamp` because this is already stored in the index's key.\n- We use single character keys in the object to keep the size small.\n\nExample response:\n\n```js\n{\"total_rows\":23169,\"offset\":0,\"rows\":[\n{\"id\":\"K7N2T4IR5JMJGJ8F\",\"key\":\"2020-04-01T00:12:37.887Z\",\"value\":{\"p\":72.23,\"b\":\"Dorathy Xiong\",\"e\":\"dorathy.xiong51243@hotmail.com\",\"d\":\"failed\"}},\n{\"id\":\"ZYAMEN3Y56M6PLGG\",\"key\":\"2020-04-01T00:25:45.818Z\",\"value\":{\"p\":75.3,\"b\":\"Yajaira Gary\",\"e\":\"yajaira_gary@yahoo.com\",\"d\":\"failed\"}},\n{\"id\":\"TUVG1CHYQ5EF5T6D\",\"key\":\"2020-04-01T00:56:50.297Z\",\"value\":{\"p\":36.51,\"b\":\"Kandi Bolduc\",\"e\":\"kandi9@meetup.com\",\"d\":\"failed\"}},\n{\"id\":\"GFY67VNMC0J8AQBX\",\"key\":\"2020-04-01T01:32:31.487Z\",\"value\":{\"p\":28.08,\"b\":\"Santina Leal\",\"e\":\"santina_leal3384@gmail.com\",\"d\":\"delivered\"}},\n{\"id\":\"S7VZS7A31GV1QS96\",\"key\":\"2020-04-01T01:55:19.592Z\",\"value\":{\"p\":8.99,\"b\":\"Thomas Abreu\",\"e\":\"thomas.abreu@duo.com\",\"d\":\"delivered\"}}\n]}\n```\n",
    "url": "/2021/11/12/Projection.html",
    "tags": "Views Performance",
    "id": "95"
  },
  {
    "title": "Migrating from TXE",
    "description": "Switching from TXE to Standard",
    "content": "\n\n\nIn this blog post we'll show how data stored in a Cloudant on Transaction Engine (TXE) instance can be easily migrated to Cloudant Standard. There are few differences betweeen the two offerrings, so we'll explore ways to avoid any pitfalls along the way.\n\nThere isn't a way of converting an existing account from TXE to Standard, so the first step is to provision a new Cloudant Standard account.\n\n![migration]({{< param \"image\" >}})\n> Photo by [Julia Craice on Unsplash](https://unsplash.com/photos/faCwTallTC0)\n\n## Create a new Cloudant Standard account\n\nIn the IBM Cloud Dashboard, locate the [Cloudant service](https://cloud.ibm.com/catalog/services/cloudant) and complete the form to create a new Cloudant Standard service:\n\n- Choose \"Standard\" as the plan.\n- Pick the same region as your Cloudant TXE service.\n\n> Note Cloudant Standard offers two authentication mechanisms: IAM only, or IAM and Legacy Credentials. TXE only has IAM.\n\n## Create new empty databases\n\nFor each of your databases that needs to be copied over, in the new Cloudant Standard dashboard choose \"Create Database\" and enter the name of the database to create a new, empty target database.\n\n> Note Cloudant Standard has two types of databases: partitioned and non-partitioned. As all of your TXE databases will be non-partitioned, choose the \"non-partitioned\" option.\n\nWe will also need a `_replicator` database in the Cloudant Standard instance which will handle the replication jobs. Follow the same steps to create a new, empty `_replicator` database.\n\n## Replicating the data\n\nCloudant's replication capabilities can be used to copy the data from the source (Cloudant TXE) to the target (Standard) and it is recommend to use the Standard account to mediate the replication - i.e. we will be sending our replication document to the Standard account and it will \"pull\" the data from TXE.\n\n![replication plan](/img/migratingtxe1.png)\n\nYou'll need a replication document for each of the TXE databases that are to be copied over. Generate the JSON ahead of time and then add a document for each database to the Cloudant Standard account's `_replicator` database:\n\n```js\n{\n  \"_id\": \"txe_to_standard_orders\",\n  \"source\": {\n    \"url\": \"https://txe.cloudant.com/orders\",\n    \"auth\": {\n      \"iam\": {\n        \"api_key\": \"abc123\"\n      }\n    }\n  },\n  \"target\": {\n    \"url\": \"https://standard.cloudant.com/orders\",\n    \"auth\": {\n      \"iam\": {\n        \"api_key\": \"def456\"\n      }\n    }\n  },\n  \"continuous\": true\n}\n```\n\nNote:\n\n- The `_id` is a recognisable name so that you can tell one replication job from another.\n- The source object contains the URL of the source TXE database and includes an `auth` object which contains an IAM API key that gives at least `Reader` and `Checkpointer` roles against the source.\n- The target object contains the URL of the target Standard database and includes an `auth` object which contains an IAM API key that gives at least `Writer` roles against the target.\n- The `continuous` flag means that the replication will job will run forever, until the `_replicator` document is deleted. This will allow a smooth transition from TXE to Standard - your app traffic can be switched over without any disruption.\n\nReplication jobs use up the read allocation of the source Cloudant service (TXE) and the write allocation of the target Cloudant service (Standard), so it is advisable to [increase the capacity of your source and target services](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-capacity) during the data copying process.\n\nOnce the replication jobs are set up, the [replication jobs can be monitored](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-advanced-replication) using the [_scheduler/docs](https://cloud.ibm.com/apidocs/cloudant#getschedulerdocs) and [_scheduler/jobs](https://cloud.ibm.com/apidocs/cloudant?code=node#getschedulerjobs) endpoints. \n\nAlso bear in mind that once data is copied to the new target databases, any secondary indexes will build. You should monitor the [_active_tasks](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-active-tasks) endpoint to ensure that index building is complete before switching traffic to the new Cloudant service.\n\n> Note if you have a large number of databases, it is advisable to copy databases over in small batches. Copying databases one at a time makes it easier to monitor.\n\n## Differences between TXE and Standard\n\nBefore we switch application traffic over to the new Cloudant Standard URL, we need to do some checks to make sure everything is in order. Here are some pitfalls that should be avoided.\n\n### JavaScript versions\n\nCloudant TXE allows newer (ES6) JavaScript syntax than Cloudant Standard, so it's worth querying each of your secondary indexes to make sure they're returning the data you are expecting. If you are seeing errors, then simplify your JavaScript code: `var` instead of `const`/`let`, simple `for` loops and conditional statements.\n\n> Note that if you modify an index definition, then the secondary index will need to rebuild again.\n\n### Other differences\n\n- Cloudant TXE has its own Pagination API, where as Cloudant Standard has a different set of [parameters for each query method](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-pagination-and-bookmarks).\n- The two products have different [pricing models](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-feature-comparison#pricing-feature-compare) so the billing for the same apps running on each would likely be different.\n- There are [service limit](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-feature-comparison) differences between the two products. Cloudant Standard is more permissive, so this is unlikely to cause problems.\n- Cloudant TXE's changes feed is linearized where as Cloudant Standard's guarantees to give you each change at least once - so duplicates are possible.\n- Cloudant TXE does not create in-region conflicts, but they are possible with Cloudant Standard, especially if a document is modified over and over in short time window. See [Best & Worst Practice]({{< ref \"/2019-11-21-Best-and-Worst-Practices.md\" >}}) for Cloudant Standard.\n\n",
    "url": "/2022/01/14/Migrating-from-Cloudant-TXE-to-Standard.html",
    "tags": "Migration TXE",
    "id": "96"
  },
  {
    "title": "Using the Changes Feed",
    "description": "Best practice and pitfalls",
    "content": "\n\n\nA Cloudant database's changes feed's primary use-case is to power the replication of data from a source to a target database. The Cloudant replicator is built to handle the changes feed and performs the necessary checks to ensure data is copied accurately to its destination. \n\nThere is a raw [changes feed API](https://cloud.ibm.com/apidocs/cloudant#getchanges-changes) that can be used to consume a single database's changes but it must be used with care. \n\nThe `_changes` API endpoint can be used in several ways and can output data in a variety of formats, but this article will focus on best practice and how to avoid some pitfalls when developing against the Changes API.\n\n![changes]({{< param \"image\" >}})\n> Photo by [Chris Lawton on Unsplash](https://unsplash.com/photos/5IHz5WhosQE)\n\n## How do I consume the changes feed?\n\nGiven a single database `orders`, I can ask the database for a list of changes, in this case limiting the result set to five changes with `?limit=5`:\n\n```js\nGET /orders/_changes?limit=5\n{\n  \"results\": [\n    {\n      \"seq\": \"1-g1AAAAB5eJzLYWBg\",\n      \"id\": \"00002Sc12XI8HD0YIBJ92n9ozC0Z7TaO\",\n      \"changes\": [\n        {\n          \"rev\": \"1-3ef45fdbb0a5245634dc31be69db35f7\"\n        }\n      ]\n    },\n    ....\n  ],\n  \"last_seq\": \"5-g1AAAAB5eJzLYWBg\"\n}\n```\n\nThe API call returns:\n\n- `results`: an array of changes.\n- `last_seq`: a token which can be supplied to the changes endpoint in a subsequent API call to get the next batch of changes.\n\n\nFetching the next batch:\n\n```js\nGET /orders/_changes?limit=5&since=5-g1AAAAB5eJzLYWBg\n{\n  \"results\": [ ...],\n  \"last_seq\": \"10-g1AAAACbeJzLY\"\n}\n```\n\nThe `since` parameter is used to define where in the changes feed you wish to start from:\n\n- `since=0` - the beginning of the changes feed.\n- `since=now` - the end of the changes feed.\n- `since=<a last seq token>` - from a known place in the changes feed.\n\nOn face value it would seem like following the changes feed would be as simple as chaining `_changes` API calls together, passing the `last_seq` from one changes feed response into the next request's `since` parameter. But there are some subtleties to the changes feed that need further discussion.\n\n## The changes feed delivers each change at least once\n\nThe Cloudant Standard changes feed promises to return each document _at least once_. This isn't the same as promising to return each document _only once_. Put another way, it is possible for a consumer of the changes feed to see the same change again, or indeed a set of changes repeated.\n\nIt is important that a consumer of the changes feed treats the changes _idempotently_. In practice this means remembering whether a change has already been dealt with before triggering an action from a change. A naive changes feed consumer might send a message to a smartphone on every change received, but a user may receive duplicate text messages if a change is not treated idempotently in the event of replayed changes.\n\nUsually these \"rewinds\" of the changes feed are short, replaying only a handful of changes but in some cases a request may see a response with thousands of changes being replayed - potentially all of the changes from the beginning of time. The potential for rewinds makes using the changes feed unsuitable for an application expecting queue-like behaviour.\n\nTo reiterate, Cloudant's changes feed promises to deliver a document _at least once_ in a changes feed, and gives no guarantees about repeated values across multiple requests.\n\n## The changes feed isn't \"real time\"\n\nThe changes feed doesn't guarantee how quickly an incoming change will appear to a client consuming the changes feed. Applications shouldn't be developed  with the assumption that data inserts, updates and deletes will be immediately be propogated to a changes reader.\n\n## Not all individual document changes may appear in the changes feed\n\nIf a document is updated several times in between changes feed calls, then the changes feed may only reflect the latest of these changes. If the client was hoping to receive every change to every document, they will be disappointed.\n\nThe Cloudant changes feed isn't a _transaction log_ containing every event that happened in time order.\n\n## A filtered changes feed is not for operational queries\n\nFiltering the changes feed, and by extension, performing filtered replication has its uses:\n\n- Copying data from source to target but ignoring deleted documents.\n- Copying data but without index definitions (design documents).\n\nThis [blog post]({{< ref \"/2019-12-13-Filtered-Replication.md\" >}}) describes how supplying a `selector` during replication makes easy work of these use cases.\n\nThe changes feed with an accompanying `selector` parameter is _not_ the way to extract slices of data from the database on a routine basis. It should not be used as a means of performing operational queries against a database. Filtered changes are slow (the filter is applied to every changed document in turn, without the help of an index), much slower than creating a secondary index (such as a MapReduce view) and querying that view. \n\n## The changes feed does not guarantee time-ordering\n\nIf the use case is:\n\n> \"Fetch me every document that has changed since a known date, in the order they were written.\"\n\nthen this cannot be achieved with the Cloudant changes feed. The Cloudant database does not record the time as each document change was written, and the changes feed makes no guarantees on the ordering of the changes in the feed - they are not guaranteed to be in the order they were sent to the database.\n\nThis use case can, however, be acheived by storing the date in the document body:\n\n```js\n{\n  \"_id\": \"2657\",\n  \"type\": \"order\",\n  \"customer\": \"bob@aol.com\",\n  \"order_date\": \"2022-01-05T10:40:00\",\n  \"status\": \"dispatched\",\n  \"last_edit_date\": \"2022-01-14T19:17:20\"\n}\n```\n\nand creating a MapReduce view with `last_edit_date` as the key:\n\n```js\nfunction(doc) {\n  emit(doc.last_edit_date, null)\n}\n```\n\nThis view can be queried to return any documents modified on or after a supplied date/time:\n\n```\n/orders/_design/query/_view/by_last_edit?startkey=\"2022-01-13T00:00:00\"&limit=100\n```\n\nThis technique will produce a time-ordered set of results with no repeated values in a performant and repeatable fashion. The consumer of this data need _not_ treat the data idempotently, making for a simpler development process.\n\n## In summary\n\nThe Cloudant changes feed is good for:\n\n- Powering Cloudant replication, optionally with a selector to filter some changes.\n- Clients consuming the changes feed in batches but dealing with each change idempotently while not being concerned with sort order and expecting to see some changes more than once.\n\nThe Cloudant changes feed is not:\n\n- A message queue. See [IBM Messages for RabbitMQ](https://www.ibm.com/cloud/messages-for-rabbitmq) for managing queues\n- A message broker. See [IBM Event Streams](https://www.ibm.com/cloud/event-streams) for handling scalable, time-ordered streams of events.\n- A real-time pubsub system. See [IBM Databases for Redis](https://www.ibm.com/uk-en/cloud/databases-for-redis) for handling pubsub topics.\n- A transaction log. Some databases store each change in a transaction log but Cloudant's distributed and eventually consistent nature mean that there is no definitive time-ordered transaction log.\n- A querying mechanism. See [MapReduce Views](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-creating-views-mapreduce) for creating views of your data ordered by a key of your choice.\n",
    "url": "/2022/01/21/Using-the-Cloudant-changes-feed.html",
    "tags": "Changes",
    "id": "97"
  },
  {
    "title": "Getting Cloudant Support",
    "description": "Raising a support ticket in the IBM Cloud",
    "content": "\n\n\nSupport is available for your paid IBM Cloudant Standard accounts but not for free, Lite accounts. This blog post shows how to raise a support ticket.\n\n![support]({{< param \"image\" >}})\n> Photo by [Matthew Waring on Unsplash](https://unsplash.com/photos/MJAoiige14E)\n\n> Note that you may still raise a support ticket against free IBM Cloud accounts pertaining to the subjects of account management, billing and IAM, just not support cases about specific Cloudant services.\n\n## Creating an IBM Cloud account\n\nIf you haven't done so already, create an IBM Cloud account at https://cloud.ibm.com/registration:\n\n![IBM Cloud account signup](/img/support1.png)\n\nEnter your details and hit \"Continue\".\n\nYou must enter a credit card against this account to be able to raise support tickets - you won't be charged anything unless you provision IBM Cloud services.\n\n## Creating Cloudant services within an IBM Cloud Account\n\nOnce an IBM Cloud account is set up, you may set up IBM Cloudant service instances within this account.\n\nCreate an IBM Cloudant service by locating Cloudant in the [IBM Cloud Catalog](https://cloud.ibm.com/catalog) or visiting https://cloud.ibm.com/catalog/services/cloudant directly.\n\n![IBM Cloud Cloudant service create](/img/support2.png)\n\n# Creating a support ticket\n\nTo create an IBM Cloud support ticket, log in to your IBM Cloud account and visit https://cloud.ibm.com/unifiedsupport/cases/form and click the \"Create A Case\" button.\n\n![IBM Cloud Cloudant service create](/img/support3.jpg)\n\nPlease visit the \"Your Resources\" section:\n\n![IBM Cloud Cloudant service create](/img/support4.jpg)\n\n- Choose \"Cloudant\" as the topic\n- Choose \"Cloudant NoSQL DB\" as the sub-topic\n- Pick the relevant Cloudant service from the list (if known)\n\n![IBM Cloud Cloudant service create](/img/support5.jpg)\n\n## Enterprise Cloudant Accounts\n\nIf you have an \"Enterprise\" Cloudant Account, support tickets must still be raised using the above procedure. If you don't already have an IBM Cloud account then follow the steps above to create an account and when raising support tickets enter the phrase \"CLOUDANT enterprise support required\" in the support ticket's subject and the hostname of your Cloudant service in the ticket description:\n\n![IBM Cloud Cloudant service create](/img/support6.jpg)\n\n> Note that support requests via the old email address are no longer accepted. All support requests must come from a ticket raised by an IBM Cloud account. \n",
    "url": "/2022/03/11/Getting-Cloudant-Support.html",
    "tags": "Changes",
    "id": "98"
  },
  {
    "title": "Indexing with the Cloudant Dashboard",
    "description": "Creating and using indexes using the Cloudant Dashboard",
    "content": "\n\n\nThe Cloudant dashboard gives new and experienced Cloudant users the opportunity to add, edit and delete documents while refining the indexing and querying options that best suit their application's use-cases.\n\n![dashboard]({{< param \"image\" >}})\n> Photo by [Zach Wiley on Unsplash](https://unsplash.com/photos/FFAq3r9u0-Y)\n\nIn this blog post we'll set up some simple indexes using the dashboard and see how each of Cloudant's querying mechanisms work.\n\n## The data set\n\nLet's first create some sample data representing books in a library:\n\n```js\n{\n  \"_id\": \"BXP9G5ZQY9Q4EA13\",\n  \"author\": \"Dickens\",\n  \"title\": \"David Copperfield\",\n  \"year\": 1840,\n  \"pages\": 723,\n  \"publisher\": \"Penguin\",\n  \"url\": \"http://www.somurl.com/dc\"\n}\n```\n\n> Create a databases called `books` and add some documents matching this pattern using the Cloudant dashboard.\n\nThe documents store simple key/value pairs holding meta data about the book, its author and its publisher. In this example we're going to deal with three use-cases:\n\n- A query facility alllowing a user to find a book by a known publisher and year.\n- A general-purpose search engine allowing a user to find books by a combination of one or more of the author, title, year and publisher.\n- A report detailing the number of books published by year.\n\n## Querying books by publisher and year - Cloudant Query\n\nCloudant Query is a query language that allows small slices of a total database to be located:\n\n```js\n{\n  \"selector\": {\n    \"$and\": [\n      { \"publisher\": \"Penguin\" },\n      { \"year\": { \"$gt\": 2000 } }\n    ]\n  },\n  \"limit\": 10\n}\n```\n\nThe above query contains a `selector` object which defines the slice of data you need:\n\n- `$and` means _both_ of the query clauses must be satisfied for a document to make it to the result set.\n- `{ \"publisher\": \"Penguin\" }`- the publisher must be \"Penguin\".\n- `{ \"year\": { \"$gt\": 2000 } }` - the year must be greater than 2000. `$gt` means \"greater than\". \n\nWe can try the query by choosing \"Query\" when viewing our `books` database in the Cloudant Dashboard and pasting in the query JSON and hitting \"Run Query\":\n\n![running the query](/img/indexingdashboard1.png)\n\nCloudant matches the documents that meet your criteria and it _seems_ to do it quickly, but there's a catch: Cloudant isn't using an index to service this query, meaning that the database is having to scan every document in the database to get your answer. This is fine for very small data sets, but if you're running a production application where the data set is expanding all the time, you definitely _don't_ want to rely on unindexed queries. \n\nTo create an index we can tell Cloudant to create an index on the `publisher` and `year` fields we are using in our query. Choose Design Documents --> Query Indexes from the `books` database's page in the Cloudant dashboard\n\n![indexing the query](/img/indexingdashboard2.png)\n\nand enter the following index definition:\n\n```js\n{\n   \"index\": {\n      \"fields\": [\n         \"publisher\", \"year\"\n      ]\n   },\n   \"name\": \"publisher-year-index\",\n   \"type\": \"json\"\n}\n```\n\nThe `fields` array contains a list of fields we wish to Cloudant to index.\n\nIf we repeat our query, it will be faster and will remain quick even as the database size reaches millions of documents. \n\n> Indexing instructs Cloudant to create a secondary data structure that allows it to find the slice of data you need much faster than pouring over every document in turn. Cloudant Query is best for fixed queries based on the same fields in the same order.\n\nFurther reading: \n\n- [Optimising Cloudant Queries]({{< ref \"/2020-05-20-Optimising-Cloudant-Queries.md\" >}})\n- [Cloudant Query documentation](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-query)\n\nThis index is useful for queries involving both the `publisher` and the `year` but if we introduce another field or make the query more complex (e.g. using the `$or` operator) then the index won't get used and we'll be back to a full database scan. \n\nFor a general-purpose search facility we need Cloudant Search.\n\n## Creating a search engine - Cloudant Search\n\nCloudant Search is based on Apache Lucene and has its own query language that allows rich queries to be constructed. Here's an example:\n\n```\npublisher:Penguin AND (year:1972 OR year:1973) AND title:Crash\n```\n\nUnlike Cloudant Query, you _must_ specify the fields to index _before_ performing a query. Cloudant Search indexes are defined by supplying Cloudant with a JavaScript function which is called once for every document in the database - if the function calls `index` then data is added to the index.\n\nChoose Design Documents -> New Search Index from the `books` database's page in the Cloudant Dashboard:\n\n![search indexing](/img/indexingdashboard3.png)\n\n- Enter a design document name.\n- Enter an index name.\n- Paste the following code into the search index function.\n- Choose the \"Standard Analyzer\".\n\n```js\nfunction (doc) {\n  index(\"author\", doc.author);\n  index(\"publisher\", doc.publisher);\n  index(\"title\", doc.title);\n  index(\"year\", doc.year);\n}\n```\n\n![search index](/img/indexingdashboard4.png)\n\nYou may then build complex queries involving one, some or all of the indexed fields combined with AND and OR operators.\n\n> Cloudant Search is best if you have many search use-cases involving different combinations of fields. \n\nFurther Reading:\n\n- Cloudant Search uses \"analyzers\" to pre-process text prior to indexing. Learn about [Search Analyzers]({{< ref \"/2018-10-19-Search-Analyzers.md\" >}}) to ensure you get the results you expect.\n- [Cloudant Search documentation](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-cloudant-search)\n\n## Aggregating data - MapReduce\n\nNeither Cloudant Query nor Cloudant Search can _aggregate_ search results, i.e. you can't ask \"how many books were published in 1973?\". Cloudant's MapReduce feature allows secondary indexes to be created that can be used for selection or aggregation. MapReduce indexes are, like Cloudant Search, created by supplying a JavaScript function - any call to an `emit` function adds a row to the index. Visit Design Documents --> New View to get started:\n\n![MapReduce view](/img/indexingdashboard5.png)\n\n- Choose a design document name.\n- Choose a view name.\n- Choose the `_count` reducer, as we want our results _counted_.\n\n\nPaste the following index defintion:\n\n```js\nfunction (doc) {\n  emit(doc.year, null);\n}\n```\n\n![MapReduce view](/img/indexingdashboard6.png)\n\nThe subsequent MapReduce view will allow documents to be found by year (as that is the key of the index), but if we switch on the _reducer_ from the \"Options\" pull-down menu the index will aggregate the results, grouping by key (year):\n\n![MapReduce view](/img/indexingdashboard7.png)\n\n![MapReduce view](/img/indexingdashboard8.png)\n\n> MapReduce views are perfect for generating ordered views of your data, containing key/value pairs you define. They can be used for selecting individual keys, range queries or aggregation grouping by the key.\n\nFurther reading:\n\n- [MapReduce blog]({{< ref \"/2011-01-13-mapreduce-from-the-basics-to-the-actually-useful.md\" >}})\n- [MapReduce documentation](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-creating-views-mapreduce)\n",
    "url": "/2022/03/28/Indexing-with-the-Cloudant-Dashboard.html",
    "tags": "Indexing Querying",
    "id": "99"
  },
  {
    "title": "Simple Geospatial Queries",
    "description": "Using Cloudant Search for simple geo searches",
    "content": "\n\n\nIn this blog post I'll show how to perform basic Geospatial queries using standard Cloudant secondary indexes:\n\n- Find documents within a \"rectangle\".\n- Find the nearest.\n\n![globe]({{< param \"image\" >}})\n> Photo by [Adolfo Félix on Unsplash](https://unsplash.com/photos/4JL_VAgxwcU)\n\n## The data\n\nTo demonstrate, I'll use a dataset containing [GeoJSON](https://en.m.wikipedia.org/wiki/GeoJSON) - an industry-standard JSON representation of geographical content. My database contains a number of \"features\" represented by a decimal latitude,longitude point representing the [WGS84](https://en.wikipedia.org/wiki/World_Geodetic_System#1984_version) coordinate of the feature. Other attributes of the feature are stored in the `properties` object.\n\n```js\n{\n  \"type\": \"Feature\",\n  \"geometry\": {\n    \"type\": \"Point\",\n    \"coordinates\": [7.7046, -143.3901]\n  },\n  \"properties\": {\n    \"name\": \"Shenika Bond\"\n  }\n}\n```\n\n## Indexing\n\nIn order to efficiently query our data, we'll need a secondary index. I can define a [Cloudant Search](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-cloudant-search) index by providing a JavaScript function that decides which portion of a document is to be indexed:\n\n```js\nfunction(doc) {\n  // index the latitude\n  index(\"latitude\", doc.geometry.coordinates[0], { store: true})\n\n  // index the longitude\n  index(\"longitude\", doc.geometry.coordinates[1], { store: true})\n\n  // index the name property\n  index(\"name\", doc.properties.name, { store: true})\n}\n```\n\nIn the Cloudant Dashboard, adding a Cloudant Search index looks like this:\n\n![adding Cloudant Search index](/img/simplegeo.png)\n\nNote:\n\n- An index definition resides in a _Design Document_, in this case `_design/find`.\n- Each index has a name e.g `geospatial`\n- Cloudant Search has a nominated \"Analyzer\" which pre-processes string fields. See [this blog post]({{< ref \"/2018-10-19-Search-Analyzers.md\" >}}) for more details.\n\n## Find documents within a rectangle\n\nWe can query the view using Lucene's query language. A typical query might be:\n\n```\nlatitude:[49.7 61.1] AND longitude: [-8 0.5]\n```\n\nThis says \"find my documents whose latitude is between 49.7 and 61.1 degrees, and whose longitude is between -8 and 0.5 degrees\". This corresponds roughly to the boundary of the UK. It's not _really_ a rectangle, but represents the South West and North East boundaries of a shape on the globe.\n\nThe API call we are making looks something like this:\n\n```\nGET poi/_design/find/_search/geospatial?q=latitude%3A[50 55] AND longitude%3A [-3 1]\n```\n\nwhere\n\n- `poi` is the database name\n- `_design/find` is the design document containing the Cloudant Search index\n- `geospatial` is the name of the Cloudant Search index\n\nIn response we get the matching documents:\n\n```js\n{\n\t\"total_rows\": 19,\n\t\"bookmark\": \"g1AAAAN3eJzLYW\",\n\t\"rows\": [{\n\t\t\"id\": \"8fb924d8fb1636c19893ba94e7fb68fb\",\n\t\t\"order\": [1.4142135381698608, 431],\n\t\t\"fields\": {\n\t\t\t\"latitude\": 53.5436,\n\t\t\t\"longitude\": -0.6182,\n\t\t\t\"name\": \"Carli Reiter-Schofield\"\n\t\t}\n\t}, {\n\t\t\"id\": \"05fdf4171319b64ed997e0be3faf391e\",\n\t\t\"order\": [1.4142135381698608, 613],\n\t\t\"fields\": {\n\t\t\t\"latitude\": 54.8739,\n\t\t\t\"longitude\": 0.5319,\n\t\t\t\"name\": \"Quinn Shuler\"\n\t\t}\n\t}\n  ...\n  ```\n\n  The `fields` object contains the content we indexed and opted to store in the index (`{store: true}`).\n\n## Find nearest\n\nFinding the nearest items is a small iteration of the _Find documents within a rectangle_ use-case. We still retain our \"rectangle\" so that the database doesn't have to sort every document in the database into nearest-first order.\n\nWe simply add a `sort` parameter, specifying the point from which distances are to be measured:\n\n```\n  sort=\"<distance,longitude,latitude,-3.25,55.4,km>\"\n```\n\nwhere:\n\n- `longitude` is the indexed field storing the longitude value.\n- `latitude` is the indexed field storing the latitude value.\n- `-3.25,55.4` is the longitude/latitude pair (not lat/long!) from where the distance will be calculated.\n- `km` is the units of distance to use. `mi` is also available.\n\ne.g.\n\n```\nGET poi/_design/find/_search/geospatial?q=latitude%3A[50 55] AND longitude%3A [-3 1]&sort=\"<distance,longitude,latitude,-3.25,55.4,km>\"\n```\n\nThe returned data is much the same:\n\n```js\n{\n\t\"total_rows\": 677,\n\t\"bookmark\": \"g1AAAAP0e\",\n\t\"rows\": [{\n\t\t\"id\": \"56174eed0f78f2173813f7530469499e\",\n\t\t\"order\": [27.42736711207944, 2406],\n\t\t\"fields\": {\n\t\t\t\"latitude\": 55.7525,\n\t\t\t\"longitude\": -3.5712,\n\t\t\t\"name\": \"Mitsuko Glaser\"\n\t\t}\n\t}, {\n\t\t\"id\": \"96ea862cd73f20d8b4998206f7279453\",\n\t\t\"order\": [38.5319843263235, 1732],\n\t\t\"fields\": {\n\t\t\t\"latitude\": 55.9093,\n\t\t\t\"longitude\": -2.85,\n\t\t\t\"name\": \"Tricia Creel\"\n\t\t}\n\t},\n  ...\n  ```\n\nBut the first element of the order array indicates the distance from the provided point in `km`. \n\n## Conclusion\n\nCloudant Search indexes allow fields to be indexed and queried using the standard \"Lucene\" query language. By indexing an object's latitude and longitude we can perform simple geospatial queries, such as find within a rectangle or find nearest. We can also combine the geospatial query with a normal lucene query e.g. find people called \"Tricia\" sorted by distance.",
    "url": "/2022/06/28/Simple-Geospatial-Queries.html",
    "tags": "Geospatial Search",
    "id": "100"
  },
  {
    "title": "CouchApp function deprecation",
    "description": "Show/List/Rewrite/Update functions are deprecated",
    "content": "\n\n\nIBM Cloudant, the Database-as-a-Service based on the Apache CouchDB project, is giving notice that the following features are deprecated:\n\n- show functions - used to modify the format of the response when requesting a single document from the database. Show functions are accessed via URLs of the form: `/<db>/_design/<design-doc>/_show/<show-name>/<docid>`. See [CouchDB docs on Show Functions](https://docs.couchdb.org/en/stable/ddocs/ddocs.html#showfun).\n- list functions - similar to show functions, but applied to the output of MapReduce views. List functions are accessed via URLs of the form `/<db>/_design/<design-doc>/_list/<list-name>/<view-name>`. See [CouchDB docs on List Functions](https://docs.couchdb.org/en/stable/ddocs/ddocs.html#list-functions).\n- rewrite functions - used to embody routing logic in CouchApps. Rewrite functions are accessed via URLs of the form `/<db>/_design/<design-doc>/_rewrite/<path>`. See [CouchDB docs on Rewrite Functions](https://docs.couchdb.org/en/stable/api/ddoc/rewrites.html?highlight=rewrite#db-design-design-doc-rewrite-path).\n- update functions - used to carry out business logic within the database e.g. adding a timestamp to all document writes. Update functions are accessed via URLs of the form `/<db>/_design/<design-doc>/_update/<update-name>/<doc-id>`. See [CouchDB docs on Update Functions](https://docs.couchdb.org/en/stable/api/ddoc/render.html?highlight=update%20handler#db-design-design-doc-update-update-name-doc-id)\n\nThese four features are already deprecated in Apache CouchDB and scheduled to be removed from the code in CouchDB 4.0. None of these features are exposed in our Cloudant SDKs.\n\n![couch]({{< param \"image\" >}})\n> Photo by [Hal Gatewood on Unsplash](https://unsplash.com/photos/Vfml26Iy4mI)\n\nAlthough these features are deprecated, they will not be removed from the service yet. We may completely remove this features in due course, but will leave them operable for the time being to give customers time to modify their applications. As deprecated features, they will not appear in our documentation, their use is not recommended and they will not be supported by our Support team.\n\n## Alternatives\n\nShow and List function simply transform single documents and view rows, respectively, within Cloudant before returning data to the client. This functionality can be easily reproduced by transforming Cloudant's usual JSON reponses on the client side.\n\nRewrite functions are used in CouchApps, where a web application is hosted by Cloudant - CouchApps [were deprecated in October 2021]({{< ref \"/2021-10-20-CouchApps-no-longer-work.md\" >}}) so application hosting must be undertaken by a separate web server or hosting provider which can perform the routing logic.\n\nUpdate functions have two main use-cases:\n\n1. Modifying a document prior to writing - this logic can be transferred into client-side application logic instead i.e. if you need a timestamp recording inside every document written, add this field in your client-side logic.\n2. Deciding whether a document is written or not - this logic can be implemented on the client side, or as Javascript logic in a [Validate Document Update function (VDU)](https://docs.couchdb.org/en/3.2.2/ddocs/ddocs.html#validate-document-update-functions) - VDU functions have _not_ been deprecated.\n",
    "url": "/2022/08/16/Show-list-rewrite-udpate-functions-deprecated.html",
    "tags": "Deprecation CouchApp",
    "id": "101"
  },
  {
    "title": "Replication Efficiency Improvements",
    "description": "How to make replication go faster",
    "content": "\n\n\n\nCloudant's replication is a rock-solid protocol that allows a database's changes to be easily synced to a different database. This feature is used widely to create multi-region Cloudant [topologies]({{< ref \"/2017-11-07-Cloudant-replication-topologies.md\" >}}), allowing dependent applications to survive a regional Cloud outage.\n\nCloudant has recently published a number of improvements that make replication even better than before - in our internal benchmarks we have seen replications speeds of 3x the previous version. Some of these features have been switched _off_ by default, but may become the default behaviour in future releases.\n\nIn this blog post we'll explore what has changed and how the new optional features can be switched on in your replications.\n\n![replication]({{< param \"image\" >}})\n> Photo by [Vince Fleming on Unsplash](https://unsplash.com/photos/Vmr8bGURExo)\n\nBefore we get to that, it is instructive to understand how Cloudant's replication works.\n\n## How does Replication work?\n\nReplication consists of three actors:\n\n- The _source_. The Cloudant database containing the data that is to be written to the target.\n- The _target_. The Cloudant database where data from the source is to be written.\n- The _mediator_. The Cloudant instance that performs the administration of the replication process. This can be either the _source_ or _target_ instance, or in some cases, an entirely different Cloudant instance.\n\nReplication data logically flows in one direction only: from the _source_ to the _target_ - changes that occur on the _source_ are carefully grafted on to data that exists in the _target_ database. (If two way \"sync\" is required, then two replications are needed - one for data flowing from A->B, the other for B->A).\n\nReplications are started by writing a document to the `_replicator` database on the _mediator_ (remember that the _mediator_ may also be either the _source_ or _target_ Cloudant service). In this case we're replicating a database `a` on one Cloudant instance to database `b` on another:\n\n```js\n{\n  \"_id\": \"a_to_b\",\n  \"source\": \"https://myfirstaccount.cloudant.com/a\",\n  \"target\": \"https://mysecondaccount.cloudant.com/b\"\n}\n```\n\n> Note that this simplified `_replicator` document omits any authentication credentials which would be necessary for the replication to proceed. The replication document has a number of [optional configuration parameters](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-advanced-replication) but `source` and `target` are the key mandatory options.\n\nOnce the document is written to the Cloudant _mediator_ instance's `_replicator` database, that Cloudant service will begin a series of repeating steps:\n\n1. A batch of changes is read from the _source_ instance. This uses the Cloudant [Changes Feed](https://cloud.ibm.com/apidocs/cloudant#postchanges-changes) API call which provides a list of document revisions that have occurred since the last _checkpoint_ (see step 5).\n2. The _target_ database is then queried to see if it already has the changes from step 1. This uses batched calls to the Cloudant [_revs_diff](https://cloud.ibm.com/apidocs/cloudant#postrevsdiff) API call, which given a list of document revisions will reply with a list of revisions the target _doesn't have_. This is an optimisation to avoid having to send data to the _target_ that it already has.\n3. The revisions required to be sent to the _target_ from Step 2, are fetched from the _source_ using the Cloudant [Document Fetch](https://cloud.ibm.com/apidocs/cloudant#getdocument) API call - one invocation for each document revision required.\n4. Batches of document revisions from Step 3 are written to the _target_ database using the Cloudant [Bulk Write](https://cloud.ibm.com/apidocs/cloudant#postbulkdocs) API. The document revisions written could be freshly inserted documents, updates to existing documents, conflicted documents or document deletions.\n5. The state of the progress of the replication job is written to the _source_ and _target_ databases as local \"checkpoint\" documents which allow a stopped replication to resume from where it left off.\n\nSteps 1 through 5 are repeated until there are no more changes for \"one-shot\" replications, or forever for \"continuous\" replications.\n\nRead more about Cloudant Replication in our [documentation](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-replication-guide).\n\n## Optimisation 1: Skipping `_revs_diff`\n\nAs discussed in the previous section, the `_revs_diff` step is there to prevent data that already exists on the _target_ being re-sent from _source_. But what if the _target_ is empty, as it would be at the start of fresh replication to a new database? The `_revs_diff` step would be a waste of time, eating up valuable database read operations from the _target_.\n\nCloudant now intelligentlly decides whether to perform the `_revs_diff` step, learning from the responses to previous `_revs_diff` requests. If it detects that the target seems to need most of the revisions being sent, then it will send them without bothering with the `_revs_diff` step in most cases.\n\nNo user action is required to unlock this optimisation - Cloudant will automatically use fewer `_revs_diff` requests when the _target_ appears to have few of the changes that the _source_ is sending. As a result, replications to empty of sparse _targets_ will proceed more quickly.\n\n## Optimisation 2: Make `_revs_diff` faster\n\nEven though some replications may use fewer `_revs_diff` API calls, the API call itself has been tweaked so that, if called, it runs much faster than before.\n\nNo user action is required to unlock this optimisation - it just goes faster!\n\n## Optimisation 3: Using `_bulk_get`\n\nInstead of using `GET /<sourcedb>/<docid>` to fetch each document revision for Step 3, the Cloudant Replicator can be configured to use the [POST /\\<sourcedb\\>/_bulk_get](https://cloud.ibm.com/apidocs/cloudant#postbulkget) endpoint instead, to fetch batches of documents in bulk.\n\nUsing fewer bulk requests in place of many more individual document fetches means that the _source_ Cloudant instance is doing less work, so replications can proceed more quickly.\n\nThis feature can be enabled by adding `\"use_bulk_get\": true` to your replication document e.g.\n\n```js\n{\n  \"_id\": \"a_to_b\",\n  \"source\": \"https://myfirstaccount.cloudant.com/a\",\n  \"target\": \"https://mysecondaccount.cloudant.com/b\",\n  \"use_bulk_get\": true\n}\n```\n\n> A word of warning on this feature: although it uses the same number of Cloudant \"reads\" as fetching the documents individually, the bulk API consumes those reads in a single _second_ - so read consumption may be more \"peaky\". Take care that your _source_ Cloudant service has enough [capacity](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-ibm-cloud-public#provisioned-throughput-capacity) to avoid exhausting the read allocation required by other API clients.\n\n## Optimisation 4: Make `_bulk_get` faster\n\nThe `_bulk_get` API call has been made more efficient so that bulk fetches of documents put less strain on the Cloudant service.\n\nNo user action is required to unlock this optimisation - it just goes faster!\n\n## Optimisation 5: Winning revisions only\n\nSometimes replication is used to repair a _source_ database that contains conflicted documents. The _source_ database can be replicated to a new _target_ but only the winning revisions are retained (leaving behind any conflicts).\n\nThis is achieved with the `\"winning_revs_only\": true` flag:\n\n```js\n{\n  \"_id\": \"a_to_b\",\n  \"source\": \"https://myfirstaccount.cloudant.com/a\",\n  \"target\": \"https://mysecondaccount.cloudant.com/b\",\n  \"use_bulk_get\": true,\n  \"winning_revs_only\": true\n}\n```\n\nSee [Repairing A Database With Conflicts]({{< ref \"/2020-11-26-Repairing-a-Database-With-Conflicts.md\" >}})\n\n> Note the `winning_revs_only` flag should only be used for one-off replications for the purposes of conflict repair. It is not suitable for general-purpose replication tasks.\n\n",
    "url": "/2023/02/08/Replication-efficiency-improvements.html",
    "tags": "Replication",
    "id": "102"
  },
  {
    "title": "Taxi Service",
    "description": "How to build a taxi service with Cloudant",
    "content": "\n\n\nWe've all used taxi or ride-sharing services where the customer uses a mobile app to define the start and end point of their journey, then a driver chooses the job and makes their way to the start point. Such applications are complex distributed, real-time systems with data originating from:\n\n- The customer (start & end points, current location, ride preferences).\n- The driver (current position, photographs).\n- The taxi company (billing etc).\n- Elsewhere (traffic conditions, mapping, route planning)\n\nThese actors are expecting that data can be shared between them from disconnected databases over potentially flaky mobile connections.\n\n![taxi driver]({{< param \"image\" >}})\n> Photo by [Waldemar on Unsplash](https://unsplash.com/photos/kYbYIWdJRh0)\n\nCloudant seems like a good fit for this application because:\n\n- It is a managed service with very high availability.\n- It can be deployed in one or more regions - keeping data closer to the customer.\n- It allows data to be synced between the cloud and mobile devices running [PouchDB](https://pouchdb.com/) giving a pseudo \"real time\" response to changing data.\n\nIn this blog post we'll see how such a system could be designed with Cloudant and PouchDB to give the best scalability and to avoid some replication pitfalls.\n\n## Cloudant replication\n\nCloudant replication is mostly used to copy data from one Cloudant database to another, typically another Cloudant service in a different IBM Cloud region. As Cloudant replication uses the same protocol as its open-source sibling [Apache CouchDB](https://couchdb.apache.org/), it can also be used to transfer data between [PouchDB](https://pouchdb.com/) or [CouchDB](https://couchdb.apache.org/) and Cloudant. PouchDB can be be embedded into mobile apps to:\n\n- Store data locally and forward the data to the cloud when there is network connectivity - an \"offline first\" approach.\n- Sync data between mobile and cloud continuously to allow data to be modified on the cloud copy or on the mobile device.\n\nIt seems like replication is going to be a powerful tool in transferring data between mobile and cloud replicas, in both directions. But before we get carried away we have some design decisions to make.\n\n## Data design\n\nWe will likely have a database containing the taxi drivers that are on the company's books and the customers that have signed up to use the app. One document per driver/user would make sense here as this data need not be updated frequently. This data can stay on the cloud-side Cloudant databases and needn't be replicated to the driver or customer's devices.\n\nAs for the journey data, it's tempting to go for a \"one document per journey\" schema. The document could start small with just the customer's location and destination. At a later date the nominated driver details could be added. The document would be modified over its life as the journey proceeds, adding driver location over time, billing information and journey status.\n\nThere are some drawbacks to consider with this approach:\n\n- If we put all of our journeys into one Cloudant database, then the database will keep growing forever. This isn't good practice with Cloudant and it's better to store ever-growing data sets in groups of \"time boxed\" databases. See [this blog post]({{< ref \"/2019-04-08-Time-series-data-storage.md\" >}}) for more details.\n- If all of our journeys are in one database, or a collection of time-boxed databases (i.e.a month's journeys per database), then how can we sync/replicate a single journey's data to the driver and customer without transferring _everyone else's journey data_ at the same time? Cloudant can do [filtered replication](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-replication-guide#filtered-replications-repl-guide) to copy a subset of data, but this wouldn't scale for thousands of drivers/customers.\n- If more than one actor modifies the document at the same time, then Cloudant and PouchDB will store the clashing revisions as one or more [document conflicts]({{< ref \"/2018-07-25-Removing-Conflicts.md\" >}}). Although Cloudant will retain both conflicting revisions, the application will have to add considerable extra complexity when deciding how to resolve the conflict.\n\nPerhaps there's a data design other than \"one document per journey\" that overcomes these pitfalls?\n\n## A possible solution\n\nThere is a solution which allows for:\n\n- Clean, scalable replication for a single journey between driver, customer and company.\n- Avoids scaling problems that occur when using filtered replication from a shared, ever-growing database.\n- Avoids the possibility of document conflicts.\n\nThe solution is to create _one database per live journey_, where the database contains _write only_ documents, each representing an event throughout the lifecycle of the trip e.g.\n\n- The profile of the customer requesting the taxi.\n- Meta data about the trip: how many seats, any special requirements.\n- An estimated price from the taxi company. *\n- The location of the pick up point supplied by the customer. *\n- The current location of the customer. *\n- The profile of the driver who is assigned to the job. *\n- The current location of the driver. *\n- A request to cancel the job, from either the customer or driver.\n- The customer has been collected.\n- The customer has been dropped off.\n- The customer has paid.\n- The customer has left a tip.\n- Customer feedback.\n\n> Note: documents types marked `*` could appear multiple times in the database.\n\nA per-journey database can be created as soon as the customer has supplied their location, destination and trip parameters e.g. \"wheelchair access required\". The database starts off small and slowly gains documents over the lifetime of the trip. Importantly, no document is ever edited or deleted so there is no possibility of document conflicts. This avoids having to deal with building a complex \"conflict resolution\" algorithm.\n\nAPI keys granting both customer and driver access to the server-side data can be generated when the database is created. When the trip is complete, the API keys can be revoked. \n\n![taxi diagram](/img/taxi1.jpg)\n\nEach party would have a copy of the data (the driver and customer in PouchDB, and Cloudant with a copy in the cloud). The client-side applications would have to fetch all of the documents in their local database to find out the current status of the trip and its entire history. Continuous replications between mobile and cloud replicas would keep each database in sync and the mobile device would be notified as new documents arrive from elsewhere.\n\nOnce the trip is complete, the individual event documents could be combined into a single summary document which can be stored in a separate reporting database (or collection of timeboxed reporting databases) on the server-side. The original per-trip database can then be deleted, as we don't want thousands of Cloudant databases lying around forever.\n\n## Conclusion\n\nIt is tempting to use Cloudant's replication in conjuction with PouchDB for mobile applications but careful planning must be undertaken to avoid creating an unscalable or unmanagable solution. The key design aims are:\n\n- Avoid ever-growing Cloudant databases. Keep individual database sizes reasonable so that data can be backed up and to make index building times practical.\n- Avoid filtered replication to distribute subsets of data to many users. This approach quickly becomes a performance drag as user numbers increase.\n- Avoid schemas that are likely to generate document conflicts. A write-only approach has the advantage of guaranteeing that individual documents can never become conflicted.\n- Keep the volume of data that needs to be replicated between mobile devices and the cloud to a minumum to keep performance slick when mobile reception is flaky.\n- For the most part, mobile applications (both driver's and customer's) will read data from their local replica and write documents locally when they need to record an event. Replication handles the transfer of documents between the mobile devices and the cloud replica.\n",
    "url": "/2023/03/09/Taxi-service-using-Cloudant.html",
    "tags": "Replication PouchDB",
    "id": "103"
  },
  {
    "title": "HTTP 200, 201 & 202",
    "description": "What does success mean in Cloudant?",
    "content": "\n\n\nWhen a Cloudant operation succeeds, it will reply with an HTTP code that is between 200 and 300. Sometimes a client application might see a 200 response and in other circumstances a 201 or 202 response. This blog post explains the difference between these response codes.\n\n![200]({{< param \"image\" >}})\n> Photo by [Kenny Eliason on Unsplash](https://unsplash.com/photos/e-g2bhvb2iQ)\n  \n## HTTP 200\n\nThe HTTP 200 response is the classic HTTP response that means \"OK\". It can can be found for most operations that fetch documents singly or in bulk:\n\n- [Fetching a document by id](https://cloud.ibm.com/apidocs/cloudant#getdocument)\n- [Listing documents in a database](https://cloud.ibm.com/apidocs/cloudant#postalldocs)\n- [Querying MapReduce views](https://cloud.ibm.com/apidocs/cloudant#getview)\n- [Querying using a selector](https://cloud.ibm.com/apidocs/cloudant#postfind)\n- [Querying a search index](https://cloud.ibm.com/apidocs/cloudant#postsearch)\n- and many others.\n\nHTTP 200 means that the API call was well formed and the response will contain the document or documents requested.\n\n## Writes in cluster\n\nCloudant clusters are multi-node distributed systems, with three copies of each document and each index being stored on separate machines. The database is _eventually consistent_ so the service prefers to be _available_ rather than _consistent_ i.e. if a node is down, it will continue to service reads and writes, even if it means that the failed node will have to catch up when it comes back online.\n\nWhen a document is written to a Cloudant database, whether it be an insert, update or delete operation, the node handling the request calculates which three nodes need to store the document. The three nodes are sent the new request and the handling node responds to the user as soon as either:\n\n- (At least) 2 or the 3 nodes respond that they have accepted the request. This results in an HTTP \"201 Created\" response. \n- One node accepts the request and other two do not (yet). This results in an \"HTTP 202 Accepted\" response.\n\n## Should I worry about an HTTP 202 response?\n\nIn short \"no\". A HTTP 202 response means that the data is safely stored in at least one Cloudant node and the others have already received the update, or will in due course via Cloudant's internal replication. It may be that the other nodes already have the document revision in question, but they were too slow to respond to the coordinating node. In any case, no further action is required.\n\nThe [Client Libraries](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-client-libraries) all treat any 2xx response as a success and client applications should follow this example.\n\n",
    "url": "/2023/03/27/HTTP-200-201-202.html",
    "tags": "HTTP",
    "id": "104"
  },
  {
    "title": "HTTP 409",
    "description": "When is a conflict a conflict?",
    "content": "\n\n\nCloudant's HTTP API will return a variety of HTTP response codes in reply to API requests. One that causes a good deal of confusion is `409 - Document update conflict`.\n\n> tl;dr - a `409 - Document update conflict` response doesn't mean that Cloudant created a conflict, in fact it avoided creating a conflict.\n\nIn this blog post we'll explore what a 409 response means and how to prevent 409s happening in your application.\n\n![tree]({{< param \"image\" >}})\n> Photo by [Jan Huber on Unsplash](https://unsplash.com/photos/4OhFZSAT3sw)\n\n## What dodes the HTTP 409 response mean?\n\nThe [HTTP RFC](https://www.rfc-editor.org/rfc/rfc9110.html#name-409-conflict) states:\n\n> The 409 (Conflict) status code indicates that the request could not be completed due to a conflict with the current state of the target resource.\n\nThis can come about in Cloudant because:\n\n1. A document with a supplied `_id` is being created and a document with that `_id` already exists.\n2. or, a document is being modified or deleted and the supplied revision has already been superceded.\n\nLet's explore those scenarios by example:\n\n### The document \\_id field must be unique\n\nCloudant documents can be created without a document `_id` and Cloudant will generate a unique `_id` for you:\n\n```sh\n# create a document with Cloudant-generated _id field\ncurl -X POST \\\n     -H 'content-type: application/json' \\\n     -d '{\"name\":\"Sue\"}' \\\n     \"$URL/mydb\"\n# {\"ok\":true,\"id\":\"524d6ff148950d4d1554946e7e91036e\",\"rev\":\"1-877206fdfc192b2a566b6eac4eaa4205\"}\n```\n\nAs the `_id` field is auto-generated, I can repeat this operation over and over again without any errors.\n\nIf I supply my own document `_id` then I am responsible for it being unique. The following operation will work the first time:\n\n```sh\n# create a document with client-generated _id field\ncurl -X POST \\\n     -H 'content-type: application/json' \\\n     -d '{\"_id\":\"myid\",\"name\":\"Sue\"}' \\\n     \"$URL/mydb\"\n# {\"ok\":true,\"id\":\"myid\",\"rev\":\"1-877206fdfc192b2a566b6eac4eaa4205\"}\n```\n\nbut if the request is repeated, it will garner an HTTP 409 response:\n\n```sh\nHTTP/2 409 \n\n{\"error\":\"conflict\",\"reason\":\"Document update conflict.\"}\n```\n\n> The fix? If you want to supply your own `_id` field when creating documents, then you are responsible for their uniqueness. See this blog post on [creating time-sortable ids]({{< ref \"/2018-08-24-Time-sortable-document-ids.md\" >}}).\n\n### A document revision can't be superceded more than once\n\nIf we create a document it will take a `1-xxx` revision (stored as the `_rev` attribute in the document). When the document is later updated, the `1-xxx` revision will become `2-yyy` where the alpha-numeric sequence after the `-` represents a hash of the document's content.\n\n```sh\n# create a the first revision of a document\ncurl -X POST \\\n     -H 'content-type: application/json' \\\n     -d '{\"_id\":\"mynewdoc\",\"name\":\"Sue\"}' \\\n     \"$URL/mydb\"\n# {\"ok\":true,\"id\":\"mynewdoc\",\"rev\":\"1-877206fdfc192b2a566b6eac4eaa4205\"}\n\n# modify the first revision to become rev 2-yyy\ncurl -X POST \\\n     -H 'content-type: application/json' \\\n     -d '{\"_id\":\"mynewdoc\",\"_rev\":\"1-877206fdfc192b2a566b6eac4eaa4205\",\"name\":\"Susan\"}' \\\n     \"$URL/mydb\"\n# {\"ok\":true,\"id\":\"mynewdoc\",\"rev\":\"2-9d949e7391749205c5acfba683eab819\"}\n```\n\nNotice:\n\n- When creating the first revision of a document, a `_rev` token is not supplied.\n- If a create/update/delete operation is successful, the resultant value of `_rev` is returned in the response as \"rev\".\n- When modifying a document, the `_rev` token *must* be supplied, in this case the rev of the first document revision.\n\nIf we try to modify revision `1` again, we will hit a HTTP 409:\n\n```sh\n# attempt to modify rev 1 again\ncurl -X POST \\\n     -H 'content-type: application/json' \\\n     -d '{\"_id\":\"mynewdoc\",\"_rev\":\"1-877206fdfc192b2a566b6eac4eaa4205\",\"name\":\"Susan\"}' \\\n     \"$URL/mydb\"\n# {\"error\":\"conflict\",\"reason\":\"Document update conflict.\"}\n```\n\nThis is Cloudant preventing the same revision being modified more than once. Cloudant knows that \"revision 1\" has already been superceded by \"revision 2\", so no further attempts to modify revision 1 will succeed.\n\n> The fix? Supply the \"rev\" of the document revision that is to be changed. If that \"rev\" has already been superceded, then Cloudant will not accept the change and will return an HTTP 409. A work around is to fetch the document again, to see the current state of the database and then resubmit your request with a different \"rev\".\n\nNote that design patterns which require a document to be modified quickly (multiple times a second) are not a good fit for Cloudant as they can result in conflicts - see next section on document conflicts.\n\n## Conflicts\n\nA conflict is a branch in the revision tree. In other words, instead of a document's revision proceeding linearly with revision 1, 2, 3, 4, there are circumstances where there are multiple revisions of same document revision number.\n\nIn the diagram below, on the left we can see an unconflicted revision tree where each new revision supercedes the last. On the right, there is a branch in the revision tree after the second document revision.\n\n![document revision trees](/img/409-1.png)\n\nIn the previous section we said that Cloudant will not accept a change to a document where the document revision has already been superceded, so how do conficts occur? There are two possible ways:\n\n1. Replication: Cloudant allows data to be replicated to other Cloudant databases, or to Apache CouchDB databases or to mobile applications running PouchDB. When there are multiple copies of data that are connected asynchronously (or perhaps only connected occasionally), it is possible that a document can be modified in two different ways and when the two conficted changes are replicated, both revisions are retained. This is intentional and allows application developers to be assured that conflicting data is not discarded - it can subsequently be repaired by deleting an unwanted revision, merging the conflicted revisions or implementing the _conflict revision_ strategy that best suits the application.\n2. Fast writes: Cloudant is a distributed database with three copies of each document on separate database nodes. It is possible when updating a document over and over in a short time window that a conflicting write can be accepted by the database (as opposed to being denied with a 409). These spurious conflicts can be eliminated by avoiding design patterns that a require a single document to be modifed very quickly.\n\nRead more about [Conflicts]({{< ref \"/2018-07-25-Removing-Conflicts.md\" >}}) and [how to repair a database with conflicts]({{< ref \"/2020-11-26-Repairing-a-Database-With-Conflicts.md\" >}}) elsewhere in the Cloudant Blog (although spurious conflicts are best avoided in the first place).\n\n## Conclusion\n\nIf an application gets an HTTP 409 response saying \"409 - Document update conflict\", it doesn't mean that Cloudant has _created_ a document conflict. On the contrary, it means that Cloudant refused to accept the request to _avoid creating a conflict_.\n",
    "url": "/2023/03/27/HTTP-409.html",
    "tags": "HTTP",
    "id": "105"
  },
  {
    "title": "HTTP 429",
    "description": "Rate limiting explained",
    "content": "\n\n\nCloudant is a Database-as-a-Service that allows customers to provision the database capacity they need in terms of the number of read, write and query operations per second.\n\nIf the consumed capacity is exceeds the provisioned capacity in a given one second window, Cloudant will respond with an _HTTP 429 Too Many Requests_ response to further requests.\n\n![speed limit]({{< param \"image\" >}})\n\nIn this blog post we'll explore which Cloudant APIs fall into the three categories and how your application can avoid HTTP 429 responses.\n\n## Reads, Writes and Queries\n\nCloudant API operations are classified as one of three types:\n\n### Reads\n\n- Single document fetches, by a known id: `GET /<db>/<doc_id>`.\n- All multi-document operations on a [partitioned database](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-database-partitioning), charged as one read per document e.g. `GET /<db>/_partition/<partition_id>/_all_docs`.\n\n### Writes\n\n- Single document writes: `PUT /<db>/<doc_id>` or `POST /<db>`.\n- Bulk document writes: `POST /<db>/_bulk_docs`.\n\n### Queries\n\n- All documents: `GET/POST /<db>/_all_docs`.\n- Bulk documents: `POST /<db>/_bulk_get`.\n- Cloudant Queries: `POST /<db>/_find`.\n- Cloudant MapReduce: `GET/POST /<db>/_design/<design_doc>/_view/<view_name>`\n- Cloudant Search: `GET/POST /<db>/_design/<design_doc>/_search/<view_name>`\n\n## Provisioned Capacity\n\nEach Cloudant instance has its own Provisioned Capacity expressed as number, with each increment representing 100 reads, 50 writes and 5 queries per second. So an instance with a Provisioned Capacity of 100 has 10,000 reads, 5,000 writes and 500 queries per second. The more you pay, the greater the number of reads, writes and queries your Cloudant instance will be able to service.\n\nThe capacity of your instance can be changed at any time using the IBM Cloud web UI or via the [API](https://cloud.ibm.com/apidocs/cloudant#putcapacitythroughputconfiguration). When your target capacity is changed, the capacity of the instance will be changed asynchronously to meet the new target.\n\n![IBM Cloud dashboard screenshot](/img/429-1.png)\n\nImportantly, the provisioned capacity is a hard limit. If a Cloudant instance receives more reads, writes or queries in a given second than its capacity, then the excess requests will recieve a _HTTP 429 Too Many Requests_ response. Further requests will be allowed again at the start of next second boundary.\n\n## How do I avoid HTTP 429 responses?\n\n### Set your provisioned capacity correctly\n\nThe provisioned capacity of your Cloudant instance should be set such that your application never exhausts the reads, writes and queries per second limit. There is an [API call](https://cloud.ibm.com/apidocs/cloudant#getcurrentthroughputinformation) which can tell you whether your applicaiton is hitting the limit and the the HTTP 429 responses can be found in your Cloudant logs. See [Logging with LogDNA]({{< ref \"/2019-09-13-Cloudant-Logging-with-LogDNA.md\" >}}).\n\nSet the capacity of your Cloudant service higher than the peak traffic required at your application's busiest time.\n\n### Retry rate-limited requests\n\nIf your application receives a HTTP 429 response, you may elect to retry the request a number of times - backing off exponentially with each attempt e.g after 1s, 2s and 4s. This technique is suitable for small blips in traffic that push your application above its normal usage. Retry logic is not suitable for sustained API traffic that exceeds the capacity of your Cloudant instance.\n\nThe official IBM Cloudant SDKs can be configured to [attempt retries after a 429 response](https://github.com/IBM/ibm-cloud-sdk-common/#automatic-retries) so that your higher-level application code need not know that a retry occurred.\n\n### Spread out load from background tasks\n\nIf your application performs background tasks periodically, make sure that they are set up such that the load they bear on your Cloudant service doesn't use up capacity needed for operational API calls.\n\n- Keep bulk inserts to reasonable batch size e.g. 100 small docs per request.\n- Remember that replication tasks contribute to your Cloudant usage - with reads consumed at the _source_ and writes at the _target.\n- If replication is proceeding too fast (and exhausting your Cloudant instance's capacity), then the speed can be tuned with additional [replication parameters](https://cloud.ibm.com/docs/Cloudant?topic=Cloudant-replication-guide#tuning-replication-speed)\n- Run background workloads at a low concurrency and a moderate rate to avoid starving operational API calls.",
    "url": "/2023/03/27/HTTP-429.html",
    "tags": "HTTP",
    "id": "106"
  }
]
